{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 基于传统ML的图像像素识别"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Image Loading"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# import cv2\n",
    "# import numpy as np\n",
    "# from PIL import Image\n",
    "# import os\n",
    "# import torch\n",
    "# import matplotlib.pyplot as plt\n",
    "# from os.path import join\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# # Function to binarize a tensor of images using Otsu's method\n",
    "# def binarize_tensor_images(tensor_images):\n",
    "#     binary_images = []\n",
    "#     for image in tensor_images:\n",
    "#         # Convert PyTorch tensor to numpy array\n",
    "#         image_np = image.numpy().squeeze()  # Remove single-dimensional entries\n",
    "#         \n",
    "#         # Ensure the image is in uint8 format\n",
    "#         if image_np.dtype != np.uint8:\n",
    "#             image_np = (image_np * 255).astype(np.uint8)\n",
    "#         \n",
    "#         # Apply Otsu's thresholding\n",
    "#         _, binary_image = cv2.threshold(image_np, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
    "#         \n",
    "#         binary_images.append(torch.tensor(binary_image))\n",
    "#     return torch.stack(binary_images)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# # Function to load and binarize images in batches\n",
    "# def process_images_in_batches(data_path, batch_size=100):\n",
    "#     img_list = []\n",
    "# \n",
    "#     for file in os.listdir(data_path):\n",
    "#         if not file.endswith('seg_data.pt'):\n",
    "#             continue\n",
    "#         img_path = join(data_path, file)\n",
    "#         data = torch.load(img_path)\n",
    "#         img_list.append(data)\n",
    "#         print(f\"Loaded {file}\")\n",
    "# \n",
    "#     img_list = torch.cat(img_list)\n",
    "#     print(\"Loaded images shape:\", img_list.shape)\n",
    "#     print(\"Data type of loaded images:\", img_list.dtype)\n",
    "# \n",
    "#     binarized_list = []\n",
    "# \n",
    "# #     Process images in batches\n",
    "    # num_batches = len(img_list) // batch_size + 1\n",
    "    # for i in range(num_batches):\n",
    "    #     start_idx = i * batch_size\n",
    "    #     end_idx = min((i + 1) * batch_size, len(img_list))\n",
    "    #     batch = img_list[start_idx:end_idx]\n",
    "    #     binarized_batch = binarize_tensor_images(batch)\n",
    "    #     binarized_list.append(binarized_batch)\n",
    "    #     print(f\"Processed batch {i+1}/{num_batches}\")\n",
    "    # \n",
    "    # binarized_images = torch.cat(binarized_list)\n",
    "    # return binarized_images"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# CVL_path ='/root/autodl-tmp/APS360_Project/Datasets/CVL_Processed'\n",
    "# IAM_path = '/root/autodl-tmp/APS360_Project/Datasets/IAM_Processed'"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# cvlt_binarized_images = process_images_in_batches(CVL_path, batch_size=100)\n",
    "# torch.save(cvlt_binarized_images, '/root/autodl-tmp/APS360_Project/Machine_Learning_Output/binary_image_bounding_box/CVL_binary.pt')\n",
    "# print(\"Saved binarized CVL images\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# # Function to invert and binarize a tensor of images using Otsu's method\n",
    "# def binarize_tensor_images_IAM(tensor_images):\n",
    "#     binary_images = []\n",
    "#     for image in tensor_images:\n",
    "#         # Convert PyTorch tensor to numpy array\n",
    "#         image_np = image.numpy().squeeze()  # Remove single-dimensional entries\n",
    "#         \n",
    "#         # Ensure the image is in uint8 format\n",
    "#         if image_np.dtype != np.uint8:\n",
    "#             image_np = (image_np * 255).astype(np.uint8)\n",
    "#         \n",
    "#         # Invert the image\n",
    "#         image_np = 255 - image_np\n",
    "#         \n",
    "#         # Apply Otsu's thresholding\n",
    "#         _, binary_image = cv2.threshold(image_np, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
    "#         \n",
    "#         binary_images.append(torch.tensor(binary_image, dtype=torch.uint8))\n",
    "#     return torch.stack(binary_images)\n",
    "# \n",
    "# # Function to process images in batches\n",
    "# def process_images_in_batches_IAM(data_path, batch_size=100):\n",
    "#     img_list = []\n",
    "# \n",
    "#     for file in os.listdir(data_path):\n",
    "#         if not file.endswith('seg_data.pt'):\n",
    "#             continue\n",
    "#         img_path = join(data_path, file)\n",
    "#         data = torch.load(img_path)\n",
    "#         img_list.append(data)\n",
    "#         print(f\"Loaded {file}\")\n",
    "# \n",
    "#     img_list = torch.cat(img_list)\n",
    "#     print(\"Loaded images shape:\", img_list.shape)\n",
    "#     print(\"Data type of loaded images:\", img_list.dtype)\n",
    "# \n",
    "#     binarized_list = []\n",
    "# \n",
    "#     # Process images in batches\n",
    "#     num_batches = len(img_list) // batch_size + 1\n",
    "#     for i in range(num_batches):\n",
    "#         start_idx = i * batch_size\n",
    "#         end_idx = min((i + 1) * batch_size, len(img_list))\n",
    "#         batch = img_list[start_idx:end_idx]\n",
    "#         binarized_batch = binarize_tensor_images_IAM(batch)\n",
    "#         binarized_list.append(binarized_batch)\n",
    "#         print(f\"Processed batch {i+1}/{num_batches}\")\n",
    "# \n",
    "#     binarized_images = torch.cat(binarized_list)\n",
    "#     return binarized_images"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# # Process IAM images\n",
    "# iam_binarized_images = process_images_in_batches_IAM(IAM_path, batch_size=100)\n",
    "# torch.save(iam_binarized_images, '/root/autodl-tmp/APS360_Project/Machine_Learning_Output/binary_image_bounding_box/IAM_binary.pt')\n",
    "# print(\"Saved binarized IAM images\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Connected Component Detection"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# def connected_component_labeling_batch(tensor_images):\n",
    "#     results = []\n",
    "#     for image in tensor_images:\n",
    "#         image_np = image.numpy()\n",
    "#         # Apply connected component labeling\n",
    "#         num_labels, labels, stats, centroids = cv2.connectedComponentsWithStats(image_np, connectivity=8)\n",
    "#         results.append((num_labels, labels, stats, centroids))\n",
    "#     return results\n",
    "# \n",
    "# def process_in_batches(tensor_images, batch_size, processing_fn, output_path):\n",
    "#     processed_batches = []\n",
    "#     num_batches = len(tensor_images) // batch_size + (1 if len(tensor_images) % batch_size != 0 else 0)\n",
    "#     for i in range(num_batches):\n",
    "#         start_idx = i * batch_size\n",
    "#         end_idx = min((i + 1) * batch_size, len(tensor_images))\n",
    "#         batch = tensor_images[start_idx:end_idx]\n",
    "#         processed_batch = processing_fn(batch)\n",
    "#         processed_batches.extend(processed_batch)\n",
    "#         \n",
    "#         # Save intermediate results to avoid memory overflow\n",
    "#         torch.save(processed_batches, output_path)\n",
    "#         processed_batches = []  # Clear list to free memory\n",
    "#         \n",
    "#         print(f\"Processed batch {i + 1}/{num_batches}\")\n",
    "#         \n",
    "#         # Free GPU memory if necessary\n",
    "#         if torch.cuda.is_available():\n",
    "#             torch.cuda.empty_cache()\n",
    "#     return processed_batches"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# output_dir = '/root/autodl-tmp/APS360_Project/Machine_Learning_Output/binary_image_bounding_box'\n",
    "# \n",
    "# # Load binarized images\n",
    "# CVL_binarized_images_path = '/root/autodl-tmp/APS360_Project/Machine_Learning_Output/binary_image_bounding_box/CVL_binary.pt'\n",
    "# IAM_binarized_images_path = '/root/autodl-tmp/APS360_Project/Machine_Learning_Output/binary_image_bounding_box/IAM_binary.pt'\n",
    "# \n",
    "# cvl_binarized_images = torch.load(CVL_binarized_images_path)\n",
    "# iam_binarized_images = torch.load(IAM_binarized_images_path)\n",
    "# \n",
    "# # Process in smaller batches\n",
    "# batch_size = 50"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# # CVL dataset\n",
    "# cvl_output_path = os.path.join(output_dir, 'CVL_seg_connected_components.pt')\n",
    "# cvl_labels_stats_centroids = process_in_batches(cvl_binarized_images, batch_size, connected_component_labeling_batch, cvl_output_path)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# # IAM dataset\n",
    "# iam_output_path = os.path.join(output_dir, 'IAM_seg_connected_components.pt')\n",
    "# iam_labels_stats_centroids = process_in_batches(iam_binarized_images, batch_size, connected_component_labeling_batch, iam_output_path)\n",
    "# \n",
    "# print(\"Connected component detection completed for CVL and IAM datasets.\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bounding Box Calculation"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# # Function to apply connected component labeling and calculate bounding boxes\n",
    "# def calculate_bounding_boxes(tensor_images):\n",
    "#     results = []\n",
    "#     for image_tensor in tensor_images:\n",
    "#         # Convert tensor to numpy array\n",
    "#         image_np = image_tensor.numpy()\n",
    "#         \n",
    "#         # Convert to binary image (assuming already binarized)\n",
    "#         binary_image_np = np.uint8(image_np * 255)\n",
    "#         \n",
    "#         # Apply connected component labeling\n",
    "#         num_labels, labels, stats, centroids = cv2.connectedComponentsWithStats(binary_image_np, connectivity=8)\n",
    "#         \n",
    "#         # Extract bounding boxes from stats\n",
    "#         bounding_boxes = []\n",
    "#         for stat in stats[1:]:  # Skip the first entry which is the background\n",
    "#             x, y, w, h, area = stat\n",
    "#             bounding_boxes.append((x, y, x + w, y + h))  # Format: (x_min, y_min, x_max, y_max)\n",
    "#         \n",
    "#         results.append((labels, bounding_boxes))\n",
    "#     \n",
    "#     return results"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# # Process in batches\n",
    "# batch_size = 100  # Adjust batch size as needed\n",
    "# num_batches = len(cvl_binarized_images) // batch_size + 1\n",
    "# \n",
    "# cvl_results = []\n",
    "# for i in range(num_batches):\n",
    "#     start_idx = i * batch_size\n",
    "#     end_idx = min((i + 1) * batch_size, len(cvl_binarized_images))\n",
    "#     batch_images = cvl_binarized_images[start_idx:end_idx]\n",
    "#     \n",
    "#     batch_results = calculate_bounding_boxes(batch_images)\n",
    "#     cvl_results.extend(batch_results)\n",
    "# \n",
    "# print(\"Bounding boxes calculated for CVL dataset.\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# # Process in batches\n",
    "# batch_size = 100  # Adjust batch size as needed\n",
    "# num_batches = len(iam_binarized_images) // batch_size + 1\n",
    "# \n",
    "# iam_results = []\n",
    "# for i in range(num_batches):\n",
    "#     start_idx = i * batch_size\n",
    "#     end_idx = min((i + 1) * batch_size, len(iam_binarized_images))\n",
    "#     batch_images = iam_binarized_images[start_idx:end_idx]\n",
    "#     \n",
    "#     batch_results = calculate_bounding_boxes(batch_images)\n",
    "#     iam_results.extend(batch_results)\n",
    "# \n",
    "# print(\"Bounding boxes calculated for IAM dataset.\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# from sklearn.cluster import DBSCAN\n",
    "# \n",
    "# def combine_bboxes(coor_list, eps=20, min_samples=1):\n",
    "#     \"\"\"\n",
    "#     Combine small boxes that are close to each other into one big box.\n",
    "#     \n",
    "#     Args:\n",
    "#         coor_list (list): List of entries where each entry is a tuple (0, array of bounding boxes).\n",
    "#                           Each array contains sets of coordinates (x1, y1, x2, y2) for the same image.\n",
    "#         eps (int): The maximum distance between two samples for them to be considered as in the same neighborhood.\n",
    "#         min_samples (int): The number of samples (or total weight) in a neighborhood for a point to be considered as a core point.\n",
    "#         \n",
    "#     Returns:\n",
    "#         updated_coor_list (list): List of updated bounding box coordinates for each image.\n",
    "#                                   Each entry is a tuple (0, array of updated bounding boxes).\n",
    "#     \"\"\"\n",
    "#     def merge_boxes(boxes):\n",
    "#         x_min = min(box[0] for box in boxes)\n",
    "#         y_min = min(box[1] for box in boxes)\n",
    "#         x_max = max(box[2] for box in boxes)\n",
    "#         y_max = max(box[3] for box in boxes)\n",
    "#         return [x_min, y_min, x_max, y_max]\n",
    "#     \n",
    "#     updated_coor_list = []\n",
    "#     \n",
    "#     for entry in coor_list:\n",
    "#         bboxes = entry[1]\n",
    "#         \n",
    "#         if len(bboxes) == 0:\n",
    "#             updated_coor_list.append((0, bboxes))\n",
    "#             continue\n",
    "#         \n",
    "#         try:\n",
    "#             # Initialize a list to hold the center coordinates\n",
    "#             centers_list = []\n",
    "#             \n",
    "#             # Calculate the center of each bounding box\n",
    "#             for box in bboxes:\n",
    "#                 x1, y1, x2, y2 = box\n",
    "#                 center_x = (x1 + x2) / 2\n",
    "#                 center_y = (y1 + y2) / 2\n",
    "#                 centers_list.append([center_x, center_y])\n",
    "#             \n",
    "#             # Convert the list of centers to a NumPy array\n",
    "#             centers = np.array(centers_list, dtype=float)\n",
    "#             \n",
    "#         except Exception as e:\n",
    "#             print(f\"Error creating centers array: {e}\")\n",
    "#             continue\n",
    "#         \n",
    "#         # Apply DBSCAN clustering\n",
    "#         try:\n",
    "#             db = DBSCAN(eps=eps, min_samples=min_samples).fit(centers)\n",
    "#             labels = db.labels_\n",
    "#             \n",
    "#             # Combine boxes based on clustering labels\n",
    "#             combined_bboxes = []\n",
    "#             unique_labels = set(labels)\n",
    "#             \n",
    "#             for label in unique_labels:\n",
    "#                 if label == -1:\n",
    "#                     # Noise points are considered as separate boxes\n",
    "#                     combined_bboxes.extend([bboxes[i] for i in range(len(labels)) if labels[i] == label])\n",
    "#                 else:\n",
    "#                     # Combine boxes with the same label\n",
    "#                     label_indices = [i for i in range(len(labels)) if labels[i] == label]\n",
    "#                     combined_bboxes.append(merge_boxes([bboxes[i] for i in label_indices]))\n",
    "#             \n",
    "#             updated_coor_list.append((0, np.array(combined_bboxes)))\n",
    "#         \n",
    "#         except Exception as e:\n",
    "#             print(f\"Error applying DBSCAN clustering: {e}\")\n",
    "#     \n",
    "#     return updated_coor_list"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# new_cvl_results = combine_bboxes(cvl_results)",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# new_iam_results = combine_bboxes(iam_results)",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# for i in range (0,10):\n",
    "#     print (new_cvl_results[i][1])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drawing Bounding Boxes"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# import cv2\n",
    "# import os\n",
    "# import numpy as np\n",
    "# from PIL import Image, ImageOps"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# def resize_image(image, size=(128, 128)):\n",
    "#     return cv2.resize(image, size)\n",
    "# \n",
    "# def resize_and_clean_words(crop_folder):\n",
    "#     if not os.path.exists(crop_folder):\n",
    "#         raise ValueError(f\"The folder '{crop_folder}' does not exist.\")\n",
    "# \n",
    "#     for root, dirs, files in os.walk(crop_folder):\n",
    "#         for img_file in files:\n",
    "#             if img_file.endswith(('.png', '.jpg', '.jpeg')):\n",
    "#                 img_path = os.path.join(root, img_file)\n",
    "#                 image = cv2.imread(img_path)\n",
    "#                 \n",
    "#                 # Resize the image\n",
    "#                 resized_image = resize_image(image)\n",
    "#                 \n",
    "#                 # Save the resized image (overwrite the original cropped image)\n",
    "#                 cv2.imwrite(img_path, resized_image)\n",
    "#                 print(f\"Resized image saved to {img_path}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# def load_image_tensor(file_path):\n",
    "#     try:\n",
    "#         # Load the tensor from the .pt file\n",
    "#         image_tensor = torch.load(file_path)\n",
    "#         if isinstance(image_tensor, torch.Tensor):\n",
    "#             # Convert tensor to numpy array\n",
    "#             image_array = image_tensor.numpy()\n",
    "#             \n",
    "#             # If the tensor is in (C, H, W) format, convert it to (H, W, C)\n",
    "#             if image_array.ndim == 4:  # (N, C, H, W) format\n",
    "#                 image_array = np.transpose(image_array, (0, 2, 3, 1))\n",
    "#             \n",
    "#             return image_array\n",
    "#         else:\n",
    "#             print(f\"Unsupported tensor type: {type(image_tensor)}\")\n",
    "#             return None\n",
    "#     except Exception as e:\n",
    "#         print(f\"Error loading image tensor from {file_path}: {e}\")\n",
    "#         return None\n",
    "# \n",
    "# def process_the_folder(binary_image_file_path, bounding_box, output_folder, prefix):\n",
    "#     # Load binary images\n",
    "#     binary_images = load_image_tensor(binary_image_file_path)\n",
    "#     if binary_images is None:\n",
    "#         return\n",
    "# \n",
    "#     # Ensure that the number of bounding boxes matches the number of images\n",
    "#     num_images = binary_images.shape[0]\n",
    "#     if len(bounding_box) != num_images:\n",
    "#         print(f\"Warning: Number of bounding boxes ({len(bounding_box)}) does not match number of images ({num_images}).\")\n",
    "#         return\n",
    "# \n",
    "#     \n",
    "#     # Process each image and its corresponding bounding boxes\n",
    "#     for i in range(num_images):\n",
    "#         image = binary_images[i]\n",
    "#         bboxes = bounding_box[i][1]\n",
    "#         img_subfolder = os.path.join(output_folder, f'{prefix}_{i}')\n",
    "#         \n",
    "#         # Create a subfolder for each image\n",
    "#         if not os.path.exists(img_subfolder):\n",
    "#             os.makedirs(img_subfolder)\n",
    "#         \n",
    "#         # Crop and save bounding boxes in the subfolder\n",
    "#         crop(image, bboxes, img_subfolder)\n",
    "#         \n",
    "# \n",
    "# def crop(image, bounding_boxes, save_folder):\n",
    "#     # Iterate over each bounding box\n",
    "#     for j, bbox in enumerate(bounding_boxes):\n",
    "#         # Assume bbox is in the format [x1, y1, x2, y2] where (x1, y1) is the top-left and (x2, y2) is the bottom-right\n",
    "#         x1, y1, x2, y2 = bbox\n",
    "#         x1, y1 = max(x1, 0), max(y1, 0)\n",
    "#         x2, y2 = min(x2, image.shape[1]), min(y2, image.shape[0])\n",
    "#         \n",
    "#         # Crop the word area\n",
    "#         word_crop = image[y1:y2, x1:x2]\n",
    "# \n",
    "#         # Save the cropped image\n",
    "#         crop_path = os.path.join(save_folder, f\"word_{j}.png\")\n",
    "#         if not cv2.imwrite(crop_path, word_crop):\n",
    "#             print(f\"Failed to save cropped image to {crop_path}\")\n",
    "#         else:\n",
    "#             print(f\"Cropped image saved to {crop_path}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# CVL_file_path = '/root/autodl-tmp/APS360_Project/Machine_Learning_Output/binary_image_bounding_box/CVL_binary.pt'\n",
    "# IAM_file_path = '/root/autodl-tmp/APS360_Project/Machine_Learning_Output/binary_image_bounding_box/IAM_binary.pt'\n",
    "# \n",
    "# output_folder1 = '/root/autodl-tmp/APS360_Project/Machine_Learning_Output/output/CVL_rec'\n",
    "# output_folder2 = '/root/autodl-tmp/APS360_Project/Machine_Learning_Output/output/IAM_rec'\n",
    "# process_the_folder(CVL_file_path, new_cvl_results ,output_folder1, 'CVL')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# process_the_folder(IAM_file_path, new_iam_results, output_folder2, 'IAM')",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# def black_to_white(image_path):\n",
    "#     \"\"\"Crop out the black background from an image and return the processed image.\"\"\"\n",
    "#     try:\n",
    "#         # Open the image\n",
    "#         image = Image.open(image_path)\n",
    "#         \n",
    "#         # Check if image has an alpha channel\n",
    "#         if image.mode == 'RGBA':\n",
    "#             r, g, b, a = image.split()\n",
    "#             rgb_image = Image.merge('RGB', (r, g, b))\n",
    "#         else:\n",
    "#             rgb_image = image.convert('RGB')\n",
    "# \n",
    "#         # Invert the image\n",
    "#         inverted_image = ImageOps.invert(rgb_image)\n",
    "#         \n",
    "#         # Re-add the alpha channel if it was originally present\n",
    "#         if image.mode == 'RGBA':\n",
    "#             r2, g2, b2 = inverted_image.split()\n",
    "#             final_image = Image.merge('RGBA', (r2, g2, b2, a))\n",
    "#         else:\n",
    "#             final_image = inverted_image.convert('RGBA')\n",
    "#         \n",
    "#         return final_image\n",
    "# \n",
    "#     except Exception as e:\n",
    "#         print(f\"Error processing image {image_path}: {e}\")\n",
    "#         return None\n",
    "# \n",
    "# def process_images_in_subfolders(root_folder):\n",
    "#     \"\"\"Process all images in a folder and its subfolders, crop out the black background, and overwrite the original images.\"\"\"\n",
    "#     for subdir, _, files in os.walk(root_folder):\n",
    "#         for file_name in files:\n",
    "#             if file_name.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp')):\n",
    "#                 # Construct full file path\n",
    "#                 img_path = os.path.join(subdir, file_name)\n",
    "#                 \n",
    "#                 # Process image\n",
    "#                 processed_image = black_to_white(img_path)\n",
    "#                 \n",
    "#                 if processed_image:\n",
    "#                     # Save the processed image by overwriting the original image\n",
    "#                     processed_image.save(img_path)\n",
    "#                     print(f\"Processed and saved {img_path}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# folder_path = '/root/autodl-tmp/APS360_Project/Machine_Learning_Output/output/CVL_rec'  # Path to the folder containing images\n",
    "# \n",
    "# # Process all images in the folder\n",
    "# process_images_in_subfolders(folder_path)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# folder_path = '/root/autodl-tmp/APS360_Project/Machine_Learning_Output/output/IAM_rec'  # Path to the folder containing images\n",
    "# \n",
    "# # Process all images in the folder\n",
    "# process_images_in_subfolders(folder_path)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-30T21:49:36.873481Z",
     "start_time": "2024-08-30T21:49:36.870353Z"
    }
   },
   "source": [
    "# resize_and_clean_words('/root/autodl-tmp/APS360_Project/Machine_Learning_Output/output/CVL_rec')\n",
    "# resize_and_clean_words('/root/autodl-tmp/APS360_Project/Machine_Learning_Output/output/IAM_rec')"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-30T21:49:45.643446Z",
     "start_time": "2024-08-30T21:49:45.640998Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": ""
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
