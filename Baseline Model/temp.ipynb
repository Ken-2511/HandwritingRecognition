{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 基于传统ML的图像像素识别"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Image Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import os\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from os.path import join\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to binarize a tensor of images using Otsu's method\n",
    "def binarize_tensor_images(tensor_images):\n",
    "    binary_images = []\n",
    "    for image in tensor_images:\n",
    "        # Convert PyTorch tensor to numpy array\n",
    "        image_np = image.numpy().squeeze()  # Remove single-dimensional entries\n",
    "        \n",
    "        # Ensure the image is in uint8 format\n",
    "        if image_np.dtype != np.uint8:\n",
    "            image_np = (image_np * 255).astype(np.uint8)\n",
    "        \n",
    "        # Apply Otsu's thresholding\n",
    "        _, binary_image = cv2.threshold(image_np, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
    "        \n",
    "        binary_images.append(torch.tensor(binary_image))\n",
    "    return torch.stack(binary_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load and binarize images in batches\n",
    "def process_images_in_batches(data_path, batch_size=100):\n",
    "    img_list = []\n",
    "\n",
    "    for file in os.listdir(data_path):\n",
    "        if not file.endswith('seg_data.pt'):\n",
    "            continue\n",
    "        img_path = join(data_path, file)\n",
    "        data = torch.load(img_path)\n",
    "        img_list.append(data)\n",
    "        print(f\"Loaded {file}\")\n",
    "\n",
    "    img_list = torch.cat(img_list)\n",
    "    print(\"Loaded images shape:\", img_list.shape)\n",
    "    print(\"Data type of loaded images:\", img_list.dtype)\n",
    "\n",
    "    binarized_list = []\n",
    "\n",
    "    # Process images in batches\n",
    "    num_batches = len(img_list) // batch_size + 1\n",
    "    for i in range(num_batches):\n",
    "        start_idx = i * batch_size\n",
    "        end_idx = min((i + 1) * batch_size, len(img_list))\n",
    "        batch = img_list[start_idx:end_idx]\n",
    "        binarized_batch = binarize_tensor_images(batch)\n",
    "        print(binarized_batch[1])\n",
    "        binarized_list.append(binarized_batch)\n",
    "        print(f\"Processed batch {i+1}/{num_batches}\")\n",
    "\n",
    "    binarized_images = torch.cat(binarized_list)\n",
    "    return binarized_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "CVL_path ='/root/autodl-tmp/APS360_Project/Datasets/CVL_Processed'\n",
    "IAM_path = '/root/autodl-tmp/APS360_Project/Datasets/IAM_Processed'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded seg_data.pt\n",
      "Loaded images shape: torch.Size([1598, 1, 1024, 1024])\n",
      "Data type of loaded images: torch.float32\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], dtype=torch.uint8)\n",
      "Processed batch 1/16\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], dtype=torch.uint8)\n",
      "Processed batch 2/16\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], dtype=torch.uint8)\n",
      "Processed batch 3/16\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], dtype=torch.uint8)\n",
      "Processed batch 4/16\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], dtype=torch.uint8)\n",
      "Processed batch 5/16\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], dtype=torch.uint8)\n",
      "Processed batch 6/16\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], dtype=torch.uint8)\n",
      "Processed batch 7/16\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], dtype=torch.uint8)\n",
      "Processed batch 8/16\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], dtype=torch.uint8)\n",
      "Processed batch 9/16\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], dtype=torch.uint8)\n",
      "Processed batch 10/16\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], dtype=torch.uint8)\n",
      "Processed batch 11/16\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], dtype=torch.uint8)\n",
      "Processed batch 12/16\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], dtype=torch.uint8)\n",
      "Processed batch 13/16\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], dtype=torch.uint8)\n",
      "Processed batch 14/16\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], dtype=torch.uint8)\n",
      "Processed batch 15/16\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], dtype=torch.uint8)\n",
      "Processed batch 16/16\n"
     ]
    }
   ],
   "source": [
    "cvlt_binarized_images = process_images_in_batches(CVL_path, batch_size=100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(cvlt_binarized_images, '/root/autodl-tmp/APS360_Project/Machine_Learning_Output/binary_image_bounding_box/CVL_binary.pt')\n",
    "print(\"Saved binarized CVL images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to invert and binarize a tensor of images using Otsu's method\n",
    "def binarize_tensor_images_IAM(tensor_images):\n",
    "    binary_images = []\n",
    "    for image in tensor_images:\n",
    "        # Convert PyTorch tensor to numpy array\n",
    "        image_np = image.numpy().squeeze()  # Remove single-dimensional entries\n",
    "        \n",
    "        # Ensure the image is in uint8 format\n",
    "        if image_np.dtype != np.uint8:\n",
    "            image_np = (image_np * 255).astype(np.uint8)\n",
    "        \n",
    "        # Invert the image\n",
    "        image_np = 255 - image_np\n",
    "        \n",
    "        # Apply Otsu's thresholding\n",
    "        _, binary_image = cv2.threshold(image_np, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
    "        \n",
    "        binary_images.append(torch.tensor(binary_image, dtype=torch.uint8))\n",
    "    return torch.stack(binary_images)\n",
    "\n",
    "# Function to process images in batches\n",
    "def process_images_in_batches_IAM(data_path, batch_size=100):\n",
    "    img_list = []\n",
    "\n",
    "    for file in os.listdir(data_path):\n",
    "        if not file.endswith('seg_data.pt'):\n",
    "            continue\n",
    "        img_path = join(data_path, file)\n",
    "        data = torch.load(img_path)\n",
    "        img_list.append(data)\n",
    "        print(f\"Loaded {file}\")\n",
    "\n",
    "    img_list = torch.cat(img_list)\n",
    "    print(\"Loaded images shape:\", img_list.shape)\n",
    "    print(\"Data type of loaded images:\", img_list.dtype)\n",
    "\n",
    "    binarized_list = []\n",
    "\n",
    "    # Process images in batches\n",
    "    num_batches = len(img_list) // batch_size + 1\n",
    "    for i in range(num_batches):\n",
    "        start_idx = i * batch_size\n",
    "        end_idx = min((i + 1) * batch_size, len(img_list))\n",
    "        batch = img_list[start_idx:end_idx]\n",
    "        binarized_batch = binarize_tensor_images_IAM(batch)\n",
    "        print(binarized_batch[1])\n",
    "        binarized_list.append(binarized_batch)\n",
    "        print(f\"Processed batch {i+1}/{num_batches}\")\n",
    "\n",
    "    binarized_images = torch.cat(binarized_list)\n",
    "    return binarized_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded seg_data.pt\n",
      "Loaded images shape: torch.Size([1539, 1, 1024, 1024])\n",
      "Data type of loaded images: torch.float32\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], dtype=torch.uint8)\n",
      "Processed batch 1/16\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], dtype=torch.uint8)\n",
      "Processed batch 2/16\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], dtype=torch.uint8)\n",
      "Processed batch 3/16\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], dtype=torch.uint8)\n",
      "Processed batch 4/16\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Process IAM images\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m iam_binarized_images \u001b[38;5;241m=\u001b[39m \u001b[43mprocess_images_in_batches_IAM\u001b[49m\u001b[43m(\u001b[49m\u001b[43mIAM_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[13], line 45\u001b[0m, in \u001b[0;36mprocess_images_in_batches_IAM\u001b[0;34m(data_path, batch_size)\u001b[0m\n\u001b[1;32m     43\u001b[0m end_idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmin\u001b[39m((i \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m*\u001b[39m batch_size, \u001b[38;5;28mlen\u001b[39m(img_list))\n\u001b[1;32m     44\u001b[0m batch \u001b[38;5;241m=\u001b[39m img_list[start_idx:end_idx]\n\u001b[0;32m---> 45\u001b[0m binarized_batch \u001b[38;5;241m=\u001b[39m \u001b[43mbinarize_tensor_images_IAM\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28mprint\u001b[39m(binarized_batch[\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m     47\u001b[0m binarized_list\u001b[38;5;241m.\u001b[39mappend(binarized_batch)\n",
      "Cell \u001b[0;32mIn[13], line 16\u001b[0m, in \u001b[0;36mbinarize_tensor_images_IAM\u001b[0;34m(tensor_images)\u001b[0m\n\u001b[1;32m     13\u001b[0m     image_np \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m255\u001b[39m \u001b[38;5;241m-\u001b[39m image_np\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;66;03m# Apply Otsu's thresholding\u001b[39;00m\n\u001b[0;32m---> 16\u001b[0m     _, binary_image \u001b[38;5;241m=\u001b[39m \u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mthreshold\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_np\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m255\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTHRESH_BINARY\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTHRESH_OTSU\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m     binary_images\u001b[38;5;241m.\u001b[39mappend(torch\u001b[38;5;241m.\u001b[39mtensor(binary_image, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39muint8))\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mstack(binary_images)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Process IAM images\n",
    "iam_binarized_images = process_images_in_batches_IAM(IAM_path, batch_size=100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(iam_binarized_images, '/root/autodl-tmp/APS360_Project/Machine_Learning_Output/binary_image_bounding_box/IAM_binary.pt')\n",
    "print(\"Saved binarized IAM images\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Connected Component Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def connected_component_labeling_batch(tensor_images):\n",
    "    results = []\n",
    "    for image in tensor_images:\n",
    "        image_np = image.numpy()\n",
    "        # Apply connected component labeling\n",
    "        num_labels, labels, stats, centroids = cv2.connectedComponentsWithStats(image_np, connectivity=8)\n",
    "        results.append((num_labels, labels, stats, centroids))\n",
    "    return results\n",
    "\n",
    "def process_in_batches(tensor_images, batch_size, processing_fn, output_path):\n",
    "    processed_batches = []\n",
    "    num_batches = len(tensor_images) // batch_size + (1 if len(tensor_images) % batch_size != 0 else 0)\n",
    "    for i in range(num_batches):\n",
    "        start_idx = i * batch_size\n",
    "        end_idx = min((i + 1) * batch_size, len(tensor_images))\n",
    "        batch = tensor_images[start_idx:end_idx]\n",
    "        processed_batch = processing_fn(batch)\n",
    "        processed_batches.extend(processed_batch)\n",
    "        \n",
    "        # Save intermediate results to avoid memory overflow\n",
    "        torch.save(processed_batches, output_path)\n",
    "        processed_batches = []  # Clear list to free memory\n",
    "        \n",
    "        print(f\"Processed batch {i + 1}/{num_batches}\")\n",
    "        \n",
    "        # Free GPU memory if necessary\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "    return processed_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = '/root/autodl-tmp/APS360_Project/Machine_Learning_Output/binary_image_bounding_box'\n",
    "\n",
    "# Load binarized images\n",
    "CVL_binarized_images_path = '/root/autodl-tmp/APS360_Project/Machine_Learning_Output/binary_image_bounding_box/CVL_binary.pt'\n",
    "IAM_binarized_images_path = '/root/autodl-tmp/APS360_Project/Machine_Learning_Output/binary_image_bounding_box/IAM_binary.pt'\n",
    "\n",
    "cvl_binarized_images = torch.load(CVL_binarized_images_path)\n",
    "iam_binarized_images = torch.load(IAM_binarized_images_path)\n",
    "\n",
    "# Process in smaller batches\n",
    "batch_size = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed batch 1/32\n",
      "Processed batch 2/32\n",
      "Processed batch 3/32\n",
      "Processed batch 4/32\n",
      "Processed batch 5/32\n",
      "Processed batch 6/32\n",
      "Processed batch 7/32\n",
      "Processed batch 8/32\n",
      "Processed batch 9/32\n",
      "Processed batch 10/32\n",
      "Processed batch 11/32\n",
      "Processed batch 12/32\n",
      "Processed batch 13/32\n",
      "Processed batch 14/32\n",
      "Processed batch 15/32\n",
      "Processed batch 16/32\n",
      "Processed batch 17/32\n",
      "Processed batch 18/32\n",
      "Processed batch 19/32\n",
      "Processed batch 20/32\n",
      "Processed batch 21/32\n",
      "Processed batch 22/32\n",
      "Processed batch 23/32\n",
      "Processed batch 24/32\n",
      "Processed batch 25/32\n",
      "Processed batch 26/32\n",
      "Processed batch 27/32\n",
      "Processed batch 28/32\n",
      "Processed batch 29/32\n",
      "Processed batch 30/32\n",
      "Processed batch 31/32\n",
      "Processed batch 32/32\n"
     ]
    }
   ],
   "source": [
    "# CVL dataset\n",
    "cvl_output_path = os.path.join(output_dir, 'CVL_seg_connected_components.pt')\n",
    "cvl_labels_stats_centroids = process_in_batches(cvl_binarized_images, batch_size, connected_component_labeling_batch, cvl_output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed batch 1/31\n",
      "Processed batch 2/31\n",
      "Processed batch 3/31\n",
      "Processed batch 4/31\n",
      "Processed batch 5/31\n",
      "Processed batch 6/31\n",
      "Processed batch 7/31\n",
      "Processed batch 8/31\n",
      "Processed batch 9/31\n",
      "Processed batch 10/31\n",
      "Processed batch 11/31\n",
      "Processed batch 12/31\n",
      "Processed batch 13/31\n",
      "Processed batch 14/31\n",
      "Processed batch 15/31\n",
      "Processed batch 16/31\n",
      "Processed batch 17/31\n",
      "Processed batch 18/31\n",
      "Processed batch 19/31\n",
      "Processed batch 20/31\n",
      "Processed batch 21/31\n",
      "Processed batch 22/31\n",
      "Processed batch 23/31\n",
      "Processed batch 24/31\n",
      "Processed batch 25/31\n",
      "Processed batch 26/31\n",
      "Processed batch 27/31\n",
      "Processed batch 28/31\n",
      "Processed batch 29/31\n",
      "Processed batch 30/31\n",
      "Processed batch 31/31\n",
      "Connected component detection completed for CVL and IAM datasets.\n"
     ]
    }
   ],
   "source": [
    "# IAM dataset\n",
    "iam_output_path = os.path.join(output_dir, 'IAM_seg_connected_components.pt')\n",
    "iam_labels_stats_centroids = process_in_batches(iam_binarized_images, batch_size, connected_component_labeling_batch, iam_output_path)\n",
    "\n",
    "print(\"Connected component detection completed for CVL and IAM datasets.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bounding Box Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to apply connected component labeling and calculate bounding boxes\n",
    "def calculate_bounding_boxes(tensor_images):\n",
    "    results = []\n",
    "    for image_tensor in tensor_images:\n",
    "        # Convert tensor to numpy array\n",
    "        image_np = image_tensor.numpy()\n",
    "        \n",
    "        # Convert to binary image (assuming already binarized)\n",
    "        binary_image_np = np.uint8(image_np * 255)\n",
    "        \n",
    "        # Apply connected component labeling\n",
    "        num_labels, labels, stats, centroids = cv2.connectedComponentsWithStats(binary_image_np, connectivity=8)\n",
    "        \n",
    "        # Extract bounding boxes from stats\n",
    "        bounding_boxes = []\n",
    "        for stat in stats[1:]:  # Skip the first entry which is the background\n",
    "            x, y, w, h, area = stat\n",
    "            bounding_boxes.append((x, y, x + w, y + h))  # Format: (x_min, y_min, x_max, y_max)\n",
    "        \n",
    "        results.append((labels, bounding_boxes))\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bounding boxes calculated for CVL dataset.\n",
      "Saved CVL bounding boxes part 0\n",
      "Saved CVL bounding boxes part 1\n",
      "Saved CVL bounding boxes part 2\n",
      "Saved CVL bounding boxes part 3\n",
      "Saved CVL bounding boxes part 4\n",
      "Saved CVL bounding boxes part 5\n",
      "Saved CVL bounding boxes part 6\n",
      "Saved CVL bounding boxes part 7\n",
      "Saved CVL bounding boxes part 8\n",
      "Saved CVL bounding boxes part 9\n"
     ]
    }
   ],
   "source": [
    "# Process in batches\n",
    "batch_size = 100  # Adjust batch size as needed\n",
    "num_batches = len(cvl_binarized_images) // batch_size + 1\n",
    "\n",
    "cvl_results = []\n",
    "for i in range(num_batches):\n",
    "    start_idx = i * batch_size\n",
    "    end_idx = min((i + 1) * batch_size, len(cvl_binarized_images))\n",
    "    batch_images = cvl_binarized_images[start_idx:end_idx]\n",
    "    \n",
    "    batch_results = calculate_bounding_boxes(batch_images)\n",
    "    cvl_results.extend(batch_results)\n",
    "\n",
    "print(\"Bounding boxes calculated for CVL dataset.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bounding boxes calculated for IAM dataset.\n"
     ]
    }
   ],
   "source": [
    "# Process in batches\n",
    "batch_size = 100  # Adjust batch size as needed\n",
    "num_batches = len(iam_binarized_images) // batch_size + 1\n",
    "\n",
    "iam_results = []\n",
    "for i in range(num_batches):\n",
    "    start_idx = i * batch_size\n",
    "    end_idx = min((i + 1) * batch_size, len(iam_binarized_images))\n",
    "    batch_images = iam_binarized_images[start_idx:end_idx]\n",
    "    \n",
    "    batch_results = calculate_bounding_boxes(batch_images)\n",
    "    iam_results.extend(batch_results)\n",
    "\n",
    "print(\"Bounding boxes calculated for IAM dataset.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drawing Bounding Boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load a single binary image\n",
    "def load_binary_image(file_path):\n",
    "    try:\n",
    "        image = torch.load(file_path)\n",
    "        return image\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading binary image from {file_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Function to overlay bounding boxes on an image\n",
    "def overlay_bounding_boxes(image, bounding_boxes, save_path, box_color='r'):\n",
    "    useless, list_of_tuples = bounding_boxes\n",
    "    image_np = image.squeeze().cpu().numpy()\n",
    "    image_np = np.uint8(image_np.copy())\n",
    "    fig, ax = plt.subplots(1)\n",
    "    ax.imshow(image_np, cmap='gray')\n",
    "    \n",
    "    for bbox in list_of_tuples:\n",
    "        x_min, y_min, x_max, y_max = bbox\n",
    "        rect = plt.Rectangle((x_min, y_min), x_max - x_min, y_max - y_min, fill=False, edgecolor=box_color, linewidth=1)\n",
    "        ax.add_patch(rect)\n",
    "    \n",
    "    ax.axis('off')\n",
    "    \n",
    "    # Save the plot as an image file\n",
    "    plt.savefig(save_path)\n",
    "\n",
    "    # Close the Matplotlib plot to free up memory\n",
    "    plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths to your binary image files (adjust paths accordingly)\n",
    "CVL_file_path = '/root/autodl-tmp/APS360_Project/Machine_Learning_Output/binary_image_bounding_box/CVL_binary.pt'\n",
    "IAM_file_path = '/root/autodl-tmp/APS360_Project/Machine_Learning_Output/binary_image_bounding_box/IAM_binary.pt'\n",
    "\n",
    "# Paths to your bounding box folders (adjust paths accordingly)\n",
    "bounding_box_folder1 = '/root/autodl-tmp/APS360_Project/Machine_Learning_Output/binary_image_bounding_box/CVL_bounding_boxes'\n",
    "bounding_box_folder2 = '/root/autodl-tmp/APS360_Project/Machine_Learning_Output/binary_image_bounding_box/IAM_bounding_boxes'\n",
    "    \n",
    "# Load binary images\n",
    "cvl_binary_images = load_binary_image(CVL_file_path)\n",
    "iam_binary_images = load_binary_image(IAM_file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "print(len(cvl_results[1000]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process CVL dataset images and bounding boxes\n",
    "if cvl_binary_images is not None:\n",
    "    for i in range(len(cvl_binary_images)):\n",
    "        filename = f'CVL_{i}.png'\n",
    "        save_path_cvl = os.path.join('/root/autodl-tmp/APS360_Project/Machine_Learning_Output/output/CVL', filename)\n",
    "        overlay_bounding_boxes(cvl_binary_images[i], cvl_results[i], save_path_cvl )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Process IAM dataset images and bounding boxes\n",
    "if iam_binary_images is not None:\n",
    "    for i in range(len(iam_binary_images)):\n",
    "        filename = f'IAM_{i}.png'\n",
    "        save_path_iam = os.path.join('/root/autodl-tmp/APS360_Project/Machine_Learning_Output/output/IAM', filename)\n",
    "        overlay_bounding_boxes(iam_binary_images[i], iam_results[i], save_path_iam)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
