{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这个记事本是用来搭建CRNN网络的，用来做recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import torch\n",
    "import torchvision\n",
    "from torch import nn\n",
    "from torchvision import models\n",
    "from torchvision import transforms\n",
    "from utils import *\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "from torch.utils import tensorboard as tb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 测试用的代码，不用看（打印出神经网络的结构）\n",
    "# model = models.resnet50(pretrained=True)\n",
    "# # model_cnn = nn.Sequential(*list(model.children())[:-1])\n",
    "# for chi in model.children():\n",
    "#     print(chi)\n",
    "#     print('-------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里用resnet18除去最后的fc作为cnn的部分，lstm作为rnn的部分。<br>\n",
    "输入1x128x128图片<br>\n",
    "经过cnn部分，先是卷到了512x4x4，然后经过平均池化层变成512x1x1<br>\n",
    "然后展平，经过线性变换放入lstm的hidden和cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CRNN(nn.Module):\n",
    "    \"\"\"\n",
    "    这是一个基于CNN和LSTM的CRNN模型，用于生成文本\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, num_classes=128, hidden_dim=1024, io_dim=1024, num_layers=4, bidirectional=True, device='cuda:0'):\n",
    "        \"\"\"\n",
    "        args:\n",
    "            num_classes: ...\n",
    "            hidden_dim: int, the dimension of the hidden state of the LSTM\n",
    "            io_dim: int, the dimension of the input and output of the LSTM\n",
    "            num_layers: int, the number of layers of the LSTM\n",
    "            bidirectional: bool, whether to use bidirectional LSTM\n",
    "            device: str, the device\n",
    "        \"\"\"\n",
    "        super(CRNN, self).__init__()\n",
    "        self.direction_factor = 2 if bidirectional else 1\n",
    "        self.num_layers = num_layers\n",
    "        # num-classes对应ascii码表的128种字符\n",
    "        self.num_classes = num_classes\n",
    "        # hidden_dim是LSTM的隐藏层（hidden state）和细胞状态（cell state）的维度\n",
    "        self.hidden_dim = hidden_dim\n",
    "        # io_dim是LSTM的输入和输出的维度\n",
    "        self.io_dim = io_dim\n",
    "        self.device = device\n",
    "        # max num of characters of the generated text\n",
    "        self.max_len = 64\n",
    "        # 1x1卷积层，用于将灰度图转换为3通道图像以适应ResNet的输入\n",
    "        self.conv1 = nn.Conv2d(1, 3, 1)\n",
    "        # 使用ResNet50作为CNN的基础模型，去掉最后一层全连接层\n",
    "        self.cnn = models.resnet50(weights=models.ResNet50_Weights.DEFAULT)\n",
    "        self.cnn = nn.Sequential(*list(self.cnn.children())[:-1])  # output dim is 2048\n",
    "        # LSTM层，输入维度为io_dim，隐藏层维度为hidden_dim\n",
    "        self.rnn = nn.LSTM(io_dim, hidden_dim, num_layers, batch_first=True, bidirectional=bidirectional)\n",
    "        # 将CNN的输出转换为LSTM的隐藏状态和细胞状态\n",
    "        self.h0_fc = nn.Linear(2048, hidden_dim * num_layers * self.direction_factor)\n",
    "        self.c0_fc = nn.Linear(2048, hidden_dim * num_layers * self.direction_factor)\n",
    "        # 将LSTM的输出转换为最终的输出，即字符概率分布\n",
    "        self.out_fc = nn.Linear(hidden_dim * self.direction_factor, num_classes)\n",
    "        # 将字符的索引转换为字符的embedding\n",
    "        self.embedding = nn.Embedding(num_classes, io_dim)\n",
    "        # dropout防止过拟合\n",
    "        self.dropout = nn.Dropout(0.75)\n",
    "        self.to(device)\n",
    "    \n",
    "    def init_state(self, img):\n",
    "        # 通过CNN卷出 lstm 的 hidden state 和 cell state\n",
    "        x = self.conv1(img)         # batch_size, 3, 64, 64\n",
    "        x = self.cnn(x)             # batch_size, 512, 1, 1\n",
    "        x = x.view(x.size(0), -1)   # batch_size, 512\n",
    "        x = x.unsqueeze(0)          # 1, batch_size, 512\n",
    "        h0 = self.h0_fc(x)          # 1, batch_size, hidden_dim\n",
    "        c0 = self.c0_fc(x)          # 1, batch_size, hidden_dim\n",
    "        h0 = h0.view(-1, self.hidden_dim, self.num_layers * self.direction_factor).permute(2, 0, 1).contiguous()  # 4, batch_size, hidden_dim\n",
    "        c0 = c0.view(-1, self.hidden_dim, self.num_layers * self.direction_factor).permute(2, 0, 1).contiguous()  # 4, batch_size, hidden_dim\n",
    "        return h0, c0\n",
    "    \n",
    "    def next_char(self, x, h_c_n):\n",
    "        # print(\"next char x shape: \", x.shape)\n",
    "        h_n, c_n = h_c_n\n",
    "        # x: the embedding of the last character\n",
    "        # h_n: the hidden state of the last character\n",
    "        # c_n: the cell state of the last character\n",
    "        x, (h_n, c_n) = self.rnn(x, (h_n, c_n))\n",
    "        # print(\"next char rnn output x shape: \", x.shape)\n",
    "        x = self.out_fc(x)\n",
    "        # x = self.dropout(x)\n",
    "        # print(\"next char output x shape: \", x.shape)\n",
    "        return x, (h_n, c_n)\n",
    "    \n",
    "    def forward(self, img):\n",
    "        batch_size = img.size(0)\n",
    "        h0, c0 = self.init_state(img)\n",
    "        x = 2  # the index of the start token\n",
    "        x = torch.tensor([x] * batch_size, dtype=torch.long).view(batch_size, 1).to(self.device)\n",
    "        x = self.embedding(x)\n",
    "        # print(\"after embedding x shape: \", x.shape)\n",
    "        h_c_n = (h0, c0)\n",
    "        temp = torch.zeros(batch_size, 1, self.num_classes).to(self.device)\n",
    "        temp[:, 0, 2] = 1\n",
    "        output = [temp]\n",
    "        for i in range(1, self.max_len):\n",
    "            x, h_c_n = self.next_char(x, h_c_n)\n",
    "            output.append(x)\n",
    "            x = x.argmax(dim=-1)\n",
    "            x = self.embedding(x)\n",
    "        output = torch.cat(output, dim=1)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载数据集\n",
    "dataset_IAM = RecDataset(\"IAM\", \"train\")\n",
    "dataset_CVL = RecDataset(\"CVL\", \"train\")\n",
    "dataset = torch.utils.data.ConcatDataset([dataset_IAM, dataset_CVL])\n",
    "train_loader = DataLoader(dataset, batch_size=10, shuffle=True, num_workers=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 1, 128, 128])\n",
      "torch.Size([10, 64, 128])\n"
     ]
    }
   ],
   "source": [
    "# 测试CRNN进行预测时的形状是否符合要求\n",
    "\n",
    "crnn = CRNN()\n",
    "for step, (img, label) in enumerate(train_loader):\n",
    "    img, label = img.to(crnn.device), label.to(crnn.device)\n",
    "    print(img.shape)\n",
    "    output = crnn(img)\n",
    "    print(output.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 1, 128, 128])\n",
      "torch.Size([8, 10, 1024]) torch.Size([8, 10, 1024])\n"
     ]
    }
   ],
   "source": [
    "# 测试CRNN进行训练时的形状是否符合要求\n",
    "\n",
    "crnn = CRNN()\n",
    "for step, (img, label) in enumerate(train_loader):\n",
    "    img, label = img.to(crnn.device), label.to(crnn.device)\n",
    "    print(img.shape)\n",
    "    h_n, c_n = crnn.init_state(img)\n",
    "    print(h_n.shape, c_n.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_name(epoch, name):\n",
    "    return f\"{name}_{epoch}.pth\"\n",
    "\n",
    "\n",
    "def get_val_loss(model, dataloader, device):\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
    "    model.eval()\n",
    "    loss = 0\n",
    "    with torch.no_grad():\n",
    "        for step, (img, target) in enumerate(dataloader):\n",
    "            img, target = img.to(device), target.to(device)\n",
    "            h_n, c_n = model.init_state(img)\n",
    "            h_n, c_n = h_n.to(device), c_n.to(device)\n",
    "            output = torch.zeros(target.shape[0], model.max_len, model.num_classes).to(device)\n",
    "            x = target[:, 0]\n",
    "            output[:, 0] = x\n",
    "            x = model.embedding(x.long().unsqueeze(1))\n",
    "            for i in range(model.max_len-1):\n",
    "                x = model.embedding(target[:, i].argmax(-1).unsqueeze(1))\n",
    "                x, (h_n, c_n) = model.next_char(x, (h_n, c_n))\n",
    "                output[:, i+1] = x.squeeze(1)\n",
    "            loss += criterion(output.view(-1, model.num_classes), target.argmax(-1).view(-1))\n",
    "    return loss / len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Val Loss 2.3464198112487793                                                                                               \n",
      "Model saved as CRNN_0.pth\n",
      "Epoch 1, Iter 1228, Loss 2.0646"
     ]
    }
   ],
   "source": [
    "# 下面是训练的代码，使用教师强制训练\n",
    "def train_crnn(model, lr, epochs, start_epoch, name, device='cuda:0'):\n",
    "    \"\"\"\n",
    "    args:\n",
    "        model: CRNN, the model to be trained\n",
    "        lr: float, the learning rate\n",
    "        epochs: int, the number of epochs to train\n",
    "        device: str, the device to use\n",
    "        start_epoch: int, the epoch to start training\n",
    "        name: str, the name of the model or the task\n",
    "    \"\"\"\n",
    "\n",
    "    # 如果start_epoch不为0，说明是从某个epoch开始训练的，需要加载模型\n",
    "    if start_epoch:\n",
    "        model_name = get_model_name(start_epoch, name)\n",
    "        model_path = \"/root/autodl-tmp/APS360_Project/Machine_Learning_Output/CRNN/\"\n",
    "        model.load_state_dict(torch.load(model_path + model_name))\n",
    "        start_epoch += 1\n",
    "        print(f\"Model loaded from {model_name}\")\n",
    "\n",
    "    writer = tb.SummaryWriter('/root/tf-logs')  # tensorboard writer，可以在浏览器中查看训练过程\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=0.9, weight_decay=1e-4)\n",
    "    model.to(device)\n",
    "    t0 = time.time()\n",
    "    for epoch in range(start_epoch, epochs):\n",
    "        model.train()\n",
    "        for step, (img, target) in enumerate(train_loader):\n",
    "            img, target = img.to(device), target.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            loss = 0\n",
    "            # 准备初始状态\n",
    "            h_n, c_n = model.init_state(img)\n",
    "            h_n, c_n = h_n.to(device), c_n.to(device)\n",
    "            output = torch.zeros(target.shape[0], model.max_len, model.num_classes).to(device)\n",
    "            x = target[:, 0]\n",
    "            output[:, 0] = x\n",
    "            x = model.embedding(x.long().unsqueeze(1))\n",
    "            for i in range(model.max_len-1):\n",
    "                x = model.embedding(target[:, i].argmax(-1).unsqueeze(1))\n",
    "                x, (h_n, c_n) = model.next_char(x, (h_n, c_n))\n",
    "                output[:, i+1] = x.squeeze(1)\n",
    "            loss = criterion(output.view(-1, model.num_classes), target.argmax(-1).view(-1))\n",
    "            loss.backward()\n",
    "            # 将cnn的梯度调小，防止过拟合\n",
    "            for param in model.cnn.parameters():\n",
    "                param.grad *= 0.1\n",
    "            optimizer.step()\n",
    "            if time.time() - t0 > 1:\n",
    "                # 每秒打印一次训练信息\n",
    "                t0 = time.time()\n",
    "                print(f\"\\rEpoch {epoch}, Iter {step}, Loss {loss.item():.4f}\", end=\"\")\n",
    "                writer.add_scalar(f'{name}/Train_Loss', loss.item(), epoch * len(train_loader) + step)\n",
    "        \n",
    "        # 计算验证集上的loss\n",
    "        val_loss = get_val_loss(model, val_loader, device)\n",
    "        print(\" \" * 100, end=\"\")\n",
    "        print(f\"\\rEpoch {epoch}, Val Loss {val_loss.item()}\", end=\"\")\n",
    "        writer.add_scalar(f'{name}/Val_Loss', val_loss.item(), epoch)\n",
    "\n",
    "        model_name = get_model_name(epoch, \"CRNN\")\n",
    "        model_path = \"/root/autodl-tmp/APS360_Project/Machine_Learning_Output/CRNN/\"\n",
    "        torch.save(model.state_dict(), model_path + model_name)\n",
    "        print(f\"\\nModel saved as {model_name}\")\n",
    "\n",
    "\n",
    "# 数据增广 & 加载数据集\n",
    "trans = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.RandomResizedCrop(size=(128, 128), scale=(0.9, 1.2), ratio=(0.9, 1.1)),\n",
    "    transforms.RandomRotation(10),\n",
    "    transforms.RandomAffine(10),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "val_trans = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "train_set_IAM = RecDataset(\"IAM\", \"train\", trans)\n",
    "train_set_CVL = RecDataset(\"CVL\", \"train\", trans)\n",
    "train_set = torch.utils.data.ConcatDataset([train_set_IAM, train_set_CVL])\n",
    "train_loader = DataLoader(train_set, batch_size=64, shuffle=True, num_workers=16)\n",
    "\n",
    "val_set_IAM = RecDataset(\"IAM\", \"val\", val_trans)\n",
    "val_set_CVL = RecDataset(\"CVL\", \"val\", val_trans)\n",
    "val_set = torch.utils.data.ConcatDataset([val_set_IAM, val_set_CVL])\n",
    "val_loader = DataLoader(val_set, batch_size=64, shuffle=False, num_workers=16)\n",
    "\n",
    "# 创建模型\n",
    "model = CRNN()\n",
    "\n",
    "# 开始训练\n",
    "train_crnn(model, lr=0.01, epochs=500, start_epoch=0, name=\"CRNN\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
