{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这个记事本是用来搭建CRNN网络的，用来做recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/autodl-tmp/APS360_Project/Train/utils.py:56: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.data = torch.load(os.path.join(self.path, 'rec_data_' + mode + '.pt'))\n",
      "/root/autodl-tmp/APS360_Project/Train/utils.py:57: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.label = torch.load(os.path.join(self.path, 'rec_label_' + mode + '.pt'))\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch import nn\n",
    "from torchvision import models\n",
    "from utils import *\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "from torch.utils import tensorboard as tb\n",
    "\n",
    "dataset_IAM = RecDataset(\"IAM\", \"train\")\n",
    "dataset_CVL = RecDataset(\"CVL\", \"train\")\n",
    "dataset = torch.utils.data.ConcatDataset([dataset_IAM, dataset_CVL])\n",
    "dataloader = DataLoader(dataset, batch_size=10, shuffle=True, num_workers=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/new_env/lib/python3.12/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/root/miniconda3/envs/new_env/lib/python3.12/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "-------------------\n",
      "BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "-------------------\n",
      "ReLU(inplace=True)\n",
      "-------------------\n",
      "MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "-------------------\n",
      "Sequential(\n",
      "  (0): Bottleneck(\n",
      "    (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu): ReLU(inplace=True)\n",
      "    (downsample): Sequential(\n",
      "      (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (1): Bottleneck(\n",
      "    (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu): ReLU(inplace=True)\n",
      "  )\n",
      "  (2): Bottleneck(\n",
      "    (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu): ReLU(inplace=True)\n",
      "  )\n",
      ")\n",
      "-------------------\n",
      "Sequential(\n",
      "  (0): Bottleneck(\n",
      "    (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu): ReLU(inplace=True)\n",
      "    (downsample): Sequential(\n",
      "      (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (1): Bottleneck(\n",
      "    (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu): ReLU(inplace=True)\n",
      "  )\n",
      "  (2): Bottleneck(\n",
      "    (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu): ReLU(inplace=True)\n",
      "  )\n",
      "  (3): Bottleneck(\n",
      "    (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu): ReLU(inplace=True)\n",
      "  )\n",
      ")\n",
      "-------------------\n",
      "Sequential(\n",
      "  (0): Bottleneck(\n",
      "    (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu): ReLU(inplace=True)\n",
      "    (downsample): Sequential(\n",
      "      (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "      (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (1): Bottleneck(\n",
      "    (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu): ReLU(inplace=True)\n",
      "  )\n",
      "  (2): Bottleneck(\n",
      "    (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu): ReLU(inplace=True)\n",
      "  )\n",
      "  (3): Bottleneck(\n",
      "    (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu): ReLU(inplace=True)\n",
      "  )\n",
      "  (4): Bottleneck(\n",
      "    (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu): ReLU(inplace=True)\n",
      "  )\n",
      "  (5): Bottleneck(\n",
      "    (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu): ReLU(inplace=True)\n",
      "  )\n",
      ")\n",
      "-------------------\n",
      "Sequential(\n",
      "  (0): Bottleneck(\n",
      "    (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu): ReLU(inplace=True)\n",
      "    (downsample): Sequential(\n",
      "      (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "      (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (1): Bottleneck(\n",
      "    (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu): ReLU(inplace=True)\n",
      "  )\n",
      "  (2): Bottleneck(\n",
      "    (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu): ReLU(inplace=True)\n",
      "  )\n",
      ")\n",
      "-------------------\n",
      "AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "-------------------\n",
      "Linear(in_features=2048, out_features=1000, bias=True)\n",
      "-------------------\n"
     ]
    }
   ],
   "source": [
    "# 测试用的代码，不用看\n",
    "model = models.resnet50(pretrained=True)\n",
    "# model_cnn = nn.Sequential(*list(model.children())[:-1])\n",
    "for chi in model.children():\n",
    "    print(chi)\n",
    "    print('-------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里用resnet18除去最后的fc作为cnn的部分，lstm作为rnn的部分。<br>\n",
    "输入1x128x128图片<br>\n",
    "经过cnn部分，先是卷到了512x4x4，然后经过平均池化层变成512x1x1<br>\n",
    "然后展平，经过线性变换放入lstm的hidden和cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CRNN(nn.Module):\n",
    "    \"\"\"\n",
    "    这是一个基于CNN和LSTM的CRNN模型，用于生成文本\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, num_classes=128, hidden_dim=512, io_dim=1024, device='cuda:0'):\n",
    "        super(CRNN, self).__init__()\n",
    "        # num-classes对应ascii码表的128种字符\n",
    "        self.num_classes = num_classes\n",
    "        # hidden_dim是LSTM的隐藏层（hidden state）和细胞状态（cell state）的维度\n",
    "        self.hidden_dim = hidden_dim\n",
    "        # io_dim是LSTM的输入和输出的维度\n",
    "        self.io_dim = io_dim\n",
    "        self.device = device\n",
    "        # max num of characters of the generated text\n",
    "        self.max_len = 64\n",
    "        # 1x1卷积层，用于将灰度图转换为3通道图像以适应ResNet的输入\n",
    "        self.conv1 = nn.Conv2d(1, 3, 1)\n",
    "        # 使用ResNet50作为CNN的基础模型，去掉最后一层全连接层\n",
    "        self.cnn = models.resnet50(weights=models.ResNet50_Weights.DEFAULT)\n",
    "        self.cnn = nn.Sequential(*list(self.cnn.children())[:-1])  # output dim is 2048\n",
    "        # LSTM层，输入维度为io_dim，隐藏层维度为hidden_dim\n",
    "        self.rnn = nn.LSTM(io_dim, hidden_dim, 1, batch_first=True)\n",
    "        # 将CNN的输出转换为LSTM的隐藏状态和细胞状态\n",
    "        self.h0_fc = nn.Linear(2048, hidden_dim)\n",
    "        self.c0_fc = nn.Linear(2048, hidden_dim)\n",
    "        # 将LSTM的输出转换为最终的输出，即字符概率分布\n",
    "        self.out_fc = nn.Linear(hidden_dim, num_classes)\n",
    "        # 将字符的索引转换为字符的embedding\n",
    "        self.embedding = nn.Embedding(num_classes, io_dim)\n",
    "        # dropout防止过拟合\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.to(device)\n",
    "    \n",
    "    def init_state(self, img):\n",
    "        # 通过CNN卷出 lstm 的 hidden state 和 cell state\n",
    "        x = self.conv1(img)         # batch_size, 3, 64, 64\n",
    "        x = self.cnn(x)             # batch_size, 512, 1, 1\n",
    "        x = x.view(x.size(0), -1)   # batch_size, 512\n",
    "        x = x.unsqueeze(0)          # 1, batch_size, 512\n",
    "        h0 = self.h0_fc(x)          # 1, batch_size, hidden_dim\n",
    "        c0 = self.c0_fc(x)          # 1, batch_size, hidden_dim\n",
    "        return h0, c0\n",
    "    \n",
    "    def next_char(self, x, h_c_n):\n",
    "        # print(\"next char x shape: \", x.shape)\n",
    "        h_n, c_n = h_c_n\n",
    "        # x: the embedding of the last character\n",
    "        # h_n: the hidden state of the last character\n",
    "        # c_n: the cell state of the last character\n",
    "        x, (h_n, c_n) = self.rnn(x, (h_n, c_n))\n",
    "        # print(\"next char rnn output x shape: \", x.shape)\n",
    "        x = self.out_fc(x)\n",
    "        x = self.dropout(x)\n",
    "        # print(\"next char output x shape: \", x.shape)\n",
    "        return x, (h_n, c_n)\n",
    "    \n",
    "    def forward(self, img):\n",
    "        batch_size = img.size(0)\n",
    "        h0, c0 = self.init_state(img)\n",
    "        x = 2  # the index of the start token\n",
    "        x = torch.tensor([x] * batch_size, dtype=torch.long).view(batch_size, 1).to(self.device)\n",
    "        x = self.embedding(x)\n",
    "        # print(\"after embedding x shape: \", x.shape)\n",
    "        h_c_n = (h0, c0)\n",
    "        temp = torch.zeros(batch_size, 1, self.num_classes).to(self.device)\n",
    "        temp[:, 0, 2] = 1\n",
    "        output = [temp]\n",
    "        for i in range(1, self.max_len):\n",
    "            x, h_c_n = self.next_char(x, h_c_n)\n",
    "            output.append(x)\n",
    "            x = x.argmax(dim=-1)\n",
    "            x = self.embedding(x)\n",
    "        output = torch.cat(output, dim=1)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 1, 128, 128])\n",
      "torch.Size([64, 64, 128])\n"
     ]
    }
   ],
   "source": [
    "# 测试CRNN进行预测时的形状是否符合要求\n",
    "\n",
    "crnn = CRNN()\n",
    "for step, (img, label) in enumerate(dataloader):\n",
    "    img, label = img.to(crnn.device), label.to(crnn.device)\n",
    "    print(img.shape)\n",
    "    output = crnn(img)\n",
    "    print(output.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 1, 128, 128])\n",
      "torch.Size([1, 64, 512]) torch.Size([1, 64, 512])\n"
     ]
    }
   ],
   "source": [
    "# 测试CRNN进行训练时的形状是否符合要求\n",
    "\n",
    "crnn = CRNN()\n",
    "for step, (img, label) in enumerate(dataloader):\n",
    "    img, label = img.to(crnn.device), label.to(crnn.device)\n",
    "    print(img.shape)\n",
    "    h_n, c_n = crnn.init_state(img)\n",
    "    print(h_n.shape, c_n.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/autodl-tmp/APS360_Project/Train/utils.py:56: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.data = torch.load(os.path.join(self.path, 'rec_data_' + mode + '.pt'))\n",
      "/root/autodl-tmp/APS360_Project/Train/utils.py:57: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.label = torch.load(os.path.join(self.path, 'rec_label_' + mode + '.pt'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded from crnn_512_1024_0.01_64_43_concat.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1879/1949177352.py:34: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(model_path + model_name))\n",
      "/root/miniconda3/envs/new_env/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:61: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 44, Iter 0, Loss 10.462600708007812\n",
      "Epoch 44, Iter 100, Loss 2.7052228450775146\n",
      "Epoch 44, Iter 200, Loss 2.5797746181488037\n",
      "Epoch 44, Iter 300, Loss 2.561100482940674\n",
      "Epoch 44, Iter 400, Loss 2.42060923576355\n",
      "Epoch 44, Iter 500, Loss 2.5613961219787598\n",
      "Epoch 44, Iter 600, Loss 2.4989161491394043\n",
      "Epoch 44, Iter 700, Loss 2.5583102703094482\n",
      "Epoch 44, Iter 800, Loss 2.4603042602539062\n",
      "Epoch 44, Iter 900, Loss 2.472644090652466\n",
      "Epoch 44, Iter 1000, Loss 2.4352757930755615\n",
      "Epoch 44, Iter 1100, Loss 2.4676551818847656\n",
      "Epoch 44, Iter 1200, Loss 2.5000548362731934\n",
      "Epoch 44, Iter 1300, Loss 2.581332206726074\n",
      "Epoch 44, Iter 1400, Loss 2.488685131072998\n",
      "Epoch 44, Iter 1500, Loss 2.535407304763794\n",
      "Epoch 44, Iter 1600, Loss 2.4326534271240234\n",
      "Epoch 44, Iter 1700, Loss 2.5917747020721436\n",
      "Epoch 44, Iter 1800, Loss 2.5547568798065186\n",
      "Epoch 44, Iter 1900, Loss 2.4932193756103516\n",
      "Epoch 44, Iter 2000, Loss 2.342564105987549\n",
      "Epoch 44, Iter 2100, Loss 2.5863428115844727\n",
      "Epoch 44, Iter 2200, Loss 2.5293686389923096\n",
      "Epoch 44, Iter 2300, Loss 2.535071849822998\n",
      "Epoch 44, Iter 2400, Loss 2.4477505683898926\n",
      "Epoch 44, Val Loss 0.9879536032676697\n",
      "Model saved as crnn_512_1024_0.01_64_44_concat.pth\n",
      "Epoch 45, Iter 0, Loss 2.4738569259643555\n",
      "Epoch 45, Iter 100, Loss 2.468140125274658\n",
      "Epoch 45, Iter 200, Loss 2.3893978595733643\n",
      "Epoch 45, Iter 300, Loss 2.63346791267395\n",
      "Epoch 45, Iter 400, Loss 2.4239938259124756\n",
      "Epoch 45, Iter 500, Loss 2.2568721771240234\n",
      "Epoch 45, Iter 600, Loss 2.525723457336426\n",
      "Epoch 45, Iter 700, Loss 2.468003511428833\n",
      "Epoch 45, Iter 800, Loss 2.5562288761138916\n",
      "Epoch 45, Iter 900, Loss 2.4338154792785645\n",
      "Epoch 45, Iter 1000, Loss 2.3506765365600586\n",
      "Epoch 45, Iter 1100, Loss 2.4326179027557373\n",
      "Epoch 45, Iter 1200, Loss 2.4648585319519043\n",
      "Epoch 45, Iter 1300, Loss 2.4766812324523926\n",
      "Epoch 45, Iter 1400, Loss 2.548030138015747\n",
      "Epoch 45, Iter 1500, Loss 2.2654714584350586\n",
      "Epoch 45, Iter 1600, Loss 2.6169686317443848\n",
      "Epoch 45, Iter 1700, Loss 2.5053045749664307\n",
      "Epoch 45, Iter 1800, Loss 2.541510581970215\n",
      "Epoch 45, Iter 1900, Loss 2.3605077266693115\n",
      "Epoch 45, Iter 2000, Loss 2.5057713985443115\n",
      "Epoch 45, Iter 2100, Loss 2.3971805572509766\n",
      "Epoch 45, Iter 2200, Loss 2.6345226764678955\n",
      "Epoch 45, Iter 2300, Loss 2.4643349647521973\n",
      "Epoch 45, Iter 2400, Loss 2.524083375930786\n",
      "Epoch 45, Val Loss 0.9531447887420654\n",
      "Model saved as crnn_512_1024_0.01_64_45_concat.pth\n",
      "Epoch 46, Iter 0, Loss 2.322395086288452\n",
      "Epoch 46, Iter 100, Loss 2.374096393585205\n",
      "Epoch 46, Iter 200, Loss 2.4000210762023926\n",
      "Epoch 46, Iter 300, Loss 2.384209394454956\n",
      "Epoch 46, Iter 400, Loss 2.502216339111328\n",
      "Epoch 46, Iter 500, Loss 2.687758207321167\n",
      "Epoch 46, Iter 600, Loss 2.5754528045654297\n",
      "Epoch 46, Iter 700, Loss 2.459164619445801\n",
      "Epoch 46, Iter 800, Loss 2.502915382385254\n",
      "Epoch 46, Iter 900, Loss 2.452268362045288\n",
      "Epoch 46, Iter 1000, Loss 2.4107730388641357\n",
      "Epoch 46, Iter 1100, Loss 2.348243474960327\n",
      "Epoch 46, Iter 1200, Loss 2.55367112159729\n",
      "Epoch 46, Iter 1300, Loss 2.4002037048339844\n",
      "Epoch 46, Iter 1400, Loss 2.4439971446990967\n",
      "Epoch 46, Iter 1500, Loss 2.4464800357818604\n",
      "Epoch 46, Iter 1600, Loss 2.5489532947540283\n",
      "Epoch 46, Iter 1700, Loss 2.3831562995910645\n",
      "Epoch 46, Iter 1800, Loss 2.3761351108551025\n",
      "Epoch 46, Iter 1900, Loss 2.3564629554748535\n",
      "Epoch 46, Iter 2000, Loss 2.449291706085205\n",
      "Epoch 46, Iter 2100, Loss 2.5081284046173096\n",
      "Epoch 46, Iter 2200, Loss 2.4329657554626465\n",
      "Epoch 46, Iter 2300, Loss 2.3816235065460205\n",
      "Epoch 46, Iter 2400, Loss 2.6167190074920654\n",
      "Epoch 46, Val Loss 0.9346185326576233\n",
      "Model saved as crnn_512_1024_0.01_64_46_concat.pth\n",
      "Epoch 47, Iter 0, Loss 2.3525712490081787\n",
      "Epoch 47, Iter 100, Loss 2.3931217193603516\n",
      "Epoch 47, Iter 200, Loss 2.566499710083008\n",
      "Epoch 47, Iter 300, Loss 2.5317399501800537\n",
      "Epoch 47, Iter 400, Loss 2.312905788421631\n",
      "Epoch 47, Iter 500, Loss 2.558117151260376\n",
      "Epoch 47, Iter 600, Loss 2.4445362091064453\n",
      "Epoch 47, Iter 700, Loss 2.444209337234497\n",
      "Epoch 47, Iter 800, Loss 2.522829294204712\n",
      "Epoch 47, Iter 900, Loss 2.4675557613372803\n",
      "Epoch 47, Iter 1000, Loss 2.335314989089966\n",
      "Epoch 47, Iter 1100, Loss 2.45625638961792\n",
      "Epoch 47, Iter 1200, Loss 2.487614154815674\n",
      "Epoch 47, Iter 1300, Loss 2.3401377201080322\n",
      "Epoch 47, Iter 1400, Loss 2.3809614181518555\n",
      "Epoch 47, Iter 1500, Loss 2.4040026664733887\n",
      "Epoch 47, Iter 1600, Loss 2.5144248008728027\n",
      "Epoch 47, Iter 1700, Loss 2.412992238998413\n",
      "Epoch 47, Iter 1800, Loss 2.313938856124878\n",
      "Epoch 47, Iter 1900, Loss 2.330443859100342\n",
      "Epoch 47, Iter 2000, Loss 2.294306755065918\n",
      "Epoch 47, Iter 2100, Loss 2.2996437549591064\n",
      "Epoch 47, Iter 2200, Loss 2.5836591720581055\n",
      "Epoch 47, Iter 2300, Loss 2.492260456085205\n",
      "Epoch 47, Iter 2400, Loss 2.633518695831299\n",
      "Epoch 47, Val Loss 0.9262177348136902\n",
      "Model saved as crnn_512_1024_0.01_64_47_concat.pth\n",
      "Epoch 48, Iter 0, Loss 2.309638261795044\n",
      "Epoch 48, Iter 100, Loss 2.372793674468994\n",
      "Epoch 48, Iter 200, Loss 2.494342088699341\n",
      "Epoch 48, Iter 300, Loss 2.423671245574951\n",
      "Epoch 48, Iter 400, Loss 2.553804874420166\n",
      "Epoch 48, Iter 500, Loss 2.4160220623016357\n",
      "Epoch 48, Iter 600, Loss 2.54616117477417\n",
      "Epoch 48, Iter 700, Loss 2.2987334728240967\n",
      "Epoch 48, Iter 800, Loss 2.4931371212005615\n",
      "Epoch 48, Iter 900, Loss 2.4109208583831787\n",
      "Epoch 48, Iter 1000, Loss 2.405674695968628\n",
      "Epoch 48, Iter 1100, Loss 2.3806254863739014\n",
      "Epoch 48, Iter 1200, Loss 2.3579695224761963\n",
      "Epoch 48, Iter 1300, Loss 2.534634590148926\n",
      "Epoch 48, Iter 1400, Loss 2.54870867729187\n",
      "Epoch 48, Iter 1500, Loss 2.4351816177368164\n",
      "Epoch 48, Iter 1600, Loss 2.33784818649292\n",
      "Epoch 48, Iter 1700, Loss 2.4574966430664062\n",
      "Epoch 48, Iter 1800, Loss 2.523317337036133\n",
      "Epoch 48, Iter 1900, Loss 2.262150764465332\n",
      "Epoch 48, Iter 2000, Loss 2.5461833477020264\n",
      "Epoch 48, Iter 2100, Loss 2.450826644897461\n",
      "Epoch 48, Iter 2200, Loss 2.5625226497650146\n",
      "Epoch 48, Iter 2300, Loss 2.3316545486450195\n",
      "Epoch 48, Iter 2400, Loss 2.3608789443969727\n",
      "Epoch 48, Val Loss 0.918734610080719\n",
      "Model saved as crnn_512_1024_0.01_64_48_concat.pth\n",
      "Epoch 49, Iter 0, Loss 2.31643009185791\n",
      "Epoch 49, Iter 100, Loss 2.475558042526245\n",
      "Epoch 49, Iter 200, Loss 2.3623762130737305\n",
      "Epoch 49, Iter 300, Loss 2.4549789428710938\n",
      "Epoch 49, Iter 400, Loss 2.4081389904022217\n",
      "Epoch 49, Iter 500, Loss 2.384995698928833\n",
      "Epoch 49, Iter 600, Loss 2.4020111560821533\n",
      "Epoch 49, Iter 700, Loss 2.439002752304077\n",
      "Epoch 49, Iter 800, Loss 2.4256532192230225\n",
      "Epoch 49, Iter 900, Loss 2.357388973236084\n",
      "Epoch 49, Iter 1000, Loss 2.5163164138793945\n",
      "Epoch 49, Iter 1100, Loss 2.4390671253204346\n",
      "Epoch 49, Iter 1200, Loss 2.2316393852233887\n",
      "Epoch 49, Iter 1300, Loss 2.3325092792510986\n",
      "Epoch 49, Iter 1400, Loss 2.39839768409729\n",
      "Epoch 49, Iter 1500, Loss 2.5543649196624756\n",
      "Epoch 49, Iter 1600, Loss 2.365387201309204\n",
      "Epoch 49, Iter 1700, Loss 2.5095620155334473\n",
      "Epoch 49, Iter 1800, Loss 2.4089200496673584\n",
      "Epoch 49, Iter 1900, Loss 2.535594940185547\n",
      "Epoch 49, Iter 2000, Loss 2.339200019836426\n",
      "Epoch 49, Iter 2100, Loss 2.286609172821045\n",
      "Epoch 49, Iter 2200, Loss 2.4558663368225098\n",
      "Epoch 49, Iter 2300, Loss 2.594698667526245\n",
      "Epoch 49, Iter 2400, Loss 2.398256540298462\n",
      "Epoch 49, Val Loss 0.9192333221435547\n",
      "Model saved as crnn_512_1024_0.01_64_49_concat.pth\n",
      "Epoch 50, Iter 0, Loss 2.363978147506714\n",
      "Epoch 50, Iter 100, Loss 2.484710693359375\n",
      "Epoch 50, Iter 200, Loss 2.3802566528320312\n",
      "Epoch 50, Iter 300, Loss 2.499612808227539\n",
      "Epoch 50, Iter 400, Loss 2.507660388946533\n",
      "Epoch 50, Iter 500, Loss 2.1954667568206787\n",
      "Epoch 50, Iter 600, Loss 2.406646251678467\n",
      "Epoch 50, Iter 700, Loss 2.299602508544922\n",
      "Epoch 50, Iter 800, Loss 2.3882486820220947\n",
      "Epoch 50, Iter 900, Loss 2.367018938064575\n",
      "Epoch 50, Iter 1000, Loss 2.415153980255127\n",
      "Epoch 50, Iter 1100, Loss 2.3614866733551025\n",
      "Epoch 50, Iter 1200, Loss 2.3403186798095703\n",
      "Epoch 50, Iter 1300, Loss 2.4444754123687744\n",
      "Epoch 50, Iter 1400, Loss 2.3864636421203613\n",
      "Epoch 50, Iter 1500, Loss 2.260127305984497\n",
      "Epoch 50, Iter 1600, Loss 2.457240581512451\n",
      "Epoch 50, Iter 1700, Loss 2.316676616668701\n",
      "Epoch 50, Iter 1800, Loss 2.4216806888580322\n",
      "Epoch 50, Iter 1900, Loss 2.4217844009399414\n",
      "Epoch 50, Iter 2000, Loss 2.3704006671905518\n",
      "Epoch 50, Iter 2100, Loss 2.498251438140869\n",
      "Epoch 50, Iter 2200, Loss 2.514336585998535\n",
      "Epoch 50, Iter 2300, Loss 2.2715983390808105\n",
      "Epoch 50, Iter 2400, Loss 2.4810357093811035\n",
      "Epoch 50, Val Loss 0.9088200330734253\n",
      "Model saved as crnn_512_1024_0.01_64_50_concat.pth\n",
      "Epoch 51, Iter 0, Loss 2.3167927265167236\n",
      "Epoch 51, Iter 100, Loss 2.2753961086273193\n",
      "Epoch 51, Iter 200, Loss 2.3906707763671875\n",
      "Epoch 51, Iter 300, Loss 2.551158905029297\n",
      "Epoch 51, Iter 400, Loss 2.401542901992798\n",
      "Epoch 51, Iter 500, Loss 2.4328622817993164\n",
      "Epoch 51, Iter 600, Loss 2.3426287174224854\n",
      "Epoch 51, Iter 700, Loss 2.548253297805786\n",
      "Epoch 51, Iter 800, Loss 2.37131667137146\n",
      "Epoch 51, Iter 900, Loss 2.5042030811309814\n",
      "Epoch 51, Iter 1000, Loss 2.242488145828247\n",
      "Epoch 51, Iter 1100, Loss 2.464933395385742\n",
      "Epoch 51, Iter 1200, Loss 2.4339654445648193\n",
      "Epoch 51, Iter 1300, Loss 2.3984451293945312\n",
      "Epoch 51, Iter 1400, Loss 2.4066176414489746\n",
      "Epoch 51, Iter 1500, Loss 2.477210521697998\n",
      "Epoch 51, Iter 1600, Loss 2.496755838394165\n",
      "Epoch 51, Iter 1700, Loss 2.251591444015503\n",
      "Epoch 51, Iter 1800, Loss 2.436528444290161\n",
      "Epoch 51, Iter 1900, Loss 2.300004243850708\n",
      "Epoch 51, Iter 2000, Loss 2.426244020462036\n",
      "Epoch 51, Iter 2100, Loss 2.2851200103759766\n",
      "Epoch 51, Iter 2200, Loss 2.4815311431884766\n",
      "Epoch 51, Iter 2300, Loss 2.255284309387207\n",
      "Epoch 51, Iter 2400, Loss 2.4842934608459473\n",
      "Epoch 51, Val Loss 0.9063693881034851\n",
      "Model saved as crnn_512_1024_0.01_64_51_concat.pth\n",
      "Epoch 52, Iter 0, Loss 2.530193328857422\n",
      "Epoch 52, Iter 100, Loss 2.237816333770752\n",
      "Epoch 52, Iter 200, Loss 2.55342698097229\n",
      "Epoch 52, Iter 300, Loss 2.4985978603363037\n",
      "Epoch 52, Iter 400, Loss 2.3042333126068115\n",
      "Epoch 52, Iter 500, Loss 2.3709044456481934\n",
      "Epoch 52, Iter 600, Loss 2.4456021785736084\n",
      "Epoch 52, Iter 700, Loss 2.5654287338256836\n",
      "Epoch 52, Iter 800, Loss 2.373795986175537\n",
      "Epoch 52, Iter 900, Loss 2.3911657333374023\n",
      "Epoch 52, Iter 1000, Loss 2.3421213626861572\n",
      "Epoch 52, Iter 1100, Loss 2.5477662086486816\n",
      "Epoch 52, Iter 1200, Loss 2.578320026397705\n",
      "Epoch 52, Iter 1300, Loss 2.3717706203460693\n",
      "Epoch 52, Iter 1400, Loss 2.4644484519958496\n",
      "Epoch 52, Iter 1500, Loss 2.2811899185180664\n",
      "Epoch 52, Iter 1600, Loss 2.4914116859436035\n",
      "Epoch 52, Iter 1700, Loss 2.391094923019409\n",
      "Epoch 52, Iter 1800, Loss 2.286940336227417\n",
      "Epoch 52, Iter 1900, Loss 2.4286293983459473\n",
      "Epoch 52, Iter 2000, Loss 2.5116159915924072\n",
      "Epoch 52, Iter 2100, Loss 2.378469467163086\n",
      "Epoch 52, Iter 2200, Loss 2.3446109294891357\n",
      "Epoch 52, Iter 2300, Loss 2.3367793560028076\n",
      "Epoch 52, Iter 2400, Loss 2.124742031097412\n",
      "Epoch 52, Val Loss 0.9046518206596375\n",
      "Model saved as crnn_512_1024_0.01_64_52_concat.pth\n",
      "Epoch 53, Iter 0, Loss 2.453505754470825\n",
      "Epoch 53, Iter 100, Loss 2.3868868350982666\n",
      "Epoch 53, Iter 200, Loss 2.3571767807006836\n",
      "Epoch 53, Iter 300, Loss 2.2763922214508057\n",
      "Epoch 53, Iter 400, Loss 2.45182466506958\n",
      "Epoch 53, Iter 500, Loss 2.3640553951263428\n",
      "Epoch 53, Iter 600, Loss 2.2694473266601562\n",
      "Epoch 53, Iter 700, Loss 2.3727145195007324\n",
      "Epoch 53, Iter 800, Loss 2.369396448135376\n",
      "Epoch 53, Iter 900, Loss 2.280348777770996\n",
      "Epoch 53, Iter 1000, Loss 2.4321351051330566\n",
      "Epoch 53, Iter 1100, Loss 2.2952301502227783\n",
      "Epoch 53, Iter 1200, Loss 2.2678916454315186\n",
      "Epoch 53, Iter 1300, Loss 2.3372669219970703\n",
      "Epoch 53, Iter 1400, Loss 2.578723907470703\n",
      "Epoch 53, Iter 1500, Loss 2.401510715484619\n",
      "Epoch 53, Iter 1600, Loss 2.3898720741271973\n",
      "Epoch 53, Iter 1700, Loss 2.368234872817993\n",
      "Epoch 53, Iter 1800, Loss 2.4987993240356445\n",
      "Epoch 53, Iter 1900, Loss 2.4803524017333984\n",
      "Epoch 53, Iter 2000, Loss 2.378854990005493\n",
      "Epoch 53, Iter 2100, Loss 2.2892675399780273\n",
      "Epoch 53, Iter 2200, Loss 2.3716797828674316\n",
      "Epoch 53, Iter 2300, Loss 2.466491937637329\n",
      "Epoch 53, Iter 2400, Loss 2.4407808780670166\n",
      "Epoch 53, Val Loss 0.9028154611587524\n",
      "Model saved as crnn_512_1024_0.01_64_53_concat.pth\n",
      "Epoch 54, Iter 0, Loss 2.4263293743133545\n",
      "Epoch 54, Iter 100, Loss 2.312208414077759\n",
      "Epoch 54, Iter 200, Loss 2.381026268005371\n",
      "Epoch 54, Iter 300, Loss 2.28710675239563\n",
      "Epoch 54, Iter 400, Loss 2.463341236114502\n",
      "Epoch 54, Iter 500, Loss 2.437323808670044\n",
      "Epoch 54, Iter 600, Loss 2.5190324783325195\n",
      "Epoch 54, Iter 700, Loss 2.447249174118042\n",
      "Epoch 54, Iter 800, Loss 2.3574559688568115\n",
      "Epoch 54, Iter 900, Loss 2.320108413696289\n",
      "Epoch 54, Iter 1000, Loss 2.335749387741089\n",
      "Epoch 54, Iter 1100, Loss 2.321927309036255\n",
      "Epoch 54, Iter 1200, Loss 2.292163848876953\n",
      "Epoch 54, Iter 1300, Loss 2.3476109504699707\n",
      "Epoch 54, Iter 1400, Loss 2.3553028106689453\n",
      "Epoch 54, Iter 1500, Loss 2.3041868209838867\n",
      "Epoch 54, Iter 1600, Loss 2.472419261932373\n",
      "Epoch 54, Iter 1700, Loss 2.3078811168670654\n",
      "Epoch 54, Iter 1800, Loss 2.3372957706451416\n",
      "Epoch 54, Iter 1900, Loss 2.211883544921875\n",
      "Epoch 54, Iter 2000, Loss 2.4368038177490234\n",
      "Epoch 54, Iter 2100, Loss 2.3569514751434326\n",
      "Epoch 54, Iter 2200, Loss 2.3837156295776367\n",
      "Epoch 54, Iter 2300, Loss 2.332810163497925\n",
      "Epoch 54, Iter 2400, Loss 2.4556963443756104\n",
      "Epoch 54, Val Loss 0.9015923738479614\n",
      "Model saved as crnn_512_1024_0.01_64_54_concat.pth\n",
      "Epoch 55, Iter 0, Loss 2.314988374710083\n",
      "Epoch 55, Iter 100, Loss 2.579352378845215\n",
      "Epoch 55, Iter 200, Loss 2.4676828384399414\n",
      "Epoch 55, Iter 300, Loss 2.3438172340393066\n",
      "Epoch 55, Iter 400, Loss 2.3977928161621094\n",
      "Epoch 55, Iter 500, Loss 2.4630491733551025\n",
      "Epoch 55, Iter 600, Loss 2.440791368484497\n",
      "Epoch 55, Iter 700, Loss 2.4839210510253906\n",
      "Epoch 55, Iter 800, Loss 2.420295238494873\n",
      "Epoch 55, Iter 900, Loss 2.471970319747925\n",
      "Epoch 55, Iter 1000, Loss 2.4293720722198486\n",
      "Epoch 55, Iter 1100, Loss 2.4563372135162354\n",
      "Epoch 55, Iter 1200, Loss 2.350841522216797\n",
      "Epoch 55, Iter 1300, Loss 2.638547420501709\n",
      "Epoch 55, Iter 1400, Loss 2.2790114879608154\n",
      "Epoch 55, Iter 1500, Loss 2.4417059421539307\n",
      "Epoch 55, Iter 1600, Loss 2.4690303802490234\n",
      "Epoch 55, Iter 1700, Loss 2.4657959938049316\n",
      "Epoch 55, Iter 1800, Loss 2.4416537284851074\n",
      "Epoch 55, Iter 1900, Loss 2.4237911701202393\n",
      "Epoch 55, Iter 2000, Loss 2.444376230239868\n",
      "Epoch 55, Iter 2100, Loss 2.416736125946045\n",
      "Epoch 55, Iter 2200, Loss 2.3224666118621826\n",
      "Epoch 55, Iter 2300, Loss 2.3990530967712402\n",
      "Epoch 55, Iter 2400, Loss 2.3457422256469727\n",
      "Epoch 55, Val Loss 0.9006870985031128\n",
      "Model saved as crnn_512_1024_0.01_64_55_concat.pth\n",
      "Epoch 56, Iter 0, Loss 2.2475903034210205\n",
      "Epoch 56, Iter 100, Loss 2.4019405841827393\n",
      "Epoch 56, Iter 200, Loss 2.461671829223633\n",
      "Epoch 56, Iter 300, Loss 2.349102735519409\n",
      "Epoch 56, Iter 400, Loss 2.3983898162841797\n",
      "Epoch 56, Iter 500, Loss 2.3930978775024414\n",
      "Epoch 56, Iter 600, Loss 2.390782356262207\n",
      "Epoch 56, Iter 700, Loss 2.358898401260376\n",
      "Epoch 56, Iter 800, Loss 2.4658918380737305\n",
      "Epoch 56, Iter 900, Loss 2.4161417484283447\n",
      "Epoch 56, Iter 1000, Loss 2.2715823650360107\n",
      "Epoch 56, Iter 1100, Loss 2.4078338146209717\n",
      "Epoch 56, Iter 1200, Loss 2.590599298477173\n",
      "Epoch 56, Iter 1300, Loss 2.4514119625091553\n",
      "Epoch 56, Iter 1400, Loss 2.393388271331787\n",
      "Epoch 56, Iter 1500, Loss 2.2776856422424316\n",
      "Epoch 56, Iter 1600, Loss 2.5216710567474365\n",
      "Epoch 56, Iter 1700, Loss 2.3267064094543457\n",
      "Epoch 56, Iter 1800, Loss 2.4064133167266846\n",
      "Epoch 56, Iter 1900, Loss 2.38844895362854\n",
      "Epoch 56, Iter 2000, Loss 2.2957122325897217\n",
      "Epoch 56, Iter 2100, Loss 2.341675043106079\n",
      "Epoch 56, Iter 2200, Loss 2.598518133163452\n",
      "Epoch 56, Iter 2300, Loss 2.292262315750122\n",
      "Epoch 56, Iter 2400, Loss 2.2251927852630615\n",
      "Epoch 56, Val Loss 0.9025641083717346\n",
      "Model saved as crnn_512_1024_0.01_64_56_concat.pth\n",
      "Epoch 57, Iter 0, Loss 2.4269144535064697\n",
      "Epoch 57, Iter 100, Loss 2.467287302017212\n",
      "Epoch 57, Iter 200, Loss 2.3519043922424316\n",
      "Epoch 57, Iter 300, Loss 2.3183279037475586\n",
      "Epoch 57, Iter 400, Loss 2.566707134246826\n",
      "Epoch 57, Iter 500, Loss 2.2285614013671875\n",
      "Epoch 57, Iter 600, Loss 2.3285253047943115\n",
      "Epoch 57, Iter 700, Loss 2.6143009662628174\n",
      "Epoch 57, Iter 800, Loss 2.376692295074463\n",
      "Epoch 57, Iter 900, Loss 2.6768712997436523\n",
      "Epoch 57, Iter 1000, Loss 2.4941868782043457\n",
      "Epoch 57, Iter 1100, Loss 2.378331184387207\n",
      "Epoch 57, Iter 1200, Loss 2.5347788333892822\n",
      "Epoch 57, Iter 1300, Loss 2.454716682434082\n",
      "Epoch 57, Iter 1400, Loss 2.2627625465393066\n",
      "Epoch 57, Iter 1500, Loss 2.4949090480804443\n",
      "Epoch 57, Iter 1600, Loss 2.322178602218628\n",
      "Epoch 57, Iter 1700, Loss 2.4255988597869873\n",
      "Epoch 57, Iter 1800, Loss 2.3277101516723633\n",
      "Epoch 57, Iter 1900, Loss 2.414398431777954\n",
      "Epoch 57, Iter 2000, Loss 2.4987776279449463\n",
      "Epoch 57, Iter 2100, Loss 2.32483172416687\n",
      "Epoch 57, Iter 2200, Loss 2.5131092071533203\n",
      "Epoch 57, Iter 2300, Loss 2.5844054222106934\n",
      "Epoch 57, Iter 2400, Loss 2.331836462020874\n",
      "Epoch 57, Val Loss 0.8975390195846558\n",
      "Model saved as crnn_512_1024_0.01_64_57_concat.pth\n",
      "Epoch 58, Iter 0, Loss 2.5160043239593506\n",
      "Epoch 58, Iter 100, Loss 2.415195941925049\n",
      "Epoch 58, Iter 200, Loss 2.310612678527832\n",
      "Epoch 58, Iter 300, Loss 2.339942216873169\n",
      "Epoch 58, Iter 400, Loss 2.41972017288208\n",
      "Epoch 58, Iter 500, Loss 2.353705406188965\n",
      "Epoch 58, Iter 600, Loss 2.447040319442749\n",
      "Epoch 58, Iter 700, Loss 2.43245530128479\n",
      "Epoch 58, Iter 800, Loss 2.48478102684021\n",
      "Epoch 58, Iter 900, Loss 2.3531112670898438\n",
      "Epoch 58, Iter 1000, Loss 2.2768759727478027\n",
      "Epoch 58, Iter 1100, Loss 2.4982106685638428\n",
      "Epoch 58, Iter 1200, Loss 2.3114101886749268\n",
      "Epoch 58, Iter 1300, Loss 2.4376935958862305\n",
      "Epoch 58, Iter 1400, Loss 2.5076887607574463\n",
      "Epoch 58, Iter 1500, Loss 2.344583511352539\n",
      "Epoch 58, Iter 1600, Loss 2.3313446044921875\n",
      "Epoch 58, Iter 1700, Loss 2.5741829872131348\n",
      "Epoch 58, Iter 1800, Loss 2.3610074520111084\n",
      "Epoch 58, Iter 1900, Loss 2.4511353969573975\n",
      "Epoch 58, Iter 2000, Loss 2.45534086227417\n",
      "Epoch 58, Iter 2100, Loss 2.4284627437591553\n",
      "Epoch 58, Iter 2200, Loss 2.303562879562378\n",
      "Epoch 58, Iter 2300, Loss 2.4418864250183105\n",
      "Epoch 58, Iter 2400, Loss 2.3180313110351562\n",
      "Epoch 58, Val Loss 0.9002237915992737\n",
      "Model saved as crnn_512_1024_0.01_64_58_concat.pth\n",
      "Epoch 59, Iter 0, Loss 2.426036834716797\n",
      "Epoch 59, Iter 100, Loss 2.345792055130005\n",
      "Epoch 59, Iter 200, Loss 2.2439916133880615\n",
      "Epoch 59, Iter 300, Loss 2.4989399909973145\n",
      "Epoch 59, Iter 400, Loss 2.4213409423828125\n",
      "Epoch 59, Iter 500, Loss 2.308525800704956\n",
      "Epoch 59, Iter 600, Loss 2.379142999649048\n",
      "Epoch 59, Iter 700, Loss 2.322638750076294\n",
      "Epoch 59, Iter 800, Loss 2.4639999866485596\n",
      "Epoch 59, Iter 900, Loss 2.4364309310913086\n",
      "Epoch 59, Iter 1000, Loss 2.433069944381714\n",
      "Epoch 59, Iter 1100, Loss 2.3779287338256836\n",
      "Epoch 59, Iter 1200, Loss 2.326432943344116\n",
      "Epoch 59, Iter 1300, Loss 2.086155414581299\n",
      "Epoch 59, Iter 1400, Loss 2.3785548210144043\n",
      "Epoch 59, Iter 1500, Loss 2.3378779888153076\n",
      "Epoch 59, Iter 1600, Loss 2.453054189682007\n",
      "Epoch 59, Iter 1700, Loss 2.3068277835845947\n",
      "Epoch 59, Iter 1800, Loss 2.2778244018554688\n",
      "Epoch 59, Iter 1900, Loss 2.371150493621826\n",
      "Epoch 59, Iter 2000, Loss 2.4536187648773193\n",
      "Epoch 59, Iter 2100, Loss 2.4153170585632324\n",
      "Epoch 59, Iter 2200, Loss 2.528547763824463\n",
      "Epoch 59, Iter 2300, Loss 2.423734664916992\n",
      "Epoch 59, Iter 2400, Loss 2.415557622909546\n",
      "Epoch 59, Val Loss 0.8999438285827637\n",
      "Model saved as crnn_512_1024_0.01_64_59_concat.pth\n",
      "Epoch 60, Iter 0, Loss 2.2617392539978027\n",
      "Epoch 60, Iter 100, Loss 2.4428768157958984\n",
      "Epoch 60, Iter 200, Loss 2.1355228424072266\n",
      "Epoch 60, Iter 300, Loss 2.3294386863708496\n",
      "Epoch 60, Iter 400, Loss 2.443162441253662\n",
      "Epoch 60, Iter 500, Loss 2.2283995151519775\n",
      "Epoch 60, Iter 600, Loss 2.2771291732788086\n",
      "Epoch 60, Iter 700, Loss 2.475628614425659\n",
      "Epoch 60, Iter 800, Loss 2.2413456439971924\n",
      "Epoch 60, Iter 900, Loss 2.338013172149658\n",
      "Epoch 60, Iter 1000, Loss 2.401733636856079\n",
      "Epoch 60, Iter 1100, Loss 2.418233633041382\n",
      "Epoch 60, Iter 1200, Loss 2.42803692817688\n",
      "Epoch 60, Iter 1300, Loss 2.4307122230529785\n",
      "Epoch 60, Iter 1400, Loss 2.543222427368164\n",
      "Epoch 60, Iter 1500, Loss 2.3237557411193848\n",
      "Epoch 60, Iter 1600, Loss 2.3357672691345215\n",
      "Epoch 60, Iter 1700, Loss 2.3819446563720703\n",
      "Epoch 60, Iter 1800, Loss 2.318023443222046\n",
      "Epoch 60, Iter 1900, Loss 2.554436206817627\n",
      "Epoch 60, Iter 2000, Loss 2.400195837020874\n",
      "Epoch 60, Iter 2100, Loss 2.266880750656128\n",
      "Epoch 60, Iter 2200, Loss 2.3009233474731445\n",
      "Epoch 60, Iter 2300, Loss 2.535637617111206\n",
      "Epoch 60, Iter 2400, Loss 2.4219324588775635\n",
      "Epoch 60, Val Loss 0.899638831615448\n",
      "Model saved as crnn_512_1024_0.01_64_60_concat.pth\n",
      "Epoch 61, Iter 0, Loss 2.3979711532592773\n",
      "Epoch 61, Iter 100, Loss 2.4232208728790283\n",
      "Epoch 61, Iter 200, Loss 2.297192096710205\n",
      "Epoch 61, Iter 300, Loss 2.2510406970977783\n",
      "Epoch 61, Iter 400, Loss 2.371344566345215\n",
      "Epoch 61, Iter 500, Loss 2.470959424972534\n",
      "Epoch 61, Iter 600, Loss 2.3566629886627197\n",
      "Epoch 61, Iter 700, Loss 2.524733781814575\n",
      "Epoch 61, Iter 800, Loss 2.394227981567383\n",
      "Epoch 61, Iter 900, Loss 2.3411951065063477\n",
      "Epoch 61, Iter 1000, Loss 2.508218765258789\n",
      "Epoch 61, Iter 1100, Loss 2.216237783432007\n",
      "Epoch 61, Iter 1200, Loss 2.322021961212158\n",
      "Epoch 61, Iter 1300, Loss 2.4385085105895996\n",
      "Epoch 61, Iter 1400, Loss 2.428426742553711\n",
      "Epoch 61, Iter 1500, Loss 2.3145525455474854\n",
      "Epoch 61, Iter 1600, Loss 2.3384766578674316\n",
      "Epoch 61, Iter 1700, Loss 2.2596068382263184\n",
      "Epoch 61, Iter 1800, Loss 2.245964765548706\n",
      "Epoch 61, Iter 1900, Loss 2.4336071014404297\n",
      "Epoch 61, Iter 2000, Loss 2.469420909881592\n",
      "Epoch 61, Iter 2100, Loss 2.44219708442688\n",
      "Epoch 61, Iter 2200, Loss 2.3717477321624756\n",
      "Epoch 61, Iter 2300, Loss 2.4589600563049316\n",
      "Epoch 61, Iter 2400, Loss 2.3956196308135986\n",
      "Epoch 61, Val Loss 0.8984193205833435\n",
      "Model saved as crnn_512_1024_0.01_64_61_concat.pth\n",
      "Epoch 62, Iter 0, Loss 2.532620906829834\n",
      "Epoch 62, Iter 100, Loss 2.3871092796325684\n",
      "Epoch 62, Iter 200, Loss 2.582639217376709\n",
      "Epoch 62, Iter 300, Loss 2.3802638053894043\n",
      "Epoch 62, Iter 400, Loss 2.4046168327331543\n",
      "Epoch 62, Iter 500, Loss 2.462874174118042\n",
      "Epoch 62, Iter 600, Loss 2.3951635360717773\n",
      "Epoch 62, Iter 700, Loss 2.2648255825042725\n",
      "Epoch 62, Iter 800, Loss 2.2779059410095215\n",
      "Epoch 62, Iter 900, Loss 2.332925796508789\n",
      "Epoch 62, Iter 1000, Loss 2.265753746032715\n",
      "Epoch 62, Iter 1100, Loss 2.288837432861328\n",
      "Epoch 62, Iter 1200, Loss 2.3306210041046143\n",
      "Epoch 62, Iter 1300, Loss 2.418318748474121\n",
      "Epoch 62, Iter 1400, Loss 2.3235628604888916\n",
      "Epoch 62, Iter 1500, Loss 2.3352527618408203\n",
      "Epoch 62, Iter 1600, Loss 2.4214227199554443\n",
      "Epoch 62, Iter 1700, Loss 2.5333261489868164\n",
      "Epoch 62, Iter 1800, Loss 2.304842233657837\n",
      "Epoch 62, Iter 1900, Loss 2.3486859798431396\n",
      "Epoch 62, Iter 2000, Loss 2.484039783477783\n",
      "Epoch 62, Iter 2100, Loss 2.4573721885681152\n",
      "Epoch 62, Iter 2200, Loss 2.2870397567749023\n",
      "Epoch 62, Iter 2300, Loss 2.3783457279205322\n",
      "Epoch 62, Iter 2400, Loss 2.274733304977417\n",
      "Epoch 62, Val Loss 0.9000622630119324\n",
      "Model saved as crnn_512_1024_0.01_64_62_concat.pth\n",
      "Epoch 63, Iter 0, Loss 2.330416679382324\n",
      "Epoch 63, Iter 100, Loss 2.5703322887420654\n",
      "Epoch 63, Iter 200, Loss 2.421067237854004\n",
      "Epoch 63, Iter 300, Loss 2.4000208377838135\n",
      "Epoch 63, Iter 400, Loss 2.5363240242004395\n",
      "Epoch 63, Iter 500, Loss 2.236969470977783\n",
      "Epoch 63, Iter 600, Loss 2.2282636165618896\n",
      "Epoch 63, Iter 700, Loss 2.3381781578063965\n",
      "Epoch 63, Iter 800, Loss 2.518616199493408\n",
      "Epoch 63, Iter 900, Loss 2.419498920440674\n",
      "Epoch 63, Iter 1000, Loss 2.6592650413513184\n",
      "Epoch 63, Iter 1100, Loss 2.4150984287261963\n",
      "Epoch 63, Iter 1200, Loss 2.4340553283691406\n",
      "Epoch 63, Iter 1300, Loss 2.4965410232543945\n",
      "Epoch 63, Iter 1400, Loss 2.387169599533081\n",
      "Epoch 63, Iter 1500, Loss 2.490842819213867\n",
      "Epoch 63, Iter 1600, Loss 2.460998773574829\n",
      "Epoch 63, Iter 1700, Loss 2.342911720275879\n",
      "Epoch 63, Iter 1800, Loss 2.503927230834961\n",
      "Epoch 63, Iter 1900, Loss 2.3982696533203125\n",
      "Epoch 63, Iter 2000, Loss 2.477130651473999\n",
      "Epoch 63, Iter 2100, Loss 2.2117700576782227\n",
      "Epoch 63, Iter 2200, Loss 2.309859275817871\n",
      "Epoch 63, Iter 2300, Loss 2.1820521354675293\n",
      "Epoch 63, Iter 2400, Loss 2.3318798542022705\n",
      "Epoch 63, Val Loss 0.8990171551704407\n",
      "Model saved as crnn_512_1024_0.01_64_63_concat.pth\n",
      "Epoch 64, Iter 0, Loss 2.481750249862671\n",
      "Epoch 64, Iter 100, Loss 2.4244704246520996\n",
      "Epoch 64, Iter 200, Loss 2.391064167022705\n",
      "Epoch 64, Iter 300, Loss 2.339238166809082\n",
      "Epoch 64, Iter 400, Loss 2.4143788814544678\n",
      "Epoch 64, Iter 500, Loss 2.2971086502075195\n",
      "Epoch 64, Iter 600, Loss 2.3547821044921875\n",
      "Epoch 64, Iter 700, Loss 2.3988425731658936\n",
      "Epoch 64, Iter 800, Loss 2.318669319152832\n",
      "Epoch 64, Iter 900, Loss 2.285141944885254\n",
      "Epoch 64, Iter 1000, Loss 2.4744997024536133\n",
      "Epoch 64, Iter 1100, Loss 2.315594434738159\n",
      "Epoch 64, Iter 1200, Loss 2.5018668174743652\n",
      "Epoch 64, Iter 1300, Loss 2.4550201892852783\n",
      "Epoch 64, Iter 1400, Loss 2.292693853378296\n",
      "Epoch 64, Iter 1500, Loss 2.3206369876861572\n",
      "Epoch 64, Iter 1600, Loss 2.1805813312530518\n",
      "Epoch 64, Iter 1700, Loss 2.085636615753174\n",
      "Epoch 64, Iter 1800, Loss 2.353041648864746\n",
      "Epoch 64, Iter 1900, Loss 2.2199854850769043\n",
      "Epoch 64, Iter 2000, Loss 2.397822380065918\n",
      "Epoch 64, Iter 2100, Loss 2.406315565109253\n",
      "Epoch 64, Iter 2200, Loss 2.5368542671203613\n",
      "Epoch 64, Iter 2300, Loss 2.5136642456054688\n",
      "Epoch 64, Iter 2400, Loss 2.5210893154144287\n",
      "Epoch 64, Val Loss 0.8932902812957764\n",
      "Model saved as crnn_512_1024_0.01_64_64_concat.pth\n",
      "Epoch 65, Iter 0, Loss 2.3596315383911133\n",
      "Epoch 65, Iter 100, Loss 2.4156742095947266\n",
      "Epoch 65, Iter 200, Loss 2.340034246444702\n",
      "Epoch 65, Iter 300, Loss 2.4339284896850586\n",
      "Epoch 65, Iter 400, Loss 2.377755641937256\n",
      "Epoch 65, Iter 500, Loss 2.5480659008026123\n",
      "Epoch 65, Iter 600, Loss 2.2273807525634766\n",
      "Epoch 65, Iter 700, Loss 2.3666598796844482\n",
      "Epoch 65, Iter 800, Loss 2.3273985385894775\n",
      "Epoch 65, Iter 900, Loss 2.296977996826172\n",
      "Epoch 65, Iter 1000, Loss 2.5892858505249023\n",
      "Epoch 65, Iter 1100, Loss 2.247401475906372\n",
      "Epoch 65, Iter 1200, Loss 2.425952911376953\n",
      "Epoch 65, Iter 1300, Loss 2.257718563079834\n",
      "Epoch 65, Iter 1400, Loss 2.534304141998291\n",
      "Epoch 65, Iter 1500, Loss 2.298586130142212\n",
      "Epoch 65, Iter 1600, Loss 2.388416290283203\n",
      "Epoch 65, Iter 1700, Loss 2.3164174556732178\n",
      "Epoch 65, Iter 1800, Loss 2.306929588317871\n",
      "Epoch 65, Iter 1900, Loss 2.3862011432647705\n",
      "Epoch 65, Iter 2000, Loss 2.3350319862365723\n",
      "Epoch 65, Iter 2100, Loss 2.26363205909729\n",
      "Epoch 65, Iter 2200, Loss 2.5044240951538086\n",
      "Epoch 65, Iter 2300, Loss 2.5232961177825928\n",
      "Epoch 65, Iter 2400, Loss 2.3010284900665283\n",
      "Epoch 65, Val Loss 0.893732488155365\n",
      "Model saved as crnn_512_1024_0.01_64_65_concat.pth\n",
      "Epoch 66, Iter 0, Loss 2.422636032104492\n",
      "Epoch 66, Iter 100, Loss 2.245147943496704\n",
      "Epoch 66, Iter 200, Loss 2.23238468170166\n",
      "Epoch 66, Iter 300, Loss 2.3365468978881836\n",
      "Epoch 66, Iter 400, Loss 2.4193761348724365\n",
      "Epoch 66, Iter 500, Loss 2.2375359535217285\n",
      "Epoch 66, Iter 600, Loss 2.306238889694214\n",
      "Epoch 66, Iter 700, Loss 2.43825101852417\n",
      "Epoch 66, Iter 800, Loss 2.3277413845062256\n",
      "Epoch 66, Iter 900, Loss 2.2180094718933105\n",
      "Epoch 66, Iter 1000, Loss 2.353543281555176\n",
      "Epoch 66, Iter 1100, Loss 2.3881304264068604\n",
      "Epoch 66, Iter 1200, Loss 2.398155689239502\n",
      "Epoch 66, Iter 1300, Loss 2.409325122833252\n",
      "Epoch 66, Iter 1400, Loss 2.315433979034424\n",
      "Epoch 66, Iter 1500, Loss 2.2818713188171387\n",
      "Epoch 66, Iter 1600, Loss 2.2619855403900146\n",
      "Epoch 66, Iter 1700, Loss 2.579061508178711\n",
      "Epoch 66, Iter 1800, Loss 2.36358904838562\n",
      "Epoch 66, Iter 1900, Loss 2.2755966186523438\n",
      "Epoch 66, Iter 2000, Loss 2.365708827972412\n",
      "Epoch 66, Iter 2100, Loss 2.333458423614502\n",
      "Epoch 66, Iter 2200, Loss 2.374403238296509\n",
      "Epoch 66, Iter 2300, Loss 2.2769553661346436\n",
      "Epoch 66, Iter 2400, Loss 2.497384786605835\n",
      "Epoch 66, Val Loss 0.8927547335624695\n",
      "Model saved as crnn_512_1024_0.01_64_66_concat.pth\n",
      "Epoch 67, Iter 0, Loss 2.4384284019470215\n",
      "Epoch 67, Iter 100, Loss 2.259958028793335\n",
      "Epoch 67, Iter 200, Loss 2.393033027648926\n",
      "Epoch 67, Iter 300, Loss 2.432351589202881\n",
      "Epoch 67, Iter 400, Loss 2.3340373039245605\n",
      "Epoch 67, Iter 500, Loss 2.3044984340667725\n",
      "Epoch 67, Iter 600, Loss 2.3252956867218018\n",
      "Epoch 67, Iter 700, Loss 2.429280996322632\n",
      "Epoch 67, Iter 800, Loss 2.461052894592285\n",
      "Epoch 67, Iter 900, Loss 2.495866537094116\n",
      "Epoch 67, Iter 1000, Loss 2.3252532482147217\n",
      "Epoch 67, Iter 1100, Loss 2.398627281188965\n",
      "Epoch 67, Iter 1200, Loss 2.6361095905303955\n",
      "Epoch 67, Iter 1300, Loss 2.4546427726745605\n",
      "Epoch 67, Iter 1400, Loss 2.35067081451416\n",
      "Epoch 67, Iter 1500, Loss 2.2815089225769043\n",
      "Epoch 67, Iter 1600, Loss 2.399282455444336\n",
      "Epoch 67, Iter 1700, Loss 2.4084997177124023\n",
      "Epoch 67, Iter 1800, Loss 2.379218816757202\n",
      "Epoch 67, Iter 1900, Loss 2.3446497917175293\n",
      "Epoch 67, Iter 2000, Loss 2.2003438472747803\n",
      "Epoch 67, Iter 2100, Loss 2.4380483627319336\n",
      "Epoch 67, Iter 2200, Loss 2.357422113418579\n",
      "Epoch 67, Iter 2300, Loss 2.3776519298553467\n",
      "Epoch 67, Iter 2400, Loss 2.351797103881836\n",
      "Epoch 67, Val Loss 0.8921177387237549\n",
      "Model saved as crnn_512_1024_0.01_64_67_concat.pth\n",
      "Epoch 68, Iter 0, Loss 2.5125231742858887\n",
      "Epoch 68, Iter 100, Loss 2.472559928894043\n",
      "Epoch 68, Iter 200, Loss 2.460134506225586\n",
      "Epoch 68, Iter 300, Loss 2.3604214191436768\n",
      "Epoch 68, Iter 400, Loss 2.454941511154175\n",
      "Epoch 68, Iter 500, Loss 2.272479772567749\n",
      "Epoch 68, Iter 600, Loss 2.199915647506714\n",
      "Epoch 68, Iter 700, Loss 2.3664963245391846\n",
      "Epoch 68, Iter 800, Loss 2.19085955619812\n",
      "Epoch 68, Iter 900, Loss 2.5669212341308594\n",
      "Epoch 68, Iter 1000, Loss 2.2463724613189697\n",
      "Epoch 68, Iter 1100, Loss 2.269287347793579\n",
      "Epoch 68, Iter 1200, Loss 2.3249669075012207\n",
      "Epoch 68, Iter 1300, Loss 2.4253299236297607\n",
      "Epoch 68, Iter 1400, Loss 2.407932996749878\n",
      "Epoch 68, Iter 1500, Loss 2.5329370498657227\n",
      "Epoch 68, Iter 1600, Loss 2.4665334224700928\n",
      "Epoch 68, Iter 1700, Loss 2.359891653060913\n",
      "Epoch 68, Iter 1800, Loss 2.4945387840270996\n",
      "Epoch 68, Iter 1900, Loss 2.1640167236328125\n",
      "Epoch 68, Iter 2000, Loss 2.365879774093628\n"
     ]
    }
   ],
   "source": [
    "# 可能要调的超参数有：hidden_dim, io_dim, lr, batch_size, num_epochs, dataset_name\n",
    "def get_model_name(hidden_dim, io_dim, lr, batch_size, num_epochs, dataset_name):\n",
    "    return f\"crnn_{hidden_dim}_{io_dim}_{lr}_{batch_size}_{num_epochs}_{dataset_name}.pth\"\n",
    "\n",
    "\n",
    "def get_val_loss(model, dataloader, device):\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
    "    model.eval()\n",
    "    loss = 0\n",
    "    with torch.no_grad():\n",
    "        for step, (img, target) in enumerate(dataloader):\n",
    "            img, target = img.to(device), target.to(device)\n",
    "            h_n, c_n = model.init_state(img)\n",
    "            h_n, c_n = h_n.to(device), c_n.to(device)\n",
    "            output = torch.zeros(target.shape[0], model.max_len, model.num_classes).to(device)\n",
    "            x = target[:, 0]\n",
    "            output[:, 0] = x\n",
    "            x = model.embedding(x.long().unsqueeze(1))\n",
    "            for i in range(model.max_len-1):\n",
    "                x = model.embedding(target[:, i].argmax(-1).unsqueeze(1))\n",
    "                x, (h_n, c_n) = model.next_char(x, (h_n, c_n))\n",
    "                output[:, i+1] = x.squeeze(1)\n",
    "            loss += criterion(output.view(-1, model.num_classes), target.argmax(-1).view(-1))\n",
    "    return loss / len(dataloader)\n",
    "\n",
    "\n",
    "# 下面是训练的代码，使用教师强制训练\n",
    "def train_crnn(model, dataloader, learning_rate, epochs, device, start_epoch=0):\n",
    "\n",
    "    # 如果start_epoch不为0，说明是从某个epoch开始训练的，需要加载模型\n",
    "    if start_epoch:\n",
    "        model_name = get_model_name(model.hidden_dim, model.io_dim, learning_rate, dataloader.batch_size, start_epoch, \"concat\")\n",
    "        model_path = \"/root/autodl-tmp/APS360_Project/Machine_Learning_Output/CRNN/\"\n",
    "        model.load_state_dict(torch.load(model_path + model_name))\n",
    "        start_epoch += 1\n",
    "        print(f\"Model loaded from {model_name}\")\n",
    "\n",
    "    writer = tb.SummaryWriter('/root/tf-logs')\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=5, factor=0.5, verbose=True)\n",
    "    model.to(device)\n",
    "    for epoch in range(start_epoch, epochs):\n",
    "        model.train()\n",
    "        for step, (img, target) in enumerate(dataloader):\n",
    "            img, target = img.to(device), target.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            loss = 0\n",
    "            # 准备初始状态\n",
    "            h_n, c_n = model.init_state(img)\n",
    "            h_n, c_n = h_n.to(device), c_n.to(device)\n",
    "            output = torch.zeros(target.shape[0], model.max_len, model.num_classes).to(device)\n",
    "            x = target[:, 0]\n",
    "            output[:, 0] = x\n",
    "            x = model.embedding(x.long().unsqueeze(1))\n",
    "            for i in range(model.max_len-1):\n",
    "                x = model.embedding(target[:, i].argmax(-1).unsqueeze(1))\n",
    "                x, (h_n, c_n) = model.next_char(x, (h_n, c_n))\n",
    "                output[:, i+1] = x.squeeze(1)\n",
    "            loss = criterion(output.view(-1, model.num_classes), target.argmax(-1).view(-1))\n",
    "            # mask = (target.argmax(-1) != 0).view(-1)\n",
    "            # loss = loss[mask].mean()\n",
    "            loss.backward()\n",
    "            # 将cnn的参数乘0.1\n",
    "            for param in model.cnn.parameters():\n",
    "                param.grad *= 0.1\n",
    "            optimizer.step()\n",
    "            if step % 100 == 0:\n",
    "                print(f\"Epoch {epoch}, Iter {step}, Loss {loss.item()}\")\n",
    "                writer.add_scalar('CRNN_Loss', loss.item(), epoch * len(dataloader) + step)\n",
    "        \n",
    "        # 计算验证集上的loss\n",
    "        val_loss = get_val_loss(model, val_dataloader, device)\n",
    "        scheduler.step(val_loss)\n",
    "        print(f\"Epoch {epoch}, Val Loss {val_loss.item()}\")\n",
    "        writer.add_scalar('CRNN_Val_Loss', val_loss.item(), epoch)\n",
    "\n",
    "        model_name = get_model_name(model.hidden_dim, model.io_dim, learning_rate, dataloader.batch_size, epoch, \"concat\")\n",
    "        model_path = \"/root/autodl-tmp/APS360_Project/Machine_Learning_Output/CRNN/\"\n",
    "        torch.save(model.state_dict(), model_path + model_name)\n",
    "        print(f\"Model saved as {model_name}\")\n",
    "\n",
    "\n",
    "model = CRNN()\n",
    "# model_name = get_model_name(model.hidden_dim, model.io_dim, 0.001, 64, 84, \"IAM\")\n",
    "model_path = \"/root/autodl-tmp/APS360_Project/Machine_Learning_Output/CRNN/\"\n",
    "# model.load_state_dict(torch.load(model_path + model_name))\n",
    "# dataset = RecDataset(\"CVL\", \"train\")\n",
    "# dataloader = DataLoader(dataset, batch_size=64, shuffle=True, num_workers=8)\n",
    "val_dataset_IAM = RecDataset(\"IAM\", \"val\")\n",
    "val_dataset_CVL = RecDataset(\"CVL\", \"val\")\n",
    "val_dataset = torch.utils.data.ConcatDataset([val_dataset_IAM, val_dataset_CVL])\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=64, shuffle=False, num_workers=16)\n",
    "dataset_IAM = RecDataset(\"IAM\", \"train\")\n",
    "dataset_CVL = RecDataset(\"CVL\", \"train\")\n",
    "dataset = torch.utils.data.ConcatDataset([dataset_IAM, dataset_CVL])\n",
    "dataloader = DataLoader(dataset, batch_size=64, shuffle=True, num_workers=16)\n",
    "train_crnn(model, dataloader, 0.01, 500, \"cuda:0\", 43)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model\n",
    "model = CRNN()\n",
    "model_name = get_model_name(512, 1024, 0.01, 64, 33, \"concat\")\n",
    "model_path = \"/root/autodl-tmp/APS360_Project/Machine_Learning_Output/CRNN/\"\n",
    "model.load_state_dict(torch.load(model_path + model_name))\n",
    "model.to(\"cuda:0\")\n",
    "model.eval()\n",
    "\n",
    "def show_img(img):\n",
    "    img = img.squeeze().cpu().numpy()\n",
    "    plt.imshow(img, cmap=\"gray\")\n",
    "    plt.show()\n",
    "\n",
    "def get_word(output):\n",
    "    output = output.cpu().detach().numpy()\n",
    "    word = \"\"\n",
    "    for i in range(1, 64):\n",
    "        o = np.argmax(output[i])\n",
    "        if o == 3:\n",
    "            break\n",
    "        c = chr(o)\n",
    "        word += c\n",
    "    return word\n",
    "\n",
    "# test\n",
    "dataset = RecDataset(\"IAM\", \"val\")\n",
    "dataloader = DataLoader(dataset, batch_size=10, shuffle=True, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predict: ,\n",
      "answer:  '\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for step, (img, label) in enumerate(dataloader):\n",
    "    img, label = img.to(\"cuda:0\"), label.to(\"cuda:0\")\n",
    "    output = model(img)\n",
    "    print(\"predict:\", get_word(output[0]))\n",
    "    print(\"answer: \", get_word(label[0]))\n",
    "    show_img(img[0])\n",
    "    plt.show()\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Val Loss 0.23437054455280304\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 20\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step, (img, target) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(val_loader):\n\u001b[1;32m     19\u001b[0m     img, target \u001b[38;5;241m=\u001b[39m img\u001b[38;5;241m.\u001b[39mto(device), target\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m---> 20\u001b[0m     h_n, c_n \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minit_state\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m     h_n, c_n \u001b[38;5;241m=\u001b[39m h_n\u001b[38;5;241m.\u001b[39mto(device), c_n\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     22\u001b[0m     output \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros(target\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], model\u001b[38;5;241m.\u001b[39mmax_len, model\u001b[38;5;241m.\u001b[39mnum_classes)\u001b[38;5;241m.\u001b[39mto(device)\n",
      "Cell \u001b[0;32mIn[3], line 23\u001b[0m, in \u001b[0;36mCRNN.init_state\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     21\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv1(img)         \u001b[38;5;66;03m# batch_size, 3, 64, 64\u001b[39;00m\n\u001b[1;32m     22\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcnn(x)             \u001b[38;5;66;03m# batch_size, 512, 1, 1\u001b[39;00m\n\u001b[0;32m---> 23\u001b[0m x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mview(\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)   \u001b[38;5;66;03m# batch_size, 512\u001b[39;00m\n\u001b[1;32m     24\u001b[0m x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)          \u001b[38;5;66;03m# 1, batch_size, 512\u001b[39;00m\n\u001b[1;32m     25\u001b[0m h0 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mh0_fc(x)          \u001b[38;5;66;03m# 1, batch_size, hidden_dim\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# plot loss crv\n",
    "\n",
    "model = CRNN().cuda()\n",
    "train_loader = DataLoader(RecDataset(\"IAM\", \"train\"), batch_size=64, shuffle=True, num_workers=8)\n",
    "val_loader = DataLoader(RecDataset(\"IAM\", \"val\"), batch_size=64, shuffle=True, num_workers=8)\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
    "device = \"cuda:0\"\n",
    "\n",
    "for epoch in range(76):\n",
    "    model_name = get_model_name(512, 1024, 0.01, 64, epoch, \"concat\")\n",
    "    model_path = \"/root/autodl-tmp/APS360_Project/Machine_Learning_Output/CRNN/\"\n",
    "    model.load_state_dict(torch.load(model_path + model_name))\n",
    "    model.eval()\n",
    "    loss = 0\n",
    "    with torch.no_grad():\n",
    "        for step, (img, target) in enumerate(val_loader):\n",
    "            img, target = img.to(device), target.to(device)\n",
    "            h_n, c_n = model.init_state(img)\n",
    "            h_n, c_n = h_n.to(device), c_n.to(device)\n",
    "            output = torch.zeros(target.shape[0], model.max_len, model.num_classes).to(device)\n",
    "            x = target[:, 0]\n",
    "            output[:, 0] = x\n",
    "            x = model.embedding(x.long().unsqueeze(1))\n",
    "            for i in range(model.max_len-1):\n",
    "                x = model.embedding(target[:, i].argmax(-1).unsqueeze(1))\n",
    "                x, (h_n, c_n) = model.next_char(x, (h_n, c_n))\n",
    "                output[:, i+1] = x.squeeze(1)\n",
    "            loss += criterion(output.view(-1, model.num_classes), target.argmax(-1).view(-1))\n",
    "    val_losses.append(loss / len(dataloader))\n",
    "    print(f\"Epoch {epoch}, Val Loss {val_losses[-1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Val Loss 3.478210210800171\n",
      "Epoch 1, Val Loss 3.0621519088745117\n",
      "Epoch 2, Val Loss 2.8425073623657227\n",
      "Epoch 3, Val Loss 2.685619592666626\n",
      "Epoch 4, Val Loss 2.55229115486145\n",
      "Epoch 5, Val Loss 2.4177439212799072\n",
      "Epoch 6, Val Loss 2.3057734966278076\n",
      "Epoch 7, Val Loss 2.2105913162231445\n",
      "Epoch 8, Val Loss 2.111955165863037\n",
      "Epoch 9, Val Loss 2.030163049697876\n",
      "Epoch 10, Val Loss 1.9540117979049683\n",
      "Epoch 11, Val Loss 1.8829811811447144\n",
      "Epoch 12, Val Loss 1.8394535779953003\n",
      "Epoch 13, Val Loss 1.757978081703186\n",
      "Epoch 14, Val Loss 1.7055529356002808\n",
      "Epoch 15, Val Loss 1.6645315885543823\n",
      "Epoch 16, Val Loss 1.606818437576294\n",
      "Epoch 17, Val Loss 1.5785996913909912\n",
      "Epoch 18, Val Loss 1.584167242050171\n",
      "Epoch 19, Val Loss 1.4771547317504883\n",
      "Epoch 20, Val Loss 1.4356818199157715\n",
      "Epoch 21, Val Loss 1.4132806062698364\n",
      "Epoch 22, Val Loss 1.370862364768982\n",
      "Epoch 23, Val Loss 1.3342267274856567\n",
      "Epoch 24, Val Loss 1.3297675848007202\n",
      "Epoch 25, Val Loss 1.282596230506897\n",
      "Epoch 26, Val Loss 1.2633514404296875\n",
      "Epoch 27, Val Loss 1.2294220924377441\n",
      "Epoch 28, Val Loss 1.2150299549102783\n",
      "Epoch 29, Val Loss 1.1806330680847168\n",
      "Epoch 30, Val Loss 1.170399785041809\n",
      "Epoch 31, Val Loss 1.1450968980789185\n",
      "Epoch 32, Val Loss 1.131713628768921\n",
      "Epoch 33, Val Loss 1.1029592752456665\n",
      "Epoch 34, Val Loss 1.0796699523925781\n",
      "Epoch 35, Val Loss 1.0609869956970215\n",
      "Epoch 36, Val Loss 1.051054835319519\n",
      "Epoch 37, Val Loss 1.0301908254623413\n",
      "Epoch 38, Val Loss 1.0150494575500488\n",
      "Epoch 39, Val Loss 1.0090326070785522\n",
      "Epoch 40, Val Loss 0.994610071182251\n",
      "Epoch 41, Val Loss 0.9958568811416626\n",
      "Epoch 42, Val Loss 0.9884503483772278\n",
      "Epoch 43, Val Loss 0.959949254989624\n",
      "Epoch 44, Val Loss 0.9637017250061035\n",
      "Epoch 45, Val Loss 0.9511829018592834\n",
      "Epoch 46, Val Loss 0.9385563135147095\n",
      "Epoch 47, Val Loss 0.9345293045043945\n",
      "Epoch 48, Val Loss 0.9347803592681885\n",
      "Epoch 49, Val Loss 0.9203394651412964\n",
      "Epoch 50, Val Loss 0.9232437014579773\n",
      "Epoch 51, Val Loss 0.9063302278518677\n",
      "Epoch 52, Val Loss 0.9094985723495483\n",
      "Epoch 53, Val Loss 0.8941757082939148\n",
      "Epoch 54, Val Loss 0.8945206999778748\n",
      "Epoch 55, Val Loss 0.8955072164535522\n",
      "Epoch 56, Val Loss 0.8866227269172668\n",
      "Epoch 57, Val Loss 0.8780379891395569\n",
      "Epoch 58, Val Loss 0.8748263120651245\n",
      "Epoch 59, Val Loss 0.8722445368766785\n",
      "Epoch 60, Val Loss 0.8674086928367615\n",
      "Epoch 61, Val Loss 0.8668352961540222\n",
      "Epoch 62, Val Loss 0.8633576035499573\n",
      "Epoch 63, Val Loss 0.8563470840454102\n",
      "Epoch 64, Val Loss 0.860308825969696\n",
      "Epoch 65, Val Loss 0.8568587899208069\n",
      "Epoch 66, Val Loss 0.8497547507286072\n",
      "Epoch 67, Val Loss 0.8493278622627258\n",
      "Epoch 68, Val Loss 0.8500618934631348\n",
      "Epoch 69, Val Loss 0.8440350890159607\n",
      "Epoch 70, Val Loss 0.8435754776000977\n",
      "Epoch 71, Val Loss 0.8398423790931702\n",
      "Epoch 72, Val Loss 0.8428664207458496\n",
      "Epoch 73, Val Loss 0.8351067900657654\n",
      "Epoch 74, Val Loss 0.8363223671913147\n",
      "Epoch 75, Val Loss 0.8345384001731873\n",
      "Epoch 76, Val Loss 0.8295291662216187\n",
      "Epoch 77, Val Loss 0.8296698331832886\n",
      "Epoch 78, Val Loss 0.8329849243164062\n",
      "Epoch 79, Val Loss 0.8267735242843628\n",
      "Epoch 80, Val Loss 0.8245104551315308\n",
      "Epoch 81, Val Loss 0.8274577856063843\n",
      "Epoch 82, Val Loss 0.8202401995658875\n",
      "Epoch 83, Val Loss 0.8214477896690369\n",
      "Epoch 84, Val Loss 0.819638192653656\n",
      "Epoch 85, Val Loss 0.8213520050048828\n",
      "Epoch 86, Val Loss 0.8173208236694336\n",
      "Epoch 87, Val Loss 0.8166518807411194\n",
      "Epoch 88, Val Loss 0.8159953355789185\n",
      "Epoch 89, Val Loss 0.8165045976638794\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(90):\n",
    "    model_name = get_model_name(model.hidden_dim, model.io_dim, 0.001, 64, epoch, \"IAM\")\n",
    "    model_path = \"/root/autodl-tmp/APS360_Project/Machine_Learning_Output/CRNN/\"\n",
    "    model.load_state_dict(torch.load(model_path + model_name))\n",
    "    model.eval()\n",
    "    loss = 0\n",
    "    with torch.no_grad():\n",
    "        for step, (img, target) in enumerate(train_loader):\n",
    "            img, target = img.to(device), target.to(device)\n",
    "            h_n, c_n = model.init_state(img)\n",
    "            h_n, c_n = h_n.to(device), c_n.to(device)\n",
    "            output = torch.zeros(target.shape[0], model.max_len, model.num_classes).to(device)\n",
    "            x = target[:, 0]\n",
    "            output[:, 0] = x\n",
    "            x = model.embedding(x.long().unsqueeze(1))\n",
    "            for i in range(model.max_len-1):\n",
    "                x = model.embedding(target[:, i].argmax(-1).unsqueeze(1))\n",
    "                x, (h_n, c_n) = model.next_char(x, (h_n, c_n))\n",
    "                output[:, i+1] = x.squeeze(1)\n",
    "            loss += criterion(output.view(-1, model.num_classes), target.argmax(-1).view(-1))\n",
    "    train_losses.append(loss / len(dataloader))\n",
    "    print(f\"Epoch {epoch}, Val Loss {train_losses[-1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "# fix bug\n",
    "val_temp = [i.item() for i in val_losses]\n",
    "train_temp = [i.item() for i in train_losses]\n",
    "dataset_len = len(dataloader)\n",
    "val_len = len(val_loader)\n",
    "train_len = len(train_loader)\n",
    "val_temp = [i / val_len * dataset_len for i in val_temp]\n",
    "train_temp = [i / train_len * dataset_len for i in train_temp]\n",
    "print(val_temp)\n",
    "print(train_temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"train_losses.pkl\", \"wb\") as f:\n",
    "    pickle.dump(train_losses, f)\n",
    "with open(\"val_losses.pkl\", \"wb\") as f:\n",
    "    pickle.dump(val_losses, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAksAAAGwCAYAAAC5ACFFAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjkuMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy80BEi2AAAACXBIWXMAAA9hAAAPYQGoP6dpAAA09ElEQVR4nO3de1hVZf7//9cG5KiAIrKhMLM00Dw0KITNNyuZAXVMFEeHIQ9lMRbaQS01T2VTTmNNVpZOfSb9WJpmU45TpilaOUoey1DRafqYeAIyA0QTENbvD3/uaSfeAgKbrc/Hda0r9r3ue6/3va6d+3Wtfe+1bZZlWQIAAECVPFxdAAAAQGNGWAIAADAgLAEAABgQlgAAAAwISwAAAAaEJQAAAAPCEgAAgIGXqwu4HFRWVurIkSNq1qyZbDabq8sBAADVYFmWTpw4oYiICHl4XPj6EWGpDhw5ckSRkZGuLgMAANTCwYMHdfXVV19wP2GpDjRr1kzS2ZMdGBjo4moAAEB1FBcXKzIy0vE+fiGEpTpw7qO3wMBAwhIAAG7mYktoWOANAABgQFgCAAAwICwBAAAYsGYJAOByFRUVKi8vd3UZuMw0adJEnp6el/w8hCUAgMtYlqW8vDwVFha6uhRcpoKDg2W32y/pPoiEJQCAy5wLSq1atZK/vz839kWdsSxLp06dUkFBgSQpPDy81s9FWAIAuERFRYUjKIWEhLi6HFyG/Pz8JEkFBQVq1apVrT+SY4E3AMAlzq1R8vf3d3EluJyde31dypo4whIAwKX46A31qS5eX4QlAAAAA8ISAACAAWEJAAAXuO222/Twww87Hrdp00azZ882jrHZbFq+fPklH7uunudKQVgCAKAG+vXrp6SkpCr3bdiwQTabTV999VWNn3fr1q1KT0+/1PKcPPHEE+ratet57UePHlXv3r3r9Fg/t2DBAgUHB9frMRoKYQkAgBoYOXKk1qxZo0OHDp23b/78+erWrZs6d+5c4+cNDQ1tsG8G2u12+fj4NMixLgeEJQBAo2FZlk6VnWnwzbKsatf4m9/8RqGhoVqwYIFTe0lJiZYtW6aRI0fq+++/V2pqqq666ir5+/urU6dOevvtt43P+/OP4b7++mvdeuut8vX1VYcOHbRmzZrzxkyYMEHt27eXv7+/2rZtq6lTpzq+Ir9gwQI9+eST2rlzp2w2m2w2m6Pmn38Ml52drTvuuEN+fn4KCQlRenq6SkpKHPtHjBih5ORkPffccwoPD1dISIgyMjIu6ev4ubm56t+/v5o2barAwEANHjxY+fn5jv07d+7U7bffrmbNmikwMFAxMTHatm2bJOnAgQPq16+fmjdvroCAAHXs2FErV66sdS0Xw00pAQCNxo/lFeowbXWDH3fPjET5e1fvLdHLy0vDhg3TggULNHnyZMdX05ctW6aKigqlpqaqpKREMTExmjBhggIDA/Xhhx9q6NChuu666xQbG3vRY1RWVmrgwIEKCwvT5s2bVVRU5LS+6ZxmzZppwYIFioiIUHZ2tu677z41a9ZMjz32mIYMGaJdu3Zp1apVWrt2rSQpKCjovOc4efKkEhMTFR8fr61bt6qgoED33nuvRo8e7RQI169fr/DwcK1fv17/+c9/NGTIEHXt2lX33Xdftc7bz+d3Lih9+umnOnPmjDIyMjRkyBB98sknkqS0tDTddNNNmjt3rjw9PfXll1+qSZMmkqSMjAyVlZXps88+U0BAgPbs2aOmTZvWuI7qIiwBAFBD99xzj2bNmqVPP/1Ut912m6SzH8GlpKQoKChIQUFBGj9+vKP/mDFjtHr1ar3zzjvVCktr167V3r17tXr1akVEREiSnnnmmfPWGU2ZMsXxd5s2bTR+/HgtWbJEjz32mPz8/NS0aVN5eXnJbrdf8FiLFy/W6dOntXDhQgUEBEiS5syZo379+unZZ59VWFiYJKl58+aaM2eOPD09FRUVpb59+yozM7NWYSkzM1PZ2dnav3+/IiMjJUkLFy5Ux44dtXXrVnXv3l25ubl69NFHFRUVJUlq166dY3xubq5SUlLUqVMnSVLbtm1rXENNEJYAAI2GXxNP7ZmR6JLj1kRUVJR69OihN954Q7fddpv+85//aMOGDZoxY4aksz/l8swzz+idd97R4cOHVVZWptLS0mqvScrJyVFkZKQjKElSfHz8ef2WLl2ql156Sd98841KSkp05swZBQYG1mguOTk56tKliyMoSdItt9yiyspK7du3zxGWOnbs6PRzIeHh4crOzq7RsX56zMjISEdQkqQOHTooODhYOTk56t69u8aOHat7771Xb775phISEvTb3/5W1113nSTpwQcf1P3336+PP/5YCQkJSklJqdU6sepizRIAoNGw2Wzy9/Zq8K02d3keOXKk/v73v+vEiROaP3++rrvuOvXs2VOSNGvWLL344ouaMGGC1q9fry+//FKJiYkqKyurs3OVlZWltLQ09enTRx988IG++OILTZ48uU6P8VPnPgI7x2azqbKysl6OJZ39Jt/u3bvVt29frVu3Th06dND7778vSbr33nv1f//3fxo6dKiys7PVrVs3vfzyy/VWC2EJAIBaGDx4sDw8PLR48WItXLhQ99xzjyN0bdy4Uf3799ddd92lLl26qG3btvr3v/9d7eeOjo7WwYMHdfToUUfb559/7tRn06ZNuuaaazR58mR169ZN7dq104EDB5z6eHt7q6Ki4qLH2rlzp06ePOlo27hxozw8PHTDDTdUu+aaODe/gwcPOtr27NmjwsJCdejQwdHWvn17PfLII/r44481cOBAzZ8/37EvMjJSo0aN0nvvvadx48bp9ddfr5daJcISAAC10rRpUw0ZMkSTJk3S0aNHNWLECMe+du3aac2aNdq0aZNycnL0hz/8wembXheTkJCg9u3ba/jw4dq5c6c2bNigyZMnO/Vp166dcnNztWTJEn3zzTd66aWXHFdezmnTpo3279+vL7/8UseOHVNpael5x0pLS5Ovr6+GDx+uXbt2af369RozZoyGDh3q+AiutioqKvTll186bTk5OUpISFCnTp2UlpamHTt2aMuWLRo2bJh69uypbt266ccff9To0aP1ySef6MCBA9q4caO2bt2q6OhoSdLDDz+s1atXa//+/dqxY4fWr1/v2FcfCEsAANTSyJEj9cMPPygxMdFpfdGUKVP0i1/8QomJibrttttkt9uVnJxc7ef18PDQ+++/rx9//FGxsbG699579fTTTzv1ufPOO/XII49o9OjR6tq1qzZt2qSpU6c69UlJSVFSUpJuv/12hYaGVnn7An9/f61evVrHjx9X9+7dNWjQIPXq1Utz5syp2cmoQklJiW666SanrV+/frLZbPrHP/6h5s2b69Zbb1VCQoLatm2rpUuXSpI8PT31/fffa9iwYWrfvr0GDx6s3r1768knn5R0NoRlZGQoOjpaSUlJat++vV599dVLrvdCbFZNbi6BKhUXFysoKEhFRUU1XlgHAFeq06dPa//+/br22mvl6+vr6nJwmTK9zqr7/s2VJQAAAAPCEgAAgAFhCQAAwICwBAAAYEBYAgAAMCAsAQAAGBCWAAAADAhLAAAABoQlAAAagTZt2mj27NnV7v/JJ5/IZrOpsLCw3mrCWYQlAABqwGazGbcnnniiVs+7detWpaenV7t/jx49dPToUQUFBdXqeNVFKJO8XF0AAADu5OjRo46/ly5dqmnTpmnfvn2OtqZNmzr+tixLFRUV8vK6+NttaGhojerw9vaW3W6v0RjUDleWAACoAbvd7tiCgoJks9kcj/fu3atmzZrpo48+UkxMjHx8fPSvf/1L33zzjfr376+wsDA1bdpU3bt319q1a52e9+cfw9lsNv3P//yPBgwYIH9/f7Vr104rVqxw7P/5FZ8FCxYoODhYq1evVnR0tJo2baqkpCSncHfmzBk9+OCDCg4OVkhIiCZMmKDhw4fX6Ed+f+6HH37QsGHD1Lx5c/n7+6t37976+uuvHfsPHDigfv36qXnz5goICFDHjh21cuVKx9i0tDSFhobKz89P7dq10/z582tdS30hLAEAGg/LkspONvxWx78pP3HiRP3pT39STk6OOnfurJKSEvXp00eZmZn64osvlJSUpH79+ik3N9f4PE8++aQGDx6sr776Sn369FFaWpqOHz9+wf6nTp3Sc889pzfffFOfffaZcnNzNX78eMf+Z599VosWLdL8+fO1ceNGFRcXa/ny5Zc01xEjRmjbtm1asWKFsrKyZFmW+vTpo/LycklSRkaGSktL9dlnnyk7O1vPPvus4+rb1KlTtWfPHn300UfKycnR3Llz1bJly0uqpz7wMRwAoPEoPyU9E9Hwx338iOQdUGdPN2PGDP3qV79yPG7RooW6dOniePzUU0/p/fff14oVKzR69OgLPs+IESOUmpoqSXrmmWf00ksvacuWLUpKSqqyf3l5uebNm6frrrtOkjR69GjNmDHDsf/ll1/WpEmTNGDAAEnSnDlzHFd5auPrr7/WihUrtHHjRvXo0UOStGjRIkVGRmr58uX67W9/q9zcXKWkpKhTp06SpLZt2zrG5+bm6qabblK3bt0knb261hhxZQkAgDp27s3/nJKSEo0fP17R0dEKDg5W06ZNlZOTc9ErS507d3b8HRAQoMDAQBUUFFywv7+/vyMoSVJ4eLijf1FRkfLz8xUbG+vY7+npqZiYmBrN7adycnLk5eWluLg4R1tISIhuuOEG5eTkSJIefPBB/fGPf9Qtt9yi6dOn66uvvnL0vf/++7VkyRJ17dpVjz32mDZt2lTrWuoTV5YAAI1HE/+zV3lccdw6FBDgfJVq/PjxWrNmjZ577jldf/318vPz06BBg1RWVmYuq0kTp8c2m02VlZU16m/V8UeMNXXvvfcqMTFRH374oT7++GPNnDlTzz//vMaMGaPevXvrwIEDWrlypdasWaNevXopIyNDzz33nEtr/jmuLAEAGg+b7ezHYQ292Wz1Oq2NGzdqxIgRGjBggDp16iS73a5vv/22Xo/5c0FBQQoLC9PWrVsdbRUVFdqxY0etnzM6OlpnzpzR5s2bHW3ff/+99u3bpw4dOjjaIiMjNWrUKL333nsaN26cXn/9dce+0NBQDR8+XG+99ZZmz56t1157rdb11BeuLAEAUM/atWun9957T/369ZPNZtPUqVONV4jqy5gxYzRz5kxdf/31ioqK0ssvv6wffvhBtmqExezsbDVr1szx2GazqUuXLurfv7/uu+8+/fWvf1WzZs00ceJEXXXVVerfv78k6eGHH1bv3r3Vvn17/fDDD1q/fr2io6MlSdOmTVNMTIw6duyo0tJSffDBB459jQlhCQCAevaXv/xF99xzj3r06KGWLVtqwoQJKi4ubvA6JkyYoLy8PA0bNkyenp5KT09XYmKiPD09Lzr21ltvdXrs6empM2fOaP78+XrooYf0m9/8RmVlZbr11lu1cuVKx0eCFRUVysjI0KFDhxQYGKikpCS98MILks7eK2rSpEn69ttv5efnp//3//6flixZUvcTv0Q2y9UfZl4GiouLFRQUpKKiIgUGBrq6HABwC6dPn9b+/ft17bXXytfX19XlXJEqKysVHR2twYMH66mnnnJ1OfXC9Dqr7vs3V5YAALhCHDhwQB9//LF69uyp0tJSzZkzR/v379fvf/97V5fWqLndAu9XXnlFbdq0ka+vr+Li4rRlyxZj/2XLlikqKkq+vr7q1KmT8X4So0aNks1mq9EPGQIA4C48PDy0YMECde/eXbfccouys7O1du3aRrlOqDFxq7C0dOlSjR07VtOnT9eOHTvUpUsXJSYmXvCeE5s2bVJqaqpGjhypL774QsnJyUpOTtauXbvO6/v+++/r888/V0SEC26GBgBAA4iMjNTGjRtVVFSk4uJibdq06by1SDifW4Wlv/zlL7rvvvt09913q0OHDpo3b578/f31xhtvVNn/xRdfVFJSkh599FFFR0frqaee0i9+8QvNmTPHqd/hw4c1ZswYLVq06Lx7VAAAgCub24SlsrIybd++XQkJCY42Dw8PJSQkKCsrq8oxWVlZTv0lKTEx0al/ZWWlhg4dqkcffVQdO3asVi2lpaUqLi522gAAtcP3jFCf6uL15TZh6dixY6qoqFBYWJhTe1hYmPLy8qock5eXd9H+zz77rLy8vPTggw9Wu5aZM2cqKCjIsUVGRtZgJgAA6b93mz516pSLK8Hl7Nzr61I+Obqivw23fft2vfjii9qxY0e1bsh1zqRJkzR27FjH4+LiYgITANSQp6engoODHetO/f39a/RvMWBiWZZOnTqlgoICBQcHV+teUhfiNmGpZcuW8vT0VH5+vlN7fn6+7HZ7lWPsdrux/4YNG1RQUKDWrVs79ldUVGjcuHGaPXv2BW9F7+PjIx8fn0uYDQBAkuPfY9OPwwKXIjg4+II5obrcJix5e3srJiZGmZmZSk5OlnR2vVFmZqZGjx5d5Zj4+HhlZmbq4YcfdrStWbNG8fHxkqShQ4dWuaZp6NChuvvuu+tlHgCA/7LZbAoPD1erVq1UXl7u6nJwmWnSpMklXVE6x23CkiSNHTtWw4cPV7du3RQbG6vZs2fr5MmTjmAzbNgwXXXVVZo5c6Yk6aGHHlLPnj31/PPPq2/fvlqyZIm2bdvm+JG+kJAQhYSEOB2jSZMmstvtuuGGGxp2cgBwBfP09KyTNzWgPrhVWBoyZIi+++47TZs2TXl5eeratatWrVrlWMSdm5srD4//rlnv0aOHFi9erClTpujxxx9Xu3bttHz5ct14442umgIAAHAz/DZcHeC34QAAcD/Vff92m1sHAAAAuAJhCQAAwICwBAAAYEBYAgAAMCAsAQAAGBCWAAAADAhLAAAABoQlAAAAA8ISAACAAWEJAADAgLAEAABgQFgCAAAwICwBAAAYEJYAAAAMCEsAAAAGhCUAAAADwhIAAIABYQkAAMCAsAQAAGBAWAIAADAgLAEAABgQlgAAAAwISwAAAAaEJQAAAAPCEgAAgAFhCQAAwICwBAAAYEBYAgAAMCAsAQAAGBCWAAAADAhLAAAABoQlAAAAA8ISAACAAWEJAADAgLAEAABgQFgCAAAwICwBAAAYEJYAAAAMCEsAAAAGhCUAAAADwhIAAIABYQkAAMCAsAQAAGBAWAIAADAgLAEAABgQlgAAAAwISwAAAAaEJQAAAAPCEgAAgAFhCQAAwICwBAAAYEBYAgAAMCAsAQAAGBCWAAAADAhLAAAABoQlAAAAA7cLS6+88oratGkjX19fxcXFacuWLcb+y5YtU1RUlHx9fdWpUyetXLnSsa+8vFwTJkxQp06dFBAQoIiICA0bNkxHjhyp72kAAAA34VZhaenSpRo7dqymT5+uHTt2qEuXLkpMTFRBQUGV/Tdt2qTU1FSNHDlSX3zxhZKTk5WcnKxdu3ZJkk6dOqUdO3Zo6tSp2rFjh9577z3t27dPd955Z0NOCwAANGI2y7IsVxdRXXFxcerevbvmzJkjSaqsrFRkZKTGjBmjiRMnntd/yJAhOnnypD744ANH280336yuXbtq3rx5VR5j69atio2N1YEDB9S6detq1VVcXKygoCAVFRUpMDCwFjMDAAANrbrv325zZamsrEzbt29XQkKCo83Dw0MJCQnKysqqckxWVpZTf0lKTEy8YH9JKioqks1mU3Bw8AX7lJaWqri42GkDAACXJ7cJS8eOHVNFRYXCwsKc2sPCwpSXl1flmLy8vBr1P336tCZMmKDU1FRjwpw5c6aCgoIcW2RkZA1nAwAA3IXbhKX6Vl5ersGDB8uyLM2dO9fYd9KkSSoqKnJsBw8ebKAqAQBAQ/NydQHV1bJlS3l6eio/P9+pPT8/X3a7vcoxdru9Wv3PBaUDBw5o3bp1F1135OPjIx8fn1rMAgAAuBu3ubLk7e2tmJgYZWZmOtoqKyuVmZmp+Pj4KsfEx8c79ZekNWvWOPU/F5S+/vprrV27ViEhIfUzAQAA4Jbc5sqSJI0dO1bDhw9Xt27dFBsbq9mzZ+vkyZO6++67JUnDhg3TVVddpZkzZ0qSHnroIfXs2VPPP/+8+vbtqyVLlmjbtm167bXXJJ0NSoMGDdKOHTv0wQcfqKKiwrGeqUWLFvL29nbNRAEAQKPhVmFpyJAh+u677zRt2jTl5eWpa9euWrVqlWMRd25urjw8/nuxrEePHlq8eLGmTJmixx9/XO3atdPy5ct14403SpIOHz6sFStWSJK6du3qdKz169frtttua5B5AQCAxsut7rPUWHGfJQAA3M9ld58lAAAAVyAsAQAAGBCWAAAADAhLAAAABoQlAAAAA8ISAACAAWEJAADAgLAEAABgQFgCAAAwICwBAAAYEJYAAAAMCEsAAAAGhCUAAAADwhIAAIABYQkAAMCAsAQAAGBAWAIAADAgLAEAABgQlgAAAAwISwAAAAaEJQAAAAPCEgAAgAFhCQAAwICwBAAAYEBYAgAAMCAsAQAAGBCWAAAADAhLAAAABoQlAAAAA8ISAACAAWEJAADAgLAEAABgQFgCAAAwICwBAAAYEJYAAAAMCEsAAAAGhCUAAAADwhIAAIABYQkAAMCAsAQAAGBAWAIAADAgLAEAABgQlgAAAAwISwAAAAaEJQAAAAPCEgAAgAFhCQAAwICwBAAAYEBYAgAAMKhVWDp48KAOHTrkeLxlyxY9/PDDeu211+qsMAAAgMagVmHp97//vdavXy9JysvL069+9Stt2bJFkydP1owZM+q0QAAAAFeqVVjatWuXYmNjJUnvvPOObrzxRm3atEmLFi3SggUL6rI+AAAAl6pVWCovL5ePj48kae3atbrzzjslSVFRUTp69GjdVQcAAOBitQpLHTt21Lx587RhwwatWbNGSUlJkqQjR44oJCSkTgsEAABwpVqFpWeffVZ//etfddtttyk1NVVdunSRJK1YscLx8RwAAMDlwGZZllWbgRUVFSouLlbz5s0dbd9++638/f3VqlWrOivQHRQXFysoKEhFRUUKDAx0dTkAAKAaqvv+XasrSz/++KNKS0sdQenAgQOaPXu29u3bV+9B6ZVXXlGbNm3k6+uruLg4bdmyxdh/2bJlioqKkq+vrzp16qSVK1c67bcsS9OmTVN4eLj8/PyUkJCgr7/+uj6nAAAA3EitwlL//v21cOFCSVJhYaHi4uL0/PPPKzk5WXPnzq3TAn9q6dKlGjt2rKZPn64dO3aoS5cuSkxMVEFBQZX9N23apNTUVI0cOVJffPGFkpOTlZycrF27djn6/PnPf9ZLL72kefPmafPmzQoICFBiYqJOnz5db/MAAABuxKqFkJAQa9euXZZlWdbrr79ude7c2aqoqLDeeecdKyoqqjZPWS2xsbFWRkaG43FFRYUVERFhzZw5s8r+gwcPtvr27evUFhcXZ/3hD3+wLMuyKisrLbvdbs2aNcuxv7Cw0PLx8bHefvvtatdVVFRkSbKKiopqMh0AAOBC1X3/rtWVpVOnTqlZs2aSpI8//lgDBw6Uh4eHbr75Zh04cKAOo9x/lZWVafv27UpISHC0eXh4KCEhQVlZWVWOycrKcuovSYmJiY7++/fvV15enlOfoKAgxcXFXfA5Jam0tFTFxcVOGwAAuDzVKixdf/31Wr58uQ4ePKjVq1fr17/+tSSpoKCg3hY4Hzt2TBUVFQoLC3NqDwsLU15eXpVj8vLyjP3P/bcmzylJM2fOVFBQkGOLjIys8XwAAIB7qFVYmjZtmsaPH682bdooNjZW8fHxks5eZbrpppvqtMDGaNKkSSoqKnJsBw8edHVJAACgnnjVZtCgQYP0y1/+UkePHnXcY0mSevXqpQEDBtRZcT/VsmVLeXp6Kj8/36k9Pz9fdru9yjF2u93Y/9x/8/PzFR4e7tSna9euF6zFx8fHcQdzAABweavVlSXpbNC46aabdOTIER06dEiSFBsbq6ioqDor7qe8vb0VExOjzMxMR1tlZaUyMzMdV7Z+Lj4+3qm/JK1Zs8bR/9prr5XdbnfqU1xcrM2bN1/wOQEAwJWlVmGpsrJSM2bMUFBQkK655hpdc801Cg4O1lNPPaXKysq6rtFh7Nixev311/W///u/ysnJ0f3336+TJ0/q7rvvliQNGzZMkyZNcvR/6KGHtGrVKj3//PPau3evnnjiCW3btk2jR4+WJNlsNj388MP64x//qBUrVig7O1vDhg1TRESEkpOT620eAADAfdTqY7jJkyfrb3/7m/70pz/plltukST961//0hNPPKHTp0/r6aefrtMizxkyZIi+++47TZs2TXl5eeratatWrVrlWKCdm5srD4//5r8ePXpo8eLFmjJlih5//HG1a9dOy5cv14033ujo89hjj+nkyZNKT09XYWGhfvnLX2rVqlXy9fWtlzkAAAD3UqufO4mIiNC8efN05513OrX/4x//0AMPPKDDhw/XWYHugJ87AQDA/dTrz50cP368yrVJUVFROn78eG2eEgAAoFGqVVjq0qWL5syZc177nDlz1Llz50suCgAAoLGo1ZqlP//5z+rbt6/Wrl3r+NZYVlaWDh48eN4P1QIAALizWl1Z6tmzp/79739rwIABKiwsVGFhoQYOHKjdu3frzTffrOsaAQAAXKZWC7wvZOfOnfrFL36hioqKunpKt8ACbwAA3E+9LvAGAAC4UhCWAAAADAhLAAAABjX6NtzAgQON+wsLCy+lFgAAgEanRmEpKCjoovuHDRt2SQUBAAA0JjUKS/Pnz6+vOgAAABol1iwBAAAYEJYAAAAMCEsAAAAGhCUAAAADwhIAAIABYQkAAMCAsAQAAGBAWAIAADAgLAEAABgQlgAAAAwISwAAAAaEJQAAAAPCEgAAgAFhCQAAwICwBAAAYEBYAgAAMCAsAQAAGBCWAAAADAhLAAAABoQlAAAAA8ISAACAAWEJAADAgLAEAABgQFgCAAAwICwBAAAYEJYAAAAMCEsAAAAGhCUAAAADwhIAAIABYQkAAMCAsAQAAGBAWAIAADAgLAEAABgQlgAAAAwISwAAAAaEJQAAAAPCEgAAgAFhCQAAwICwBAAAYEBYAgAAMCAsAQAAGBCWAAAADAhLAAAABoQlAAAAA8ISAACAAWEJAADAwG3C0vHjx5WWlqbAwEAFBwdr5MiRKikpMY45ffq0MjIyFBISoqZNmyolJUX5+fmO/Tt37lRqaqoiIyPl5+en6Ohovfjii/U9FQAA4EbcJiylpaVp9+7dWrNmjT744AN99tlnSk9PN4555JFH9M9//lPLli3Tp59+qiNHjmjgwIGO/du3b1erVq301ltvaffu3Zo8ebImTZqkOXPm1Pd0AACAm7BZlmW5uoiLycnJUYcOHbR161Z169ZNkrRq1Sr16dNHhw4dUkRExHljioqKFBoaqsWLF2vQoEGSpL179yo6OlpZWVm6+eabqzxWRkaGcnJytG7dugvWU1paqtLSUsfj4uJiRUZGqqioSIGBgZcyVQAA0ECKi4sVFBR00fdvt7iylJWVpeDgYEdQkqSEhAR5eHho8+bNVY7Zvn27ysvLlZCQ4GiLiopS69atlZWVdcFjFRUVqUWLFsZ6Zs6cqaCgIMcWGRlZwxkBAAB34RZhKS8vT61atXJq8/LyUosWLZSXl3fBMd7e3goODnZqDwsLu+CYTZs2aenSpRf9eG/SpEkqKipybAcPHqz+ZAAAgFtxaViaOHGibDabcdu7d2+D1LJr1y71799f06dP169//WtjXx8fHwUGBjptAADg8uTlyoOPGzdOI0aMMPZp27at7Ha7CgoKnNrPnDmj48ePy263VznObrerrKxMhYWFTleX8vPzzxuzZ88e9erVS+np6ZoyZUqt5gIAAC5PLg1LoaGhCg0NvWi/+Ph4FRYWavv27YqJiZEkrVu3TpWVlYqLi6tyTExMjJo0aaLMzEylpKRIkvbt26fc3FzFx8c7+u3evVt33HGHhg8frqeffroOZgUAAC4nbvFtOEnq3bu38vPzNW/ePJWXl+vuu+9Wt27dtHjxYknS4cOH1atXLy1cuFCxsbGSpPvvv18rV67UggULFBgYqDFjxkg6uzZJOvvR2x133KHExETNmjXLcSxPT89qhbhzqruaHgAANB7Vff926ZWlmli0aJFGjx6tXr16ycPDQykpKXrppZcc+8vLy7Vv3z6dOnXK0fbCCy84+paWlioxMVGvvvqqY/+7776r7777Tm+99ZbeeustR/s111yjb7/9tkHmBQAAGje3ubLUmHFlCQAA93NZ3WcJAADAVQhLAAAABoQlAAAAA8ISAACAAWEJAADAgLAEAABgQFgCAAAwICwBAAAYEJYAAAAMCEsAAAAGhCUAAAADwhIAAIABYQkAAMCAsAQAAGBAWAIAADAgLAEAABgQlgAAAAwISwAAAAaEJQAAAAPCEgAAgAFhCQAAwICwBAAAYEBYAgAAMCAsAQAAGBCWAAAADAhLAAAABoQlAAAAA8ISAACAAWEJAADAgLAEAABgQFgCAAAwICwBAAAYEJYAAAAMCEsAAAAGhCUAAAADwhIAAIABYQkAAMCAsAQAAGBAWAIAADAgLAEAABgQlgAAAAwISwAAAAaEJQAAAAPCEgAAgAFhCQAAwICwBAAAYEBYAgAAMCAsAQAAGBCWAAAADAhLAAAABoQlAAAAA8ISAACAAWEJAADAgLAEAABgQFgCAAAwICwBAAAYuE1YOn78uNLS0hQYGKjg4GCNHDlSJSUlxjGnT59WRkaGQkJC1LRpU6WkpCg/P7/Kvt9//72uvvpq2Ww2FRYW1sMMAACAO3KbsJSWlqbdu3drzZo1+uCDD/TZZ58pPT3dOOaRRx7RP//5Ty1btkyffvqpjhw5ooEDB1bZd+TIkercuXN9lA4AANyYzbIsy9VFXExOTo46dOigrVu3qlu3bpKkVatWqU+fPjp06JAiIiLOG1NUVKTQ0FAtXrxYgwYNkiTt3btX0dHRysrK0s033+zoO3fuXC1dulTTpk1Tr1699MMPPyg4OPiC9ZSWlqq0tNTxuLi4WJGRkSoqKlJgYGAdzRoAANSn4uJiBQUFXfT92y2uLGVlZSk4ONgRlCQpISFBHh4e2rx5c5Vjtm/frvLyciUkJDjaoqKi1Lp1a2VlZTna9uzZoxkzZmjhwoXy8Kje6Zg5c6aCgoIcW2RkZC1nBgAAGju3CEt5eXlq1aqVU5uXl5datGihvLy8C47x9vY+7wpRWFiYY0xpaalSU1M1a9YstW7dutr1TJo0SUVFRY7t4MGDNZsQAABwGy4NSxMnTpTNZjNue/furbfjT5o0SdHR0brrrrtqNM7Hx0eBgYFOGwAAuDx5ufLg48aN04gRI4x92rZtK7vdroKCAqf2M2fO6Pjx47Lb7VWOs9vtKisrU2FhodPVpfz8fMeYdevWKTs7W++++64k6dzyrZYtW2ry5Ml68sknazkzAABwuXBpWAoNDVVoaOhF+8XHx6uwsFDbt29XTEyMpLNBp7KyUnFxcVWOiYmJUZMmTZSZmamUlBRJ0r59+5Sbm6v4+HhJ0t///nf9+OOPjjFbt27VPffcow0bNui666671OkBAIDLgEvDUnVFR0crKSlJ9913n+bNm6fy8nKNHj1av/vd7xzfhDt8+LB69eqlhQsXKjY2VkFBQRo5cqTGjh2rFi1aKDAwUGPGjFF8fLzjm3A/D0THjh1zHM/0bTgAAHDlcIuwJEmLFi3S6NGj1atXL3l4eCglJUUvvfSSY395ebn27dunU6dOOdpeeOEFR9/S0lIlJibq1VdfdUX5AADATbnFfZYau+repwEAADQel9V9lgAAAFyFsAQAAGBAWAIAADAgLAEAABgQlgAAAAwISwAAAAaEJQAAAAPCEgAAgAFhCQAAwICwBAAAYEBYAgAAMCAsAQAAGBCWAAAADAhLAAAABoQlAAAAA8ISAACAAWEJAADAgLAEAABgQFgCAAAwICwBAAAYEJYAAAAMCEsAAAAGhCUAAAADwhIAAIABYQkAAMCAsAQAAGBAWAIAADAgLAEAABgQlgAAAAwISwAAAAaEJQAAAAPCEgAAgAFhCQAAwICwBAAAYEBYAgAAMCAsAQAAGBCWAAAADAhLAAAABoQlAAAAA8ISAACAAWEJAADAgLAEAABg4OXqAi4HlmVJkoqLi11cCQAAqK5z79vn3scvhLBUB06cOCFJioyMdHElAACgpk6cOKGgoKAL7rdZF4tTuKjKykodOXJEzZo1k81mc3U5LlVcXKzIyEgdPHhQgYGBri7nssV5bjic64bBeW4YnGdnlmXpxIkTioiIkIfHhVcmcWWpDnh4eOjqq692dRmNSmBgIP8jNgDOc8PhXDcMznPD4Dz/l+mK0jks8AYAADAgLAEAABgQllCnfHx8NH36dPn4+Li6lMsa57nhcK4bBue5YXCea4cF3gAAAAZcWQIAADAgLAEAABgQlgAAAAwISwAAAAaEJdTY8ePHlZaWpsDAQAUHB2vkyJEqKSkxjjl9+rQyMjIUEhKipk2bKiUlRfn5+VX2/f7773X11VfLZrOpsLCwHmbgHurjPO/cuVOpqamKjIyUn5+foqOj9eKLL9b3VBqVV155RW3atJGvr6/i4uK0ZcsWY/9ly5YpKipKvr6+6tSpk1auXOm037IsTZs2TeHh4fLz81NCQoK+/vrr+pyCW6jL81xeXq4JEyaoU6dOCggIUEREhIYNG6YjR47U9zQavbp+Pf/UqFGjZLPZNHv27Dqu2g1ZQA0lJSVZXbp0sT7//HNrw4YN1vXXX2+lpqYax4waNcqKjIy0MjMzrW3btlk333yz1aNHjyr79u/f3+rdu7clyfrhhx/qYQbuoT7O89/+9jfrwQcftD755BPrm2++sd58803Lz8/Pevnll+t7Oo3CkiVLLG9vb+uNN96wdu/ebd13331WcHCwlZ+fX2X/jRs3Wp6entaf//xna8+ePdaUKVOsJk2aWNnZ2Y4+f/rTn6ygoCBr+fLl1s6dO60777zTuvbaa60ff/yxoabV6NT1eS4sLLQSEhKspUuXWnv37rWysrKs2NhYKyYmpiGn1ejUx+v5nPfee8/q0qWLFRERYb3wwgv1PJPGj7CEGtmzZ48lydq6dauj7aOPPrJsNpt1+PDhKscUFhZaTZo0sZYtW+Zoy8nJsSRZWVlZTn1fffVVq2fPnlZmZuYVHZbq+zz/1AMPPGDdfvvtdVd8IxYbG2tlZGQ4HldUVFgRERHWzJkzq+w/ePBgq2/fvk5tcXFx1h/+8AfLsiyrsrLSstvt1qxZsxz7CwsLLR8fH+vtt9+uhxm4h7o+z1XZsmWLJck6cOBA3RTthurrPB86dMi66qqrrF27dlnXXHMNYcmyLD6GQ41kZWUpODhY3bp1c7QlJCTIw8NDmzdvrnLM9u3bVV5eroSEBEdbVFSUWrduraysLEfbnj17NGPGDC1cuND4g4ZXgvo8zz9XVFSkFi1a1F3xjVRZWZm2b9/udH48PDyUkJBwwfOTlZXl1F+SEhMTHf3379+vvLw8pz5BQUGKi4sznvPLWX2c56oUFRXJZrMpODi4Tup2N/V1nisrKzV06FA9+uij6tixY/0U74au7Hck1FheXp5atWrl1Obl5aUWLVooLy/vgmO8vb3P+0ctLCzMMaa0tFSpqamaNWuWWrduXS+1u5P6Os8/t2nTJi1dulTp6el1UndjduzYMVVUVCgsLMyp3XR+8vLyjP3P/bcmz3m5q4/z/HOnT5/WhAkTlJqaesX+GGx9nednn31WXl5eevDBB+u+aDdGWIIkaeLEibLZbMZt79699Xb8SZMmKTo6WnfddVe9HaMxcPV5/qldu3apf//+mj59un796183yDGBS1VeXq7BgwfLsizNnTvX1eVcVrZv364XX3xRCxYskM1mc3U5jYqXqwtA4zBu3DiNGDHC2Kdt27ay2+0qKChwaj9z5oyOHz8uu91e5Ti73a6ysjIVFhY6XfXIz893jFm3bp2ys7P17rvvSjr7DSNJatmypSZPnqwnn3yyljNrXFx9ns/Zs2ePevXqpfT0dE2ZMqVWc3E3LVu2lKen53nfwqzq/Jxjt9uN/c/9Nz8/X+Hh4U59unbtWofVu4/6OM/nnAtKBw4c0Lp1667Yq0pS/ZznDRs2qKCgwOnqfkVFhcaNG6fZs2fr22+/rdtJuBNXL5qCezm38Hjbtm2OttWrV1dr4fG7777raNu7d6/TwuP//Oc/VnZ2tmN74403LEnWpk2bLvjNjstZfZ1ny7KsXbt2Wa1atbIeffTR+ptAIxUbG2uNHj3a8biiosK66qqrjAtif/Ob3zi1xcfHn7fA+7nnnnPsLyoqYoF3HZ9ny7KssrIyKzk52erYsaNVUFBQP4W7mbo+z8eOHXP6dzg7O9uKiIiwJkyYYO3du7f+JuIGCEuosaSkJOumm26yNm/ebP3rX/+y2rVr5/SV9kOHDlk33HCDtXnzZkfbqFGjrNatW1vr1q2ztm3bZsXHx1vx8fEXPMb69euv6G/DWVb9nOfs7GwrNDTUuuuuu6yjR486tivlzWfJkiWWj4+PtWDBAmvPnj1Wenq6FRwcbOXl5VmWZVlDhw61Jk6c6Oi/ceNGy8vLy3ruueesnJwca/r06VXeOiA4ONj6xz/+YX311VdW//79uXVAHZ/nsrIy684777Suvvpq68svv3R67ZaWlrpkjo1Bfbyef45vw51FWEKNff/991ZqaqrVtGlTKzAw0Lr77rutEydOOPbv37/fkmStX7/e0fbjjz9aDzzwgNW8eXPL39/fGjBggHX06NELHoOwVD/nefr06Zak87ZrrrmmAWfmWi+//LLVunVry9vb24qNjbU+//xzx76ePXtaw4cPd+r/zjvvWO3bt7e8vb2tjh07Wh9++KHT/srKSmvq1KlWWFiY5ePjY/Xq1cvat29fQ0ylUavL83zutV7V9tPX/5Worl/PP0dYOstmWf//4hAAAACch2/DAQAAGBCWAAAADAhLAAAABoQlAAAAA8ISAACAAWEJAADAgLAEAABgQFgCAAAwICwBQD2w2Wxavny5q8sAUAcISwAuOyNGjJDNZjtvS0pKcnVpANyQl6sLAID6kJSUpPnz5zu1+fj4uKgaAO6MK0sALks+Pj6y2+1OW/PmzSWd/Yhs7ty56t27t/z8/NS2bVu9++67TuOzs7N1xx13yM/PTyEhIUpPT1dJSYlTnzfeeEMdO3aUj4+PwsPDNXr0aKf9x44d04ABA+Tv76927dppxYoV9TtpAPWCsATgijR16lSlpKRo586dSktL0+9+9zvl5ORIkk6ePKnExEQ1b95cW7du1bJly7R27VqnMDR37lxlZGQoPT1d2dnZWrFiha6//nqnYzz55JMaPHiwvvrqK/Xp00dpaWk6fvx4g84TQB2wAOAyM3z4cMvT09MKCAhw2p5++mnLsixLkjVq1CinMXFxcdb9999vWZZlvfbaa1bz5s2tkpISx/4PP/zQ8vDwsPLy8izLsqyIiAhr8uTJF6xBkjVlyhTH45KSEkuS9dFHH9XZPAE0DNYsAbgs3X777Zo7d65TW4sWLRx/x8fHO+2Lj4/Xl19+KUnKyclRly5dFBAQ4Nh/yy23qLKyUvv27ZPNZtORI0fUq1cvYw2dO3d2/B0QEKDAwEAVFBTUdkoAXISwBOCyFBAQcN7HYnXFz8+vWv2aNGni9Nhms6mysrI+SgJQj1izBOCK9Pnnn5/3ODo6WpIUHR2tnTt36uTJk479GzdulIeHh2644QY1a9ZMbdq0UWZmZoPWDMA1uLIE4LJUWlqqvLw8pzYvLy+1bNlSkrRs2TJ169ZNv/zlL7Vo0SJt2bJFf/vb3yRJaWlpmj59uoYPH64nnnhC3333ncaMGaOhQ4cqLCxMkvTEE09o1KhRatWqlXr37q0TJ05o48aNGjNmTMNOFEC9IywBuCytWrVK4eHhTm033HCD9u7dK+nsN9WWLFmiBx54QOHh4Xr77bfVoUMHSZK/v79Wr16thx56SN27d5e/v79SUlL0l7/8xfFcw4cP1+nTp/XCCy9o/PjxatmypQYNGtRwEwTQYGyWZVmuLgIAGpLNZtP777+v5ORkV5cCwA2wZgkAAMCAsAQAAGDAmiUAVxxWHwCoCa4sAQAAGBCWAAAADAhLAAAABoQlAAAAA8ISAACAAWEJAADAgLAEAABgQFgCAAAw+P8AYXodpqDb/PcAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(val_temp)\n",
    "plt.plot(train_temp)\n",
    "plt.legend([\"Validation Loss\", \"Training Loss\"])\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
