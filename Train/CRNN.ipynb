{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这个记事本是用来搭建CRNN网络的，用来做recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import torch\n",
    "import torchvision\n",
    "from torch import nn\n",
    "from torchvision import models\n",
    "from torchvision import transforms\n",
    "from utils import *\n",
    "from helper import *\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "from torch.utils import tensorboard as tb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 测试用的代码，不用看（打印出神经网络的结构）\n",
    "# model = models.resnet50(pretrained=True)\n",
    "# # model_cnn = nn.Sequential(*list(model.children())[:-1])\n",
    "# for chi in model.children():\n",
    "#     print(chi)\n",
    "#     print('-------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里用resnet18除去最后的fc作为cnn的部分，lstm作为rnn的部分。<br>\n",
    "输入1x128x128图片<br>\n",
    "经过cnn部分，先是卷到了512x4x4，然后经过平均池化层变成512x1x1<br>\n",
    "然后展平，经过线性变换放入lstm的hidden和cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CRNN(nn.Module):\n",
    "    \"\"\"\n",
    "    这是一个基于CNN和LSTM的CRNN模型，用于生成文本\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, num_classes=128, hidden_dim=1024, io_dim=1024, num_layers=4, bidirectional=True, device='cuda:0'):\n",
    "        \"\"\"\n",
    "        args:\n",
    "            num_classes: ...\n",
    "            hidden_dim: int, the dimension of the hidden state of the LSTM\n",
    "            io_dim: int, the dimension of the input and output of the LSTM\n",
    "            num_layers: int, the number of layers of the LSTM\n",
    "            bidirectional: bool, whether to use bidirectional LSTM\n",
    "            device: str, the device\n",
    "        \"\"\"\n",
    "        super(CRNN, self).__init__()\n",
    "        self.direction_factor = 2 if bidirectional else 1\n",
    "        self.num_layers = num_layers\n",
    "        # num-classes对应ascii码表的128种字符\n",
    "        self.num_classes = num_classes\n",
    "        # hidden_dim是LSTM的隐藏层（hidden state）和细胞状态（cell state）的维度\n",
    "        self.hidden_dim = hidden_dim\n",
    "        # io_dim是LSTM的输入和输出的维度\n",
    "        self.io_dim = io_dim\n",
    "        self.device = device\n",
    "        # max num of characters of the generated text\n",
    "        self.max_len = 64\n",
    "        # 1x1卷积层，用于将灰度图转换为3通道图像以适应ResNet的输入\n",
    "        self.conv1 = nn.Conv2d(1, 3, 1)\n",
    "        # 使用ResNet50作为CNN的基础模型，去掉最后一层全连接层\n",
    "        self.cnn = models.resnet50(weights=models.ResNet50_Weights.DEFAULT)\n",
    "        self.cnn = nn.Sequential(*list(self.cnn.children())[:-1])  # output dim is 2048\n",
    "        # LSTM层，输入维度为io_dim，隐藏层维度为hidden_dim\n",
    "        self.rnn = nn.LSTM(io_dim, hidden_dim, num_layers, batch_first=True, bidirectional=bidirectional)\n",
    "        # 将CNN的输出转换为LSTM的隐藏状态和细胞状态\n",
    "        self.h0_fc = nn.Linear(2048, hidden_dim * num_layers * self.direction_factor)\n",
    "        self.c0_fc = nn.Linear(2048, hidden_dim * num_layers * self.direction_factor)\n",
    "        # 将LSTM的输出转换为最终的输出，即字符概率分布\n",
    "        self.out_fc = nn.Linear(hidden_dim * self.direction_factor, num_classes)\n",
    "        # 将字符的索引转换为字符的embedding\n",
    "        self.embedding = nn.Embedding(num_classes, io_dim)\n",
    "        # dropout防止过拟合\n",
    "        self.dropout = nn.Dropout(0.8)\n",
    "        self.to(device)\n",
    "    \n",
    "    def init_state(self, img):\n",
    "        # 通过CNN卷出 lstm 的 hidden state 和 cell state\n",
    "        x = self.conv1(img)         # batch_size, 3, 64, 64\n",
    "        x = self.cnn(x)             # batch_size, 512, 1, 1\n",
    "        x = x.view(x.size(0), -1)   # batch_size, 512\n",
    "        x = x.unsqueeze(0)          # 1, batch_size, 512\n",
    "        h0 = self.h0_fc(x)          # 1, batch_size, hidden_dim\n",
    "        c0 = self.c0_fc(x)          # 1, batch_size, hidden_dim\n",
    "        h0 = h0.view(-1, self.hidden_dim, self.num_layers * self.direction_factor).permute(2, 0, 1).contiguous()  # 4, batch_size, hidden_dim\n",
    "        c0 = c0.view(-1, self.hidden_dim, self.num_layers * self.direction_factor).permute(2, 0, 1).contiguous()  # 4, batch_size, hidden_dim\n",
    "        return h0, c0\n",
    "    \n",
    "    def next_char(self, x, h_c_n):\n",
    "        # print(\"next char x shape: \", x.shape)\n",
    "        h_n, c_n = h_c_n\n",
    "        # x: the embedding of the last character\n",
    "        # h_n: the hidden state of the last character\n",
    "        # c_n: the cell state of the last character\n",
    "        x, (h_n, c_n) = self.rnn(x, (h_n, c_n))\n",
    "        # print(\"next char rnn output x shape: \", x.shape)\n",
    "        x = self.out_fc(x)\n",
    "        if not self.training:\n",
    "            x = self.dropout(x)\n",
    "        # print(\"next char output x shape: \", x.shape)\n",
    "        return x, (h_n, c_n)\n",
    "    \n",
    "    def forward(self, img):\n",
    "        batch_size = img.size(0)\n",
    "        h0, c0 = self.init_state(img)\n",
    "        x = 2  # the index of the start token\n",
    "        x = torch.tensor([x] * batch_size, dtype=torch.long).view(batch_size, 1).to(self.device)\n",
    "        x = self.embedding(x)\n",
    "        # print(\"after embedding x shape: \", x.shape)\n",
    "        h_c_n = (h0, c0)\n",
    "        temp = torch.zeros(batch_size, 1, self.num_classes).to(self.device)\n",
    "        temp[:, 0, 2] = 1\n",
    "        output = [temp]\n",
    "        for i in range(1, self.max_len):\n",
    "            x, h_c_n = self.next_char(x, h_c_n)\n",
    "            output.append(x)\n",
    "            x = x.argmax(dim=-1)\n",
    "            x = self.embedding(x)\n",
    "        output = torch.cat(output, dim=1)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载数据集\n",
    "# dataset_IAM = RecDataset(\"IAM\", \"train\")\n",
    "# dataset_CVL = RecDataset(\"CVL\", \"train\")\n",
    "dataset_Aug = RecDatasetAug(\"gatsby.txt\")\n",
    "dataset = dataset_Aug\n",
    "# dataset = torch.utils.data.ConcatDataset([dataset_IAM, dataset_CVL])\n",
    "train_loader = DataLoader(dataset, batch_size=10, shuffle=True, num_workers=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 1, 128, 128])\n",
      "torch.Size([10, 64, 128])\n"
     ]
    }
   ],
   "source": [
    "# 测试CRNN进行预测时的形状是否符合要求\n",
    "\n",
    "crnn = CRNN()\n",
    "for step, (img, label) in enumerate(train_loader):\n",
    "    img, label = img.to(crnn.device), label.to(crnn.device)\n",
    "    print(img.shape)\n",
    "    output = crnn(img)\n",
    "    print(output.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 1, 128, 128])\n",
      "torch.Size([8, 10, 1024]) torch.Size([8, 10, 1024])\n"
     ]
    }
   ],
   "source": [
    "# 测试CRNN进行训练时的形状是否符合要求\n",
    "\n",
    "crnn = CRNN()\n",
    "for step, (img, label) in enumerate(train_loader):\n",
    "    img, label = img.to(crnn.device), label.to(crnn.device)\n",
    "    print(img.shape)\n",
    "    h_n, c_n = crnn.init_state(img)\n",
    "    print(h_n.shape, c_n.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_name(epoch, name):\n",
    "    return f\"{name}_{epoch}.pth\"\n",
    "\n",
    "\n",
    "def get_val_loss(model, dataloader, device):\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
    "    model.eval()\n",
    "    loss = 0\n",
    "    with torch.no_grad():\n",
    "        for step, (img, target) in enumerate(dataloader):\n",
    "            img, target = img.to(device), target.to(device)\n",
    "            h_n, c_n = model.init_state(img)\n",
    "            h_n, c_n = h_n.to(device), c_n.to(device)\n",
    "            output = torch.zeros(target.shape[0], model.max_len, model.num_classes).to(device)\n",
    "            x = target[:, 0]\n",
    "            output[:, 0] = x\n",
    "            x = model.embedding(x.long().unsqueeze(1))\n",
    "            for i in range(model.max_len-1):\n",
    "                x = model.embedding(target[:, i].argmax(-1).unsqueeze(1))\n",
    "                x, (h_n, c_n) = model.next_char(x, (h_n, c_n))\n",
    "                output[:, i+1] = x.squeeze(1)\n",
    "            loss += criterion(output.view(-1, model.num_classes), target.argmax(-1).view(-1))\n",
    "    return loss / len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Val Loss 2.346468210220337                                                                                                \n",
      "Model saved as CRNN_0.pth\n",
      "Epoch 1, Val Loss 1.7675639390945435                                                                                               \n",
      "Model saved as CRNN_1.pth\n",
      "Epoch 2, Val Loss 1.3417832851409912                                                                                               \n",
      "Model saved as CRNN_2.pth\n",
      "Epoch 3, Val Loss 1.1026983261108398                                                                                               \n",
      "Model saved as CRNN_3.pth\n",
      "Epoch 4, Val Loss 1.020115852355957                                                                                                \n",
      "Model saved as CRNN_4.pth\n",
      "Epoch 5, Val Loss 0.9706825613975525                                                                                               \n",
      "Model saved as CRNN_5.pth\n",
      "Epoch 6, Val Loss 0.9456015229225159                                                                                               \n",
      "Model saved as CRNN_6.pth\n",
      "Epoch 7, Val Loss 0.8996827006340027                                                                                               \n",
      "Model saved as CRNN_7.pth\n",
      "Epoch 8, Val Loss 0.8754121661186218                                                                                               \n",
      "Model saved as CRNN_8.pth\n",
      "Epoch 9, Val Loss 0.8639203310012817                                                                                               \n",
      "Model saved as CRNN_9.pth\n",
      "Epoch 10, Val Loss 0.8530073761940002                                                                                               \n",
      "Model saved as CRNN_10.pth\n",
      "Epoch 11, Val Loss 0.8667446970939636                                                                                               \n",
      "Model saved as CRNN_11.pth\n",
      "Epoch 12, Val Loss 0.8689748644828796                                                                                               \n",
      "Model saved as CRNN_12.pth\n",
      "Epoch 13, Val Loss 0.8607152700424194                                                                                               \n",
      "Model saved as CRNN_13.pth\n",
      "Epoch 14, Iter 1768, Loss 0.7375"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 94\u001b[0m\n\u001b[1;32m     91\u001b[0m model \u001b[38;5;241m=\u001b[39m CRNN()\n\u001b[1;32m     93\u001b[0m \u001b[38;5;66;03m# 开始训练\u001b[39;00m\n\u001b[0;32m---> 94\u001b[0m \u001b[43mtrain_crnn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.01\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m500\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mAug\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;66;03m# 记录：\u001b[39;00m\n\u001b[1;32m     97\u001b[0m \u001b[38;5;66;03m# 一开始lr=0.01\u001b[39;00m\n\u001b[1;32m     98\u001b[0m \u001b[38;5;66;03m# 从epoch=8开始，lr=0.001\u001b[39;00m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;66;03m# 从epoch=15开始，添加dropout=0.8\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[8], line 44\u001b[0m, in \u001b[0;36mtrain_crnn\u001b[0;34m(model, lr, epochs, start_epoch, name, device)\u001b[0m\n\u001b[1;32m     42\u001b[0m     output[:, i\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     43\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(output\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, model\u001b[38;5;241m.\u001b[39mnum_classes), target\u001b[38;5;241m.\u001b[39margmax(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m---> 44\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;66;03m# 将cnn的梯度调小，防止过拟合\u001b[39;00m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m param \u001b[38;5;129;01min\u001b[39;00m model\u001b[38;5;241m.\u001b[39mcnn\u001b[38;5;241m.\u001b[39mparameters():\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/torch/_tensor.py:525\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    515\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    517\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    518\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    523\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    524\u001b[0m     )\n\u001b[0;32m--> 525\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    526\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    527\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/torch/autograd/__init__.py:267\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    262\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    266\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 267\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    275\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/torch/autograd/graph.py:744\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    742\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    743\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 744\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    745\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    746\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    747\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    748\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 下面是训练的代码，使用教师强制训练\n",
    "def train_crnn(model, lr, epochs, start_epoch, name, device='cuda:0'):\n",
    "    \"\"\"\n",
    "    args:\n",
    "        model: CRNN, the model to be trained\n",
    "        lr: float, the learning rate\n",
    "        epochs: int, the number of epochs to train\n",
    "        device: str, the device to use\n",
    "        start_epoch: int, the epoch to start training\n",
    "        name: str, the name of the model or the task\n",
    "    \"\"\"\n",
    "\n",
    "    # 如果start_epoch不为0，说明是从某个epoch开始训练的，需要加载模型\n",
    "    if start_epoch:\n",
    "        model_name = get_model_name(start_epoch, name)\n",
    "        model_path = \"/root/autodl-tmp/APS360_Project/Machine_Learning_Output/CRNN/\"\n",
    "        model.load_state_dict(torch.load(model_path + model_name))\n",
    "        start_epoch += 1\n",
    "        print(f\"Model loaded from {model_name}\")\n",
    "\n",
    "    writer = tb.SummaryWriter('/root/tf-logs')  # tensorboard writer，可以在浏览器中查看训练过程\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=0.9, weight_decay=1e-4)\n",
    "    model.to(device)\n",
    "    t0 = time.time()\n",
    "    for epoch in range(start_epoch, epochs):\n",
    "        model.train()\n",
    "        for step, (img, target) in enumerate(train_loader):\n",
    "            img, target = img.to(device), target.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            loss = 0\n",
    "            # 准备初始状态\n",
    "            h_n, c_n = model.init_state(img)\n",
    "            h_n, c_n = h_n.to(device), c_n.to(device)\n",
    "            output = torch.zeros(target.shape[0], model.max_len, model.num_classes).to(device)\n",
    "            x = target[:, 0]\n",
    "            output[:, 0] = x\n",
    "            x = model.embedding(x.long().unsqueeze(1))\n",
    "            for i in range(model.max_len-1):\n",
    "                x = model.embedding(target[:, i].argmax(-1).unsqueeze(1))\n",
    "                x, (h_n, c_n) = model.next_char(x, (h_n, c_n))\n",
    "                output[:, i+1] = x.squeeze(1)\n",
    "            loss = criterion(output.view(-1, model.num_classes), target.argmax(-1).view(-1))\n",
    "            loss.backward()\n",
    "            # 将cnn的梯度调小，防止过拟合\n",
    "            for param in model.cnn.parameters():\n",
    "                param.grad *= 0.1\n",
    "            optimizer.step()\n",
    "            if time.time() - t0 > 1:\n",
    "                # 每秒打印一次训练信息\n",
    "                t0 = time.time()\n",
    "                print(f\"\\rEpoch {epoch}, Iter {step}, Loss {loss.item():.4f}\", end=\"\")\n",
    "                writer.add_scalar(f'{name}/Train_Loss', loss.item(), epoch * len(train_loader) + step)\n",
    "        \n",
    "        # 计算验证集上的loss\n",
    "        val_loss = get_val_loss(model, val_loader, device)\n",
    "        print(\" \" * 100, end=\"\")\n",
    "        print(f\"\\rEpoch {epoch}, Val Loss {val_loss.item()}\", end=\"\")\n",
    "        writer.add_scalar(f'{name}/Val_Loss', val_loss.item(), epoch)\n",
    "\n",
    "        model_name = get_model_name(epoch, name)\n",
    "        model_path = \"/root/autodl-tmp/APS360_Project/Machine_Learning_Output/CRNN/\"\n",
    "        torch.save(model.state_dict(), model_path + model_name)\n",
    "        print(f\"\\nModel saved as {model_name}\")\n",
    "\n",
    "\n",
    "# 数据增广 & 加载数据集\n",
    "trans = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.RandomResizedCrop(size=(128, 128), scale=(0.9, 1.2), ratio=(0.9, 1.1)),\n",
    "    transforms.RandomRotation(10),\n",
    "    transforms.RandomAffine(10),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "val_trans = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "train_set_IAM = RecDataset(\"IAM\", \"train\", trans)\n",
    "train_set_CVL = RecDataset(\"CVL\", \"train\", trans)\n",
    "train_set = torch.utils.data.ConcatDataset([train_set_IAM, train_set_CVL])\n",
    "train_loader = DataLoader(train_set, batch_size=64, shuffle=True, num_workers=16)\n",
    "\n",
    "val_set_IAM = RecDataset(\"IAM\", \"val\", val_trans)\n",
    "val_set_CVL = RecDataset(\"CVL\", \"val\", val_trans)\n",
    "val_set = torch.utils.data.ConcatDataset([val_set_IAM, val_set_CVL])\n",
    "val_loader = DataLoader(val_set, batch_size=64, shuffle=False, num_workers=16)\n",
    "\n",
    "# 创建模型\n",
    "model = CRNN()\n",
    "\n",
    "# 开始训练\n",
    "train_crnn(model, lr=0.01, epochs=500, start_epoch=13, name=\"Aug\")\n",
    "\n",
    "# 记录：\n",
    "# 一开始lr=0.01\n",
    "# 从epoch=8开始，lr=0.001\n",
    "# 从epoch=15开始，添加dropout=0.8"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
