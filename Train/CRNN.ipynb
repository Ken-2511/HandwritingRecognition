{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这个记事本是用来搭建CRNN网络的，用来做recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torchvision import models\n",
    "from utils import *\n",
    "\n",
    "dataset = RecDataset(\"IAM\", \"train\")\n",
    "dataloader = DataLoader(dataset, batch_size=10, shuffle=True, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.12/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/root/miniconda3/lib/python3.12/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "-------------------\n",
      "BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "-------------------\n",
      "ReLU(inplace=True)\n",
      "-------------------\n",
      "MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "-------------------\n",
      "Sequential(\n",
      "  (0): BasicBlock(\n",
      "    (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu): ReLU(inplace=True)\n",
      "    (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (1): BasicBlock(\n",
      "    (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu): ReLU(inplace=True)\n",
      "    (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      ")\n",
      "-------------------\n",
      "Sequential(\n",
      "  (0): BasicBlock(\n",
      "    (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu): ReLU(inplace=True)\n",
      "    (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (downsample): Sequential(\n",
      "      (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (1): BasicBlock(\n",
      "    (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu): ReLU(inplace=True)\n",
      "    (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      ")\n",
      "-------------------\n",
      "Sequential(\n",
      "  (0): BasicBlock(\n",
      "    (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu): ReLU(inplace=True)\n",
      "    (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (downsample): Sequential(\n",
      "      (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (1): BasicBlock(\n",
      "    (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu): ReLU(inplace=True)\n",
      "    (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      ")\n",
      "-------------------\n",
      "Sequential(\n",
      "  (0): BasicBlock(\n",
      "    (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu): ReLU(inplace=True)\n",
      "    (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (downsample): Sequential(\n",
      "      (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (1): BasicBlock(\n",
      "    (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu): ReLU(inplace=True)\n",
      "    (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      ")\n",
      "-------------------\n",
      "AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "-------------------\n",
      "Linear(in_features=512, out_features=1000, bias=True)\n",
      "-------------------\n"
     ]
    }
   ],
   "source": [
    "# test\n",
    "model = models.resnet18(pretrained=True)\n",
    "# model_cnn = nn.Sequential(*list(model.children())[:-1])\n",
    "for chi in model.children():\n",
    "    print(chi)\n",
    "    print('-------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里用resnet18除去最后的fc作为cnn的部分，lstm作为rnn的部分。<br>\n",
    "输入1x128x128图片<br>\n",
    "经过cnn部分，先是卷到了512x4x4，然后经过平均池化层变成512x1x1<br>\n",
    "然后展平，经过线性变换放入lstm的hidden和cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CRNN(nn.Module):\n",
    "    def __init__(self, num_classes=128, hidden_dim=256, io_dim=512, device='cuda:0'):\n",
    "        super(CRNN, self).__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.io_dim = io_dim\n",
    "        self.device = device\n",
    "        self.max_len = 64  # max num of characters of the generated text\n",
    "        self.conv1 = nn.Conv2d(1, 3, 1)\n",
    "        self.cnn = models.resnet18(pretrained=True)\n",
    "        self.cnn = nn.Sequential(*list(self.cnn.children())[:-1])  # output dim is 512\n",
    "        self.rnn = nn.LSTM(io_dim, hidden_dim, 1, batch_first=True)\n",
    "        self.h0_fc = nn.Linear(512, hidden_dim)\n",
    "        self.c0_fc = nn.Linear(512, hidden_dim)\n",
    "        self.out_fc = nn.Linear(hidden_dim, num_classes)\n",
    "        self.embedding = nn.Embedding(num_classes, io_dim)\n",
    "        self.to(device)\n",
    "    \n",
    "    def init_state(self, img):\n",
    "        # 通过CNN卷出 lstm 的 hidden state 和 cell state\n",
    "        x = self.conv1(img)         # batch_size, 3, 64, 64\n",
    "        x = self.cnn(x)             # batch_size, 512, 1, 1\n",
    "        x = x.view(x.size(0), -1)   # batch_size, 512\n",
    "        x = x.unsqueeze(0)          # 1, batch_size, 512\n",
    "        h0 = self.h0_fc(x)          # 1, batch_size, hidden_dim\n",
    "        c0 = self.c0_fc(x)          # 1, batch_size, hidden_dim\n",
    "        return h0, c0\n",
    "    \n",
    "    def next_char(self, x, h_c_n):\n",
    "        # print(\"next char x shape: \", x.shape)\n",
    "        h_n, c_n = h_c_n\n",
    "        # x: the embedding of the last character\n",
    "        # h_n: the hidden state of the last character\n",
    "        # c_n: the cell state of the last character\n",
    "        x, (h_n, c_n) = self.rnn(x, (h_n, c_n))\n",
    "        # print(\"next char rnn output x shape: \", x.shape)\n",
    "        x = self.out_fc(x)\n",
    "        # print(\"next char output x shape: \", x.shape)\n",
    "        return x, (h_n, c_n)\n",
    "    \n",
    "    def forward(self, img):\n",
    "        batch_size = img.size(0)\n",
    "        h0, c0 = self.init_state(img)\n",
    "        x = 2  # the index of the start token\n",
    "        x = torch.tensor([x] * batch_size, dtype=torch.long).view(batch_size, 1).to(self.device)\n",
    "        x = self.embedding(x)\n",
    "        # print(\"after embedding x shape: \", x.shape)\n",
    "        h_c_n = (h0, c0)\n",
    "        output = []\n",
    "        for i in range(self.max_len):\n",
    "            x, h_c_n = self.next_char(x, h_c_n)\n",
    "            output.append(x)\n",
    "            x = x.argmax(dim=-1)\n",
    "            x = self.embedding(x)\n",
    "        output = torch.cat(output, dim=1)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 1, 128, 128])\n",
      "torch.Size([10, 64, 128])\n"
     ]
    }
   ],
   "source": [
    "# 测试CRNN进行预测时的形状是否符合要求\n",
    "\n",
    "crnn = CRNN()\n",
    "for i, (img, label) in enumerate(dataloader):\n",
    "    img, label = img.to(crnn.device), label.to(crnn.device)\n",
    "    print(img.shape)\n",
    "    output = crnn(img)\n",
    "    print(output.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 1, 128, 128])\n",
      "torch.Size([1, 10, 256]) torch.Size([1, 10, 256])\n"
     ]
    }
   ],
   "source": [
    "# 测试CRNN进行训练时的形状是否符合要求\n",
    "\n",
    "crnn = CRNN()\n",
    "for i, (img, label) in enumerate(dataloader):\n",
    "    img, label = img.to(crnn.device), label.to(crnn.device)\n",
    "    print(img.shape)\n",
    "    h0, c0 = crnn.init_state(img)\n",
    "    print(h0.shape, c0.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Iter 0, Loss 4.752656936645508\n",
      "Epoch 0, Iter 10, Loss 4.150956630706787\n",
      "Epoch 0, Iter 20, Loss 2.4928619861602783\n",
      "Epoch 0, Iter 30, Loss 0.9234460592269897\n",
      "Epoch 0, Iter 40, Loss 0.6512908339500427\n",
      "Epoch 0, Iter 50, Loss 0.6668704152107239\n",
      "Epoch 0, Iter 60, Loss 0.6871585845947266\n",
      "Epoch 0, Iter 70, Loss 0.6272826790809631\n",
      "Epoch 0, Iter 80, Loss 0.772942066192627\n",
      "Epoch 0, Iter 90, Loss 0.7369575500488281\n",
      "Epoch 0, Iter 100, Loss 0.6620486974716187\n",
      "Epoch 0, Iter 110, Loss 0.6445013880729675\n",
      "Epoch 0, Iter 120, Loss 0.5646364688873291\n",
      "Epoch 0, Iter 130, Loss 0.5715126395225525\n",
      "Epoch 0, Iter 140, Loss 0.5998734831809998\n",
      "Epoch 0, Iter 150, Loss 0.6283571124076843\n",
      "Epoch 0, Iter 160, Loss 0.5739874243736267\n",
      "Epoch 0, Iter 170, Loss 0.5709889531135559\n",
      "Epoch 0, Iter 180, Loss 0.6036087274551392\n",
      "Epoch 0, Iter 190, Loss 0.5977658033370972\n",
      "Epoch 0, Iter 200, Loss 0.5659731030464172\n",
      "Epoch 0, Iter 210, Loss 0.5328385233879089\n",
      "Epoch 0, Iter 220, Loss 0.573197603225708\n",
      "Epoch 0, Iter 230, Loss 0.5830907821655273\n",
      "Epoch 0, Iter 240, Loss 0.5165957808494568\n",
      "Epoch 0, Iter 250, Loss 0.5027070641517639\n",
      "Epoch 0, Iter 260, Loss 0.4963987171649933\n",
      "Epoch 0, Iter 270, Loss 0.6204150319099426\n",
      "Epoch 0, Iter 280, Loss 0.498219758272171\n",
      "Epoch 0, Iter 290, Loss 0.5267706513404846\n",
      "Epoch 0, Iter 300, Loss 0.5639508366584778\n",
      "Epoch 0, Iter 310, Loss 0.5119527578353882\n",
      "Epoch 0, Iter 320, Loss 0.5048869252204895\n",
      "Epoch 0, Iter 330, Loss 0.5085473656654358\n",
      "Epoch 0, Iter 340, Loss 0.48563238978385925\n",
      "Epoch 0, Iter 350, Loss 0.5025516152381897\n",
      "Epoch 0, Iter 360, Loss 0.5181718468666077\n",
      "Epoch 0, Iter 370, Loss 0.5137366056442261\n",
      "Epoch 0, Iter 380, Loss 0.4812988042831421\n",
      "Epoch 0, Iter 390, Loss 0.48957398533821106\n",
      "Epoch 0, Iter 400, Loss 0.5038316249847412\n",
      "Epoch 0, Iter 410, Loss 0.48856067657470703\n",
      "Epoch 0, Iter 420, Loss 0.5095529556274414\n",
      "Epoch 0, Iter 430, Loss 0.5146377682685852\n",
      "Epoch 0, Iter 440, Loss 0.5262406468391418\n",
      "Epoch 0, Iter 450, Loss 0.45821550488471985\n",
      "Epoch 0, Iter 460, Loss 0.4790186285972595\n",
      "Epoch 0, Iter 470, Loss 0.5076045393943787\n",
      "Epoch 0, Iter 480, Loss 0.523155152797699\n",
      "Epoch 0, Iter 490, Loss 0.4486481547355652\n",
      "Epoch 0, Iter 500, Loss 0.5016058087348938\n",
      "Epoch 0, Iter 510, Loss 0.4134967625141144\n",
      "Epoch 0, Iter 520, Loss 0.4678385853767395\n",
      "Epoch 0, Iter 530, Loss 0.4512280225753784\n",
      "Epoch 0, Iter 540, Loss 0.4586564600467682\n",
      "Epoch 0, Iter 550, Loss 0.4704980254173279\n",
      "Epoch 0, Iter 560, Loss 0.43570610880851746\n",
      "Epoch 0, Iter 570, Loss 0.4300324618816376\n",
      "Epoch 0, Iter 580, Loss 0.47555646300315857\n",
      "Epoch 0, Iter 590, Loss 0.5031972527503967\n",
      "Epoch 0, Iter 600, Loss 0.4870792627334595\n",
      "Epoch 0, Iter 610, Loss 0.47611406445503235\n",
      "Epoch 0, Iter 620, Loss 0.4203951954841614\n",
      "Epoch 0, Iter 630, Loss 0.426921010017395\n",
      "Epoch 0, Iter 640, Loss 0.4152376353740692\n",
      "Epoch 0, Iter 650, Loss 0.47159650921821594\n",
      "Epoch 0, Iter 660, Loss 0.4613051414489746\n",
      "Epoch 0, Iter 670, Loss 0.4375571012496948\n",
      "Epoch 0, Iter 680, Loss 0.452563613653183\n",
      "Epoch 0, Iter 690, Loss 0.48016801476478577\n",
      "Epoch 0, Iter 700, Loss 0.4759460389614105\n",
      "Epoch 0, Iter 710, Loss 0.42715466022491455\n",
      "Epoch 0, Iter 720, Loss 0.4005364775657654\n",
      "Epoch 0, Iter 730, Loss 0.41694286465644836\n",
      "Epoch 0, Iter 740, Loss 0.41801583766937256\n",
      "Epoch 0, Iter 750, Loss 0.45169055461883545\n",
      "Epoch 0, Iter 760, Loss 0.42462679743766785\n",
      "Epoch 0, Iter 770, Loss 0.3743054270744324\n",
      "Epoch 0, Iter 780, Loss 0.3740816116333008\n",
      "Epoch 0, Iter 790, Loss 0.39772629737854004\n",
      "Epoch 0, Iter 800, Loss 0.3823933005332947\n",
      "Epoch 0, Iter 810, Loss 0.485655814409256\n",
      "Epoch 0, Iter 820, Loss 0.4082884192466736\n",
      "Epoch 0, Iter 830, Loss 0.429339200258255\n",
      "Epoch 0, Iter 840, Loss 0.44304966926574707\n",
      "Epoch 0, Iter 850, Loss 0.40475910902023315\n",
      "Epoch 0, Iter 860, Loss 0.40961936116218567\n",
      "Epoch 0, Iter 870, Loss 0.4293828010559082\n",
      "Epoch 0, Iter 880, Loss 0.41244590282440186\n",
      "Epoch 0, Iter 890, Loss 0.410725861787796\n",
      "Epoch 0, Iter 900, Loss 0.4002988934516907\n",
      "Epoch 0, Iter 910, Loss 0.40263447165489197\n",
      "Epoch 0, Iter 920, Loss 0.4214457869529724\n",
      "Epoch 0, Iter 930, Loss 0.43068212270736694\n",
      "Epoch 0, Iter 940, Loss 0.41850391030311584\n",
      "Epoch 0, Iter 950, Loss 0.39982736110687256\n",
      "Epoch 0, Iter 960, Loss 0.3844653367996216\n",
      "Epoch 0, Iter 970, Loss 0.4109150767326355\n",
      "Epoch 0, Iter 980, Loss 0.4141058325767517\n",
      "Epoch 0, Iter 990, Loss 0.3890647292137146\n",
      "Epoch 0, Iter 1000, Loss 0.40208563208580017\n",
      "Epoch 0, Iter 1010, Loss 0.39398521184921265\n",
      "Epoch 0, Iter 1020, Loss 0.39395520091056824\n",
      "Epoch 0, Iter 1030, Loss 0.42469602823257446\n",
      "Epoch 0, Iter 1040, Loss 0.4612208604812622\n",
      "Epoch 0, Iter 1050, Loss 0.4251426160335541\n",
      "Epoch 0, Iter 1060, Loss 0.3907475471496582\n",
      "Epoch 0, Iter 1070, Loss 0.42410901188850403\n",
      "Epoch 0, Iter 1080, Loss 0.37636879086494446\n",
      "Epoch 0, Iter 1090, Loss 0.3668399453163147\n",
      "Epoch 0, Iter 1100, Loss 0.472419410943985\n",
      "Epoch 0, Iter 1110, Loss 0.3645620346069336\n",
      "Epoch 0, Iter 1120, Loss 0.39264652132987976\n",
      "Epoch 0, Iter 1130, Loss 0.4194653332233429\n",
      "Epoch 0, Iter 1140, Loss 0.4436289668083191\n",
      "Epoch 0, Iter 1150, Loss 0.39923757314682007\n",
      "Epoch 0, Iter 1160, Loss 0.3897923529148102\n",
      "Epoch 0, Iter 1170, Loss 0.38950952887535095\n",
      "Epoch 0, Iter 1180, Loss 0.42999574542045593\n",
      "Epoch 0, Iter 1190, Loss 0.40427884459495544\n",
      "Epoch 0, Iter 1200, Loss 0.38139766454696655\n",
      "Model saved as crnn_256_512_0.001_64_10_IAM.pth\n",
      "Epoch 1, Iter 0, Loss 0.3405810296535492\n",
      "Epoch 1, Iter 10, Loss 0.3789963126182556\n",
      "Epoch 1, Iter 20, Loss 0.3454796075820923\n",
      "Epoch 1, Iter 30, Loss 0.3974980115890503\n",
      "Epoch 1, Iter 40, Loss 0.3544796109199524\n",
      "Epoch 1, Iter 50, Loss 0.4177935719490051\n",
      "Epoch 1, Iter 60, Loss 0.39520034193992615\n",
      "Epoch 1, Iter 70, Loss 0.41176265478134155\n",
      "Epoch 1, Iter 80, Loss 0.3590768277645111\n",
      "Epoch 1, Iter 90, Loss 0.3977746069431305\n",
      "Epoch 1, Iter 100, Loss 0.3801269829273224\n",
      "Epoch 1, Iter 110, Loss 0.391417920589447\n",
      "Epoch 1, Iter 120, Loss 0.411038339138031\n",
      "Epoch 1, Iter 130, Loss 0.39332315325737\n",
      "Epoch 1, Iter 140, Loss 0.3628634810447693\n",
      "Epoch 1, Iter 150, Loss 0.3981560468673706\n",
      "Epoch 1, Iter 160, Loss 0.36010095477104187\n",
      "Epoch 1, Iter 170, Loss 0.3616817593574524\n",
      "Epoch 1, Iter 180, Loss 0.40159115195274353\n",
      "Epoch 1, Iter 190, Loss 0.3583240807056427\n",
      "Epoch 1, Iter 200, Loss 0.3617819547653198\n",
      "Epoch 1, Iter 210, Loss 0.4136202037334442\n",
      "Epoch 1, Iter 220, Loss 0.37254804372787476\n",
      "Epoch 1, Iter 230, Loss 0.32515740394592285\n",
      "Epoch 1, Iter 240, Loss 0.3873489201068878\n",
      "Epoch 1, Iter 250, Loss 0.400989830493927\n",
      "Epoch 1, Iter 260, Loss 0.32025307416915894\n",
      "Epoch 1, Iter 270, Loss 0.35875529050827026\n",
      "Epoch 1, Iter 280, Loss 0.3241739273071289\n",
      "Epoch 1, Iter 290, Loss 0.34911346435546875\n",
      "Epoch 1, Iter 300, Loss 0.3654172718524933\n",
      "Epoch 1, Iter 310, Loss 0.33155813813209534\n",
      "Epoch 1, Iter 320, Loss 0.36595743894577026\n",
      "Epoch 1, Iter 330, Loss 0.3480561375617981\n",
      "Epoch 1, Iter 340, Loss 0.35966256260871887\n",
      "Epoch 1, Iter 350, Loss 0.3720899224281311\n",
      "Epoch 1, Iter 360, Loss 0.3576822280883789\n",
      "Epoch 1, Iter 370, Loss 0.3481023907661438\n",
      "Epoch 1, Iter 380, Loss 0.32999488711357117\n",
      "Epoch 1, Iter 390, Loss 0.384550005197525\n",
      "Epoch 1, Iter 400, Loss 0.32760828733444214\n",
      "Epoch 1, Iter 410, Loss 0.37616461515426636\n",
      "Epoch 1, Iter 420, Loss 0.37186840176582336\n",
      "Epoch 1, Iter 430, Loss 0.32152655720710754\n",
      "Epoch 1, Iter 440, Loss 0.3333337903022766\n",
      "Epoch 1, Iter 450, Loss 0.3498339354991913\n",
      "Epoch 1, Iter 460, Loss 0.3617093563079834\n",
      "Epoch 1, Iter 470, Loss 0.33874934911727905\n",
      "Epoch 1, Iter 480, Loss 0.34143197536468506\n",
      "Epoch 1, Iter 490, Loss 0.3252027630805969\n",
      "Epoch 1, Iter 500, Loss 0.3332917392253876\n",
      "Epoch 1, Iter 510, Loss 0.3575441837310791\n",
      "Epoch 1, Iter 520, Loss 0.30751392245292664\n",
      "Epoch 1, Iter 530, Loss 0.3269611597061157\n",
      "Epoch 1, Iter 540, Loss 0.35287928581237793\n",
      "Epoch 1, Iter 550, Loss 0.35024088621139526\n",
      "Epoch 1, Iter 560, Loss 0.3293111324310303\n",
      "Epoch 1, Iter 570, Loss 0.34688350558280945\n",
      "Epoch 1, Iter 580, Loss 0.36448588967323303\n",
      "Epoch 1, Iter 590, Loss 0.3267056345939636\n",
      "Epoch 1, Iter 600, Loss 0.3149226903915405\n",
      "Epoch 1, Iter 610, Loss 0.3625756502151489\n",
      "Epoch 1, Iter 620, Loss 0.326125830411911\n",
      "Epoch 1, Iter 630, Loss 0.33986932039260864\n",
      "Epoch 1, Iter 640, Loss 0.3911603093147278\n",
      "Epoch 1, Iter 650, Loss 0.37151792645454407\n",
      "Epoch 1, Iter 660, Loss 0.31838104128837585\n",
      "Epoch 1, Iter 670, Loss 0.2940994203090668\n",
      "Epoch 1, Iter 680, Loss 0.3360377550125122\n",
      "Epoch 1, Iter 690, Loss 0.36016055941581726\n",
      "Epoch 1, Iter 700, Loss 0.36129289865493774\n",
      "Epoch 1, Iter 710, Loss 0.3736536502838135\n",
      "Epoch 1, Iter 720, Loss 0.3547520637512207\n",
      "Epoch 1, Iter 730, Loss 0.3592682182788849\n",
      "Epoch 1, Iter 740, Loss 0.3353607952594757\n",
      "Epoch 1, Iter 750, Loss 0.3537399172782898\n",
      "Epoch 1, Iter 760, Loss 0.3045204281806946\n",
      "Epoch 1, Iter 770, Loss 0.30837589502334595\n",
      "Epoch 1, Iter 780, Loss 0.3438762128353119\n",
      "Epoch 1, Iter 790, Loss 0.3574868142604828\n",
      "Epoch 1, Iter 800, Loss 0.3767915666103363\n",
      "Epoch 1, Iter 810, Loss 0.29417672753334045\n",
      "Epoch 1, Iter 820, Loss 0.30869099497795105\n",
      "Epoch 1, Iter 830, Loss 0.35559582710266113\n",
      "Epoch 1, Iter 840, Loss 0.3233206272125244\n",
      "Epoch 1, Iter 850, Loss 0.330363005399704\n",
      "Epoch 1, Iter 860, Loss 0.35038331151008606\n",
      "Epoch 1, Iter 870, Loss 0.3075324594974518\n",
      "Epoch 1, Iter 880, Loss 0.31356510519981384\n",
      "Epoch 1, Iter 890, Loss 0.3359619081020355\n",
      "Epoch 1, Iter 900, Loss 0.32819414138793945\n",
      "Epoch 1, Iter 910, Loss 0.35358843207359314\n",
      "Epoch 1, Iter 920, Loss 0.33100181818008423\n",
      "Epoch 1, Iter 930, Loss 0.33966949582099915\n",
      "Epoch 1, Iter 940, Loss 0.358424574136734\n",
      "Epoch 1, Iter 950, Loss 0.3305530846118927\n",
      "Epoch 1, Iter 960, Loss 0.3351329565048218\n",
      "Epoch 1, Iter 970, Loss 0.32574230432510376\n",
      "Epoch 1, Iter 980, Loss 0.32199281454086304\n",
      "Epoch 1, Iter 990, Loss 0.3355041742324829\n",
      "Epoch 1, Iter 1000, Loss 0.37546366453170776\n",
      "Epoch 1, Iter 1010, Loss 0.32798540592193604\n",
      "Epoch 1, Iter 1020, Loss 0.31114158034324646\n",
      "Epoch 1, Iter 1030, Loss 0.3173086643218994\n",
      "Epoch 1, Iter 1040, Loss 0.3383258581161499\n",
      "Epoch 1, Iter 1050, Loss 0.3244236707687378\n",
      "Epoch 1, Iter 1060, Loss 0.37672990560531616\n",
      "Epoch 1, Iter 1070, Loss 0.26590609550476074\n",
      "Epoch 1, Iter 1080, Loss 0.3501741588115692\n",
      "Epoch 1, Iter 1090, Loss 0.2995701730251312\n",
      "Epoch 1, Iter 1100, Loss 0.3151923418045044\n",
      "Epoch 1, Iter 1110, Loss 0.3200487792491913\n",
      "Epoch 1, Iter 1120, Loss 0.2846737802028656\n",
      "Epoch 1, Iter 1130, Loss 0.3315820097923279\n",
      "Epoch 1, Iter 1140, Loss 0.2996833622455597\n",
      "Epoch 1, Iter 1150, Loss 0.32223787903785706\n",
      "Epoch 1, Iter 1160, Loss 0.2868688404560089\n",
      "Epoch 1, Iter 1170, Loss 0.3068839907646179\n",
      "Epoch 1, Iter 1180, Loss 0.3448721468448639\n",
      "Epoch 1, Iter 1190, Loss 0.3282812237739563\n",
      "Epoch 1, Iter 1200, Loss 0.3097221553325653\n",
      "Model saved as crnn_256_512_0.001_64_10_IAM.pth\n",
      "Epoch 2, Iter 0, Loss 0.31053462624549866\n",
      "Epoch 2, Iter 10, Loss 0.30321264266967773\n",
      "Epoch 2, Iter 20, Loss 0.33242860436439514\n",
      "Epoch 2, Iter 30, Loss 0.2914094626903534\n",
      "Epoch 2, Iter 40, Loss 0.3422965705394745\n",
      "Epoch 2, Iter 50, Loss 0.2989693880081177\n",
      "Epoch 2, Iter 60, Loss 0.3131242096424103\n",
      "Epoch 2, Iter 70, Loss 0.29343757033348083\n",
      "Epoch 2, Iter 80, Loss 0.32461339235305786\n",
      "Epoch 2, Iter 90, Loss 0.31004735827445984\n",
      "Epoch 2, Iter 100, Loss 0.3625674247741699\n",
      "Epoch 2, Iter 110, Loss 0.337307333946228\n",
      "Epoch 2, Iter 120, Loss 0.3170575797557831\n",
      "Epoch 2, Iter 130, Loss 0.2873416543006897\n",
      "Epoch 2, Iter 140, Loss 0.3061949610710144\n",
      "Epoch 2, Iter 150, Loss 0.3025394082069397\n",
      "Epoch 2, Iter 160, Loss 0.3349500894546509\n",
      "Epoch 2, Iter 170, Loss 0.28905820846557617\n",
      "Epoch 2, Iter 180, Loss 0.3409813344478607\n",
      "Epoch 2, Iter 190, Loss 0.32435280084609985\n",
      "Epoch 2, Iter 200, Loss 0.3192344903945923\n",
      "Epoch 2, Iter 210, Loss 0.29098835587501526\n",
      "Epoch 2, Iter 220, Loss 0.2956502139568329\n",
      "Epoch 2, Iter 230, Loss 0.3503788411617279\n",
      "Epoch 2, Iter 240, Loss 0.3502698540687561\n",
      "Epoch 2, Iter 250, Loss 0.29741567373275757\n",
      "Epoch 2, Iter 260, Loss 0.32822418212890625\n",
      "Epoch 2, Iter 270, Loss 0.30920422077178955\n",
      "Epoch 2, Iter 280, Loss 0.27498772740364075\n",
      "Epoch 2, Iter 290, Loss 0.3254220485687256\n",
      "Epoch 2, Iter 300, Loss 0.2969556152820587\n",
      "Epoch 2, Iter 310, Loss 0.3344232439994812\n",
      "Epoch 2, Iter 320, Loss 0.3583606481552124\n",
      "Epoch 2, Iter 330, Loss 0.32504355907440186\n",
      "Epoch 2, Iter 340, Loss 0.3055953085422516\n",
      "Epoch 2, Iter 350, Loss 0.31556689739227295\n",
      "Epoch 2, Iter 360, Loss 0.30223792791366577\n",
      "Epoch 2, Iter 370, Loss 0.3131949007511139\n",
      "Epoch 2, Iter 380, Loss 0.3672229051589966\n",
      "Epoch 2, Iter 390, Loss 0.3182927668094635\n",
      "Epoch 2, Iter 400, Loss 0.30751413106918335\n",
      "Epoch 2, Iter 410, Loss 0.28270190954208374\n",
      "Epoch 2, Iter 420, Loss 0.3025273084640503\n",
      "Epoch 2, Iter 430, Loss 0.30599355697631836\n",
      "Epoch 2, Iter 440, Loss 0.3047715127468109\n",
      "Epoch 2, Iter 450, Loss 0.34615394473075867\n",
      "Epoch 2, Iter 460, Loss 0.3031690716743469\n",
      "Epoch 2, Iter 470, Loss 0.29326361417770386\n",
      "Epoch 2, Iter 480, Loss 0.33334413170814514\n",
      "Epoch 2, Iter 490, Loss 0.3137933015823364\n",
      "Epoch 2, Iter 500, Loss 0.2985920011997223\n",
      "Epoch 2, Iter 510, Loss 0.3013087511062622\n",
      "Epoch 2, Iter 520, Loss 0.296951562166214\n",
      "Epoch 2, Iter 530, Loss 0.2896839678287506\n",
      "Epoch 2, Iter 540, Loss 0.2844680845737457\n",
      "Epoch 2, Iter 550, Loss 0.3210262060165405\n",
      "Epoch 2, Iter 560, Loss 0.2911025881767273\n",
      "Epoch 2, Iter 570, Loss 0.34413498640060425\n",
      "Epoch 2, Iter 580, Loss 0.27777668833732605\n",
      "Epoch 2, Iter 590, Loss 0.283840149641037\n",
      "Epoch 2, Iter 600, Loss 0.2968269884586334\n",
      "Epoch 2, Iter 610, Loss 0.31189340353012085\n",
      "Epoch 2, Iter 620, Loss 0.3094582259654999\n",
      "Epoch 2, Iter 630, Loss 0.3077719211578369\n",
      "Epoch 2, Iter 640, Loss 0.3790827691555023\n",
      "Epoch 2, Iter 650, Loss 0.28367382287979126\n",
      "Epoch 2, Iter 660, Loss 0.31269365549087524\n",
      "Epoch 2, Iter 670, Loss 0.3021366596221924\n",
      "Epoch 2, Iter 680, Loss 0.3001287281513214\n",
      "Epoch 2, Iter 690, Loss 0.3202633261680603\n",
      "Epoch 2, Iter 700, Loss 0.29069527983665466\n",
      "Epoch 2, Iter 710, Loss 0.3120005428791046\n",
      "Epoch 2, Iter 720, Loss 0.29920393228530884\n",
      "Epoch 2, Iter 730, Loss 0.29995429515838623\n",
      "Epoch 2, Iter 740, Loss 0.322858601808548\n",
      "Epoch 2, Iter 750, Loss 0.2961633503437042\n",
      "Epoch 2, Iter 760, Loss 0.2990412414073944\n",
      "Epoch 2, Iter 770, Loss 0.291641503572464\n",
      "Epoch 2, Iter 780, Loss 0.337223619222641\n",
      "Epoch 2, Iter 790, Loss 0.2931145131587982\n",
      "Epoch 2, Iter 800, Loss 0.28067004680633545\n",
      "Epoch 2, Iter 810, Loss 0.2796909511089325\n",
      "Epoch 2, Iter 820, Loss 0.3019315302371979\n",
      "Epoch 2, Iter 830, Loss 0.3081931173801422\n",
      "Epoch 2, Iter 840, Loss 0.3268250524997711\n",
      "Epoch 2, Iter 850, Loss 0.26562628149986267\n",
      "Epoch 2, Iter 860, Loss 0.31352031230926514\n",
      "Epoch 2, Iter 870, Loss 0.32152771949768066\n",
      "Epoch 2, Iter 880, Loss 0.2922535240650177\n",
      "Epoch 2, Iter 890, Loss 0.26302751898765564\n",
      "Epoch 2, Iter 900, Loss 0.3092389404773712\n",
      "Epoch 2, Iter 910, Loss 0.26688992977142334\n",
      "Epoch 2, Iter 920, Loss 0.32503771781921387\n",
      "Epoch 2, Iter 930, Loss 0.2731442153453827\n",
      "Epoch 2, Iter 940, Loss 0.29350942373275757\n",
      "Epoch 2, Iter 950, Loss 0.278668612241745\n",
      "Epoch 2, Iter 960, Loss 0.2783683240413666\n",
      "Epoch 2, Iter 970, Loss 0.29576513171195984\n",
      "Epoch 2, Iter 980, Loss 0.2924557328224182\n",
      "Epoch 2, Iter 990, Loss 0.31885477900505066\n",
      "Epoch 2, Iter 1000, Loss 0.30834537744522095\n",
      "Epoch 2, Iter 1010, Loss 0.3038384020328522\n",
      "Epoch 2, Iter 1020, Loss 0.2920600175857544\n",
      "Epoch 2, Iter 1030, Loss 0.322628378868103\n",
      "Epoch 2, Iter 1040, Loss 0.28485697507858276\n",
      "Epoch 2, Iter 1050, Loss 0.3006458282470703\n",
      "Epoch 2, Iter 1060, Loss 0.3248361647129059\n",
      "Epoch 2, Iter 1070, Loss 0.32088154554367065\n",
      "Epoch 2, Iter 1080, Loss 0.326789915561676\n",
      "Epoch 2, Iter 1090, Loss 0.3567192852497101\n",
      "Epoch 2, Iter 1100, Loss 0.3695489466190338\n",
      "Epoch 2, Iter 1110, Loss 0.3146960437297821\n",
      "Epoch 2, Iter 1120, Loss 0.3301266133785248\n",
      "Epoch 2, Iter 1130, Loss 0.3520268499851227\n",
      "Epoch 2, Iter 1140, Loss 0.36278268694877625\n",
      "Epoch 2, Iter 1150, Loss 0.28391581773757935\n",
      "Epoch 2, Iter 1160, Loss 0.3238544166088104\n",
      "Epoch 2, Iter 1170, Loss 0.32166850566864014\n",
      "Epoch 2, Iter 1180, Loss 0.3378734588623047\n",
      "Epoch 2, Iter 1190, Loss 0.330428808927536\n",
      "Epoch 2, Iter 1200, Loss 0.3313722014427185\n",
      "Model saved as crnn_256_512_0.001_64_10_IAM.pth\n",
      "Epoch 3, Iter 0, Loss 0.3117293417453766\n",
      "Epoch 3, Iter 10, Loss 0.3377431035041809\n",
      "Epoch 3, Iter 20, Loss 0.29159241914749146\n",
      "Epoch 3, Iter 30, Loss 0.3026927411556244\n",
      "Epoch 3, Iter 40, Loss 0.3009336292743683\n",
      "Epoch 3, Iter 50, Loss 0.30986344814300537\n",
      "Epoch 3, Iter 60, Loss 0.3245304822921753\n",
      "Epoch 3, Iter 70, Loss 0.321722149848938\n",
      "Epoch 3, Iter 80, Loss 0.2815517485141754\n",
      "Epoch 3, Iter 90, Loss 0.30021435022354126\n",
      "Epoch 3, Iter 100, Loss 0.34824997186660767\n",
      "Epoch 3, Iter 110, Loss 0.3346593976020813\n",
      "Epoch 3, Iter 120, Loss 0.30043619871139526\n",
      "Epoch 3, Iter 130, Loss 0.31766605377197266\n",
      "Epoch 3, Iter 140, Loss 0.30054599046707153\n",
      "Epoch 3, Iter 150, Loss 0.3260907232761383\n",
      "Epoch 3, Iter 160, Loss 0.30074015259742737\n",
      "Epoch 3, Iter 170, Loss 0.2990705668926239\n",
      "Epoch 3, Iter 180, Loss 0.3259018063545227\n",
      "Epoch 3, Iter 190, Loss 0.3138032555580139\n",
      "Epoch 3, Iter 200, Loss 0.344256192445755\n",
      "Epoch 3, Iter 210, Loss 0.29561784863471985\n",
      "Epoch 3, Iter 220, Loss 0.2883358895778656\n",
      "Epoch 3, Iter 230, Loss 0.3188253939151764\n",
      "Epoch 3, Iter 240, Loss 0.29966986179351807\n",
      "Epoch 3, Iter 250, Loss 0.31662461161613464\n",
      "Epoch 3, Iter 260, Loss 0.334555447101593\n",
      "Epoch 3, Iter 270, Loss 0.3094789683818817\n",
      "Epoch 3, Iter 280, Loss 0.28836899995803833\n",
      "Epoch 3, Iter 290, Loss 0.36717769503593445\n",
      "Epoch 3, Iter 300, Loss 0.28430914878845215\n",
      "Epoch 3, Iter 310, Loss 0.3086533844470978\n",
      "Epoch 3, Iter 320, Loss 0.30071014165878296\n",
      "Epoch 3, Iter 330, Loss 0.34354278445243835\n",
      "Epoch 3, Iter 340, Loss 0.3120410144329071\n",
      "Epoch 3, Iter 350, Loss 0.30504775047302246\n",
      "Epoch 3, Iter 360, Loss 0.3191649317741394\n",
      "Epoch 3, Iter 370, Loss 0.2709539532661438\n",
      "Epoch 3, Iter 380, Loss 0.27395352721214294\n",
      "Epoch 3, Iter 390, Loss 0.28826460242271423\n",
      "Epoch 3, Iter 400, Loss 0.3043111264705658\n",
      "Epoch 3, Iter 410, Loss 0.26351305842399597\n",
      "Epoch 3, Iter 420, Loss 0.2948058247566223\n",
      "Epoch 3, Iter 430, Loss 0.31716158986091614\n",
      "Epoch 3, Iter 440, Loss 0.29312312602996826\n",
      "Epoch 3, Iter 450, Loss 0.28815191984176636\n",
      "Epoch 3, Iter 460, Loss 0.3275710940361023\n",
      "Epoch 3, Iter 470, Loss 0.31656473875045776\n",
      "Epoch 3, Iter 480, Loss 0.30630016326904297\n",
      "Epoch 3, Iter 490, Loss 0.30565232038497925\n",
      "Epoch 3, Iter 500, Loss 0.30318334698677063\n",
      "Epoch 3, Iter 510, Loss 0.2830451726913452\n",
      "Epoch 3, Iter 520, Loss 0.3023873269557953\n",
      "Epoch 3, Iter 530, Loss 0.2750248908996582\n",
      "Epoch 3, Iter 540, Loss 0.31109389662742615\n",
      "Epoch 3, Iter 550, Loss 0.2830764949321747\n",
      "Epoch 3, Iter 560, Loss 0.2657780349254608\n",
      "Epoch 3, Iter 570, Loss 0.31702920794487\n",
      "Epoch 3, Iter 580, Loss 0.28925055265426636\n",
      "Epoch 3, Iter 590, Loss 0.30275607109069824\n",
      "Epoch 3, Iter 600, Loss 0.28751733899116516\n",
      "Epoch 3, Iter 610, Loss 0.27407708764076233\n",
      "Epoch 3, Iter 620, Loss 0.27682778239250183\n",
      "Epoch 3, Iter 630, Loss 0.26765501499176025\n",
      "Epoch 3, Iter 640, Loss 0.339477002620697\n",
      "Epoch 3, Iter 650, Loss 0.28916823863983154\n",
      "Epoch 3, Iter 660, Loss 0.3357540965080261\n",
      "Epoch 3, Iter 670, Loss 0.2885403037071228\n",
      "Epoch 3, Iter 680, Loss 0.29762598872184753\n",
      "Epoch 3, Iter 690, Loss 0.28532472252845764\n",
      "Epoch 3, Iter 700, Loss 0.2769193649291992\n",
      "Epoch 3, Iter 710, Loss 0.33701229095458984\n",
      "Epoch 3, Iter 720, Loss 0.3167116343975067\n",
      "Epoch 3, Iter 730, Loss 0.3511028289794922\n",
      "Epoch 3, Iter 740, Loss 0.28225961327552795\n",
      "Epoch 3, Iter 750, Loss 0.2562333941459656\n",
      "Epoch 3, Iter 760, Loss 0.33577480912208557\n",
      "Epoch 3, Iter 770, Loss 0.3381381034851074\n",
      "Epoch 3, Iter 780, Loss 0.3052261769771576\n",
      "Epoch 3, Iter 790, Loss 0.27219733595848083\n",
      "Epoch 3, Iter 800, Loss 0.27740687131881714\n",
      "Epoch 3, Iter 810, Loss 0.2935713529586792\n",
      "Epoch 3, Iter 820, Loss 0.3212856948375702\n",
      "Epoch 3, Iter 830, Loss 0.2998819351196289\n",
      "Epoch 3, Iter 840, Loss 0.27573978900909424\n",
      "Epoch 3, Iter 850, Loss 0.28978994488716125\n",
      "Epoch 3, Iter 860, Loss 0.3005619943141937\n",
      "Epoch 3, Iter 870, Loss 0.32387036085128784\n",
      "Epoch 3, Iter 880, Loss 0.29268679022789\n",
      "Epoch 3, Iter 890, Loss 0.2770693898200989\n",
      "Epoch 3, Iter 900, Loss 0.3114008903503418\n",
      "Epoch 3, Iter 910, Loss 0.3013618290424347\n",
      "Epoch 3, Iter 920, Loss 0.3157441020011902\n",
      "Epoch 3, Iter 930, Loss 0.31368622183799744\n",
      "Epoch 3, Iter 940, Loss 0.2906036376953125\n",
      "Epoch 3, Iter 950, Loss 0.2636309266090393\n",
      "Epoch 3, Iter 960, Loss 0.2688214182853699\n",
      "Epoch 3, Iter 970, Loss 0.28073474764823914\n",
      "Epoch 3, Iter 980, Loss 0.2994375228881836\n",
      "Epoch 3, Iter 990, Loss 0.27066588401794434\n",
      "Epoch 3, Iter 1000, Loss 0.28011706471443176\n",
      "Epoch 3, Iter 1010, Loss 0.28852880001068115\n",
      "Epoch 3, Iter 1020, Loss 0.2961123287677765\n",
      "Epoch 3, Iter 1030, Loss 0.2986717224121094\n",
      "Epoch 3, Iter 1040, Loss 0.264598548412323\n",
      "Epoch 3, Iter 1050, Loss 0.31232160329818726\n",
      "Epoch 3, Iter 1060, Loss 0.2946004569530487\n",
      "Epoch 3, Iter 1070, Loss 0.27604028582572937\n",
      "Epoch 3, Iter 1080, Loss 0.2920265793800354\n",
      "Epoch 3, Iter 1090, Loss 0.3044872581958771\n",
      "Epoch 3, Iter 1100, Loss 0.29932963848114014\n",
      "Epoch 3, Iter 1110, Loss 0.2867174744606018\n",
      "Epoch 3, Iter 1120, Loss 0.2854746878147125\n",
      "Epoch 3, Iter 1130, Loss 0.2448650300502777\n",
      "Epoch 3, Iter 1140, Loss 0.310372918844223\n",
      "Epoch 3, Iter 1150, Loss 0.2789754271507263\n",
      "Epoch 3, Iter 1160, Loss 0.31744131445884705\n",
      "Epoch 3, Iter 1170, Loss 0.320146769285202\n",
      "Epoch 3, Iter 1180, Loss 0.2632642686367035\n",
      "Epoch 3, Iter 1190, Loss 0.2887372076511383\n",
      "Epoch 3, Iter 1200, Loss 0.3105844259262085\n",
      "Model saved as crnn_256_512_0.001_64_10_IAM.pth\n",
      "Epoch 4, Iter 0, Loss 0.272201806306839\n",
      "Epoch 4, Iter 10, Loss 0.29927554726600647\n",
      "Epoch 4, Iter 20, Loss 0.2717183828353882\n",
      "Epoch 4, Iter 30, Loss 0.2855575978755951\n",
      "Epoch 4, Iter 40, Loss 0.24314473569393158\n",
      "Epoch 4, Iter 50, Loss 0.2697148323059082\n",
      "Epoch 4, Iter 60, Loss 0.27942249178886414\n",
      "Epoch 4, Iter 70, Loss 0.2938541769981384\n",
      "Epoch 4, Iter 80, Loss 0.28752484917640686\n",
      "Epoch 4, Iter 90, Loss 0.28462302684783936\n",
      "Epoch 4, Iter 100, Loss 0.26914840936660767\n",
      "Epoch 4, Iter 110, Loss 0.26012685894966125\n",
      "Epoch 4, Iter 120, Loss 0.2690412700176239\n",
      "Epoch 4, Iter 130, Loss 0.28428488969802856\n",
      "Epoch 4, Iter 140, Loss 0.2885017693042755\n",
      "Epoch 4, Iter 150, Loss 0.3229175806045532\n",
      "Epoch 4, Iter 160, Loss 0.26509466767311096\n",
      "Epoch 4, Iter 170, Loss 0.2719617784023285\n",
      "Epoch 4, Iter 180, Loss 0.2859952449798584\n",
      "Epoch 4, Iter 190, Loss 0.2928917706012726\n",
      "Epoch 4, Iter 200, Loss 0.2808328866958618\n",
      "Epoch 4, Iter 210, Loss 0.3045197129249573\n",
      "Epoch 4, Iter 220, Loss 0.31045347452163696\n",
      "Epoch 4, Iter 230, Loss 0.29219192266464233\n",
      "Epoch 4, Iter 240, Loss 0.3268347382545471\n",
      "Epoch 4, Iter 250, Loss 0.2638665735721588\n",
      "Epoch 4, Iter 260, Loss 0.32522451877593994\n",
      "Epoch 4, Iter 270, Loss 0.2915990650653839\n",
      "Epoch 4, Iter 280, Loss 0.2662779986858368\n",
      "Epoch 4, Iter 290, Loss 0.2766042947769165\n",
      "Epoch 4, Iter 300, Loss 0.29210859537124634\n",
      "Epoch 4, Iter 310, Loss 0.31177833676338196\n",
      "Epoch 4, Iter 320, Loss 0.25608953833580017\n",
      "Epoch 4, Iter 330, Loss 0.2839140295982361\n",
      "Epoch 4, Iter 340, Loss 0.2428448647260666\n",
      "Epoch 4, Iter 350, Loss 0.2819402813911438\n",
      "Epoch 4, Iter 360, Loss 0.2826320230960846\n",
      "Epoch 4, Iter 370, Loss 0.2924400866031647\n",
      "Epoch 4, Iter 380, Loss 0.28432953357696533\n",
      "Epoch 4, Iter 390, Loss 0.2893484830856323\n",
      "Epoch 4, Iter 400, Loss 0.267991840839386\n",
      "Epoch 4, Iter 410, Loss 0.2800481915473938\n",
      "Epoch 4, Iter 420, Loss 0.2639937698841095\n",
      "Epoch 4, Iter 430, Loss 0.25646960735321045\n",
      "Epoch 4, Iter 440, Loss 0.27322059869766235\n",
      "Epoch 4, Iter 450, Loss 0.2466362565755844\n",
      "Epoch 4, Iter 460, Loss 0.25059494376182556\n",
      "Epoch 4, Iter 470, Loss 0.2698362469673157\n",
      "Epoch 4, Iter 480, Loss 0.27785244584083557\n",
      "Epoch 4, Iter 490, Loss 0.26288220286369324\n",
      "Epoch 4, Iter 500, Loss 0.27850931882858276\n",
      "Epoch 4, Iter 510, Loss 0.3151293098926544\n",
      "Epoch 4, Iter 520, Loss 0.27349090576171875\n",
      "Epoch 4, Iter 530, Loss 0.27906668186187744\n",
      "Epoch 4, Iter 540, Loss 0.2728143632411957\n",
      "Epoch 4, Iter 550, Loss 0.2657906115055084\n",
      "Epoch 4, Iter 560, Loss 0.2904548943042755\n",
      "Epoch 4, Iter 570, Loss 0.27740412950515747\n",
      "Epoch 4, Iter 580, Loss 0.2709907293319702\n",
      "Epoch 4, Iter 590, Loss 0.2710622549057007\n",
      "Epoch 4, Iter 600, Loss 0.25793224573135376\n",
      "Epoch 4, Iter 610, Loss 0.25973600149154663\n",
      "Epoch 4, Iter 620, Loss 0.2853851020336151\n",
      "Epoch 4, Iter 630, Loss 0.28911423683166504\n",
      "Epoch 4, Iter 640, Loss 0.3001076579093933\n",
      "Epoch 4, Iter 650, Loss 0.2854015529155731\n",
      "Epoch 4, Iter 660, Loss 0.2689969837665558\n",
      "Epoch 4, Iter 670, Loss 0.26865866780281067\n",
      "Epoch 4, Iter 680, Loss 0.3339816927909851\n",
      "Epoch 4, Iter 690, Loss 0.3204531669616699\n",
      "Epoch 4, Iter 700, Loss 0.3082760274410248\n",
      "Epoch 4, Iter 710, Loss 0.24608317017555237\n",
      "Epoch 4, Iter 720, Loss 0.2939053475856781\n",
      "Epoch 4, Iter 730, Loss 0.2568419277667999\n",
      "Epoch 4, Iter 740, Loss 0.28916484117507935\n",
      "Epoch 4, Iter 750, Loss 0.2895582318305969\n",
      "Epoch 4, Iter 760, Loss 0.2635546326637268\n",
      "Epoch 4, Iter 770, Loss 0.26739242672920227\n",
      "Epoch 4, Iter 780, Loss 0.2630453407764435\n",
      "Epoch 4, Iter 790, Loss 0.2697840929031372\n",
      "Epoch 4, Iter 800, Loss 0.24445900321006775\n",
      "Epoch 4, Iter 810, Loss 0.26254332065582275\n",
      "Epoch 4, Iter 820, Loss 0.30985942482948303\n",
      "Epoch 4, Iter 830, Loss 0.25038111209869385\n",
      "Epoch 4, Iter 840, Loss 0.29044410586357117\n",
      "Epoch 4, Iter 850, Loss 0.2790294289588928\n",
      "Epoch 4, Iter 860, Loss 0.27679547667503357\n",
      "Epoch 4, Iter 870, Loss 0.2520020604133606\n",
      "Epoch 4, Iter 880, Loss 0.24818047881126404\n",
      "Epoch 4, Iter 890, Loss 0.28732192516326904\n",
      "Epoch 4, Iter 900, Loss 0.3180299997329712\n",
      "Epoch 4, Iter 910, Loss 0.24716025590896606\n",
      "Epoch 4, Iter 920, Loss 0.30278822779655457\n",
      "Epoch 4, Iter 930, Loss 0.27578938007354736\n",
      "Epoch 4, Iter 940, Loss 0.23667553067207336\n",
      "Epoch 4, Iter 950, Loss 0.2672230005264282\n",
      "Epoch 4, Iter 960, Loss 0.259032279253006\n",
      "Epoch 4, Iter 970, Loss 0.2569647431373596\n",
      "Epoch 4, Iter 980, Loss 0.26165491342544556\n",
      "Epoch 4, Iter 990, Loss 0.2793590724468231\n",
      "Epoch 4, Iter 1000, Loss 0.2746616005897522\n",
      "Epoch 4, Iter 1010, Loss 0.26752379536628723\n",
      "Epoch 4, Iter 1020, Loss 0.25142937898635864\n",
      "Epoch 4, Iter 1030, Loss 0.25390464067459106\n",
      "Epoch 4, Iter 1040, Loss 0.27136072516441345\n",
      "Epoch 4, Iter 1050, Loss 0.2610291838645935\n",
      "Epoch 4, Iter 1060, Loss 0.2466869205236435\n",
      "Epoch 4, Iter 1070, Loss 0.2537927031517029\n",
      "Epoch 4, Iter 1080, Loss 0.2384318858385086\n",
      "Epoch 4, Iter 1090, Loss 0.29866597056388855\n",
      "Epoch 4, Iter 1100, Loss 0.2512028217315674\n",
      "Epoch 4, Iter 1110, Loss 0.27423903346061707\n",
      "Epoch 4, Iter 1120, Loss 0.28793537616729736\n",
      "Epoch 4, Iter 1130, Loss 0.30722299218177795\n",
      "Epoch 4, Iter 1140, Loss 0.2874557077884674\n",
      "Epoch 4, Iter 1150, Loss 0.2671600878238678\n",
      "Epoch 4, Iter 1160, Loss 0.2578934133052826\n",
      "Epoch 4, Iter 1170, Loss 0.25878190994262695\n",
      "Epoch 4, Iter 1180, Loss 0.30637624859809875\n",
      "Epoch 4, Iter 1190, Loss 0.30345630645751953\n",
      "Epoch 4, Iter 1200, Loss 0.2606309950351715\n",
      "Model saved as crnn_256_512_0.001_64_10_IAM.pth\n",
      "Epoch 5, Iter 0, Loss 0.2774708569049835\n",
      "Epoch 5, Iter 10, Loss 0.24308329820632935\n",
      "Epoch 5, Iter 20, Loss 0.3188164532184601\n",
      "Epoch 5, Iter 30, Loss 0.25367870926856995\n",
      "Epoch 5, Iter 40, Loss 0.2845989763736725\n",
      "Epoch 5, Iter 50, Loss 0.2720082700252533\n",
      "Epoch 5, Iter 60, Loss 0.30357810854911804\n",
      "Epoch 5, Iter 70, Loss 0.27012932300567627\n",
      "Epoch 5, Iter 80, Loss 0.2625865042209625\n",
      "Epoch 5, Iter 90, Loss 0.25333818793296814\n",
      "Epoch 5, Iter 100, Loss 0.26858949661254883\n",
      "Epoch 5, Iter 110, Loss 0.24274855852127075\n",
      "Epoch 5, Iter 120, Loss 0.2533002197742462\n",
      "Epoch 5, Iter 130, Loss 0.2588878571987152\n",
      "Epoch 5, Iter 140, Loss 0.2923634648323059\n",
      "Epoch 5, Iter 150, Loss 0.25654157996177673\n",
      "Epoch 5, Iter 160, Loss 0.27900588512420654\n",
      "Epoch 5, Iter 170, Loss 0.25178638100624084\n",
      "Epoch 5, Iter 180, Loss 0.28320083022117615\n",
      "Epoch 5, Iter 190, Loss 0.27797698974609375\n",
      "Epoch 5, Iter 200, Loss 0.2587829530239105\n",
      "Epoch 5, Iter 210, Loss 0.275199294090271\n",
      "Epoch 5, Iter 220, Loss 0.3015024662017822\n",
      "Epoch 5, Iter 230, Loss 0.2488083839416504\n",
      "Epoch 5, Iter 240, Loss 0.2637810707092285\n",
      "Epoch 5, Iter 250, Loss 0.24277529120445251\n",
      "Epoch 5, Iter 260, Loss 0.25929024815559387\n",
      "Epoch 5, Iter 270, Loss 0.2675432860851288\n",
      "Epoch 5, Iter 280, Loss 0.3102923035621643\n",
      "Epoch 5, Iter 290, Loss 0.29021066427230835\n",
      "Epoch 5, Iter 300, Loss 0.25553998351097107\n",
      "Epoch 5, Iter 310, Loss 0.26714086532592773\n",
      "Epoch 5, Iter 320, Loss 0.26068413257598877\n",
      "Epoch 5, Iter 330, Loss 0.2768218517303467\n",
      "Epoch 5, Iter 340, Loss 0.24697284400463104\n",
      "Epoch 5, Iter 350, Loss 0.257660835981369\n",
      "Epoch 5, Iter 360, Loss 0.2542533874511719\n",
      "Epoch 5, Iter 370, Loss 0.22982829809188843\n",
      "Epoch 5, Iter 380, Loss 0.24516773223876953\n",
      "Epoch 5, Iter 390, Loss 0.30966371297836304\n",
      "Epoch 5, Iter 400, Loss 0.255996435880661\n",
      "Epoch 5, Iter 410, Loss 0.2723701000213623\n",
      "Epoch 5, Iter 420, Loss 0.25724703073501587\n",
      "Epoch 5, Iter 430, Loss 0.25490379333496094\n",
      "Epoch 5, Iter 440, Loss 0.2516900897026062\n",
      "Epoch 5, Iter 450, Loss 0.25489604473114014\n",
      "Epoch 5, Iter 460, Loss 0.34185516834259033\n",
      "Epoch 5, Iter 470, Loss 0.2635534405708313\n",
      "Epoch 5, Iter 480, Loss 0.29711419343948364\n",
      "Epoch 5, Iter 490, Loss 0.2997974753379822\n",
      "Epoch 5, Iter 500, Loss 0.29010701179504395\n",
      "Epoch 5, Iter 510, Loss 0.2740478813648224\n",
      "Epoch 5, Iter 520, Loss 0.2726004719734192\n",
      "Epoch 5, Iter 530, Loss 0.26056379079818726\n",
      "Epoch 5, Iter 540, Loss 0.2871183753013611\n",
      "Epoch 5, Iter 550, Loss 0.2801017463207245\n",
      "Epoch 5, Iter 560, Loss 0.2632771134376526\n",
      "Epoch 5, Iter 570, Loss 0.24884681403636932\n",
      "Epoch 5, Iter 580, Loss 0.2660081386566162\n",
      "Epoch 5, Iter 590, Loss 0.2752615511417389\n",
      "Epoch 5, Iter 600, Loss 0.2538831830024719\n",
      "Epoch 5, Iter 610, Loss 0.24212853610515594\n",
      "Epoch 5, Iter 620, Loss 0.2771914601325989\n",
      "Epoch 5, Iter 630, Loss 0.29667484760284424\n",
      "Epoch 5, Iter 640, Loss 0.28051403164863586\n",
      "Epoch 5, Iter 650, Loss 0.2576727271080017\n",
      "Epoch 5, Iter 660, Loss 0.2571912705898285\n",
      "Epoch 5, Iter 670, Loss 0.254768431186676\n",
      "Epoch 5, Iter 680, Loss 0.2592511475086212\n",
      "Epoch 5, Iter 690, Loss 0.2626529037952423\n",
      "Epoch 5, Iter 700, Loss 0.2572985589504242\n",
      "Epoch 5, Iter 710, Loss 0.2604058086872101\n",
      "Epoch 5, Iter 720, Loss 0.28763121366500854\n",
      "Epoch 5, Iter 730, Loss 0.255033940076828\n",
      "Epoch 5, Iter 740, Loss 0.29424190521240234\n",
      "Epoch 5, Iter 750, Loss 0.2504067122936249\n",
      "Epoch 5, Iter 760, Loss 0.2697548568248749\n",
      "Epoch 5, Iter 770, Loss 0.2631481885910034\n",
      "Epoch 5, Iter 780, Loss 0.24784418940544128\n",
      "Epoch 5, Iter 790, Loss 0.24405817687511444\n",
      "Epoch 5, Iter 800, Loss 0.2912006080150604\n",
      "Epoch 5, Iter 810, Loss 0.2312360256910324\n",
      "Epoch 5, Iter 820, Loss 0.2650087773799896\n",
      "Epoch 5, Iter 830, Loss 0.28817567229270935\n",
      "Epoch 5, Iter 840, Loss 0.23871707916259766\n",
      "Epoch 5, Iter 850, Loss 0.24614331126213074\n",
      "Epoch 5, Iter 860, Loss 0.29788723587989807\n",
      "Epoch 5, Iter 870, Loss 0.23655298352241516\n",
      "Epoch 5, Iter 880, Loss 0.26171600818634033\n",
      "Epoch 5, Iter 890, Loss 0.2400495707988739\n",
      "Epoch 5, Iter 900, Loss 0.24077405035495758\n",
      "Epoch 5, Iter 910, Loss 0.2607210874557495\n",
      "Epoch 5, Iter 920, Loss 0.2591155171394348\n",
      "Epoch 5, Iter 930, Loss 0.2609193027019501\n",
      "Epoch 5, Iter 940, Loss 0.2777044475078583\n",
      "Epoch 5, Iter 950, Loss 0.3229435384273529\n",
      "Epoch 5, Iter 960, Loss 0.26377081871032715\n",
      "Epoch 5, Iter 970, Loss 0.2601463198661804\n",
      "Epoch 5, Iter 980, Loss 0.25392451882362366\n",
      "Epoch 5, Iter 990, Loss 0.25171777606010437\n",
      "Epoch 5, Iter 1000, Loss 0.3038996756076813\n",
      "Epoch 5, Iter 1010, Loss 0.24064570665359497\n",
      "Epoch 5, Iter 1020, Loss 0.25897204875946045\n",
      "Epoch 5, Iter 1030, Loss 0.2403593808412552\n",
      "Epoch 5, Iter 1040, Loss 0.24520663917064667\n",
      "Epoch 5, Iter 1050, Loss 0.24384969472885132\n",
      "Epoch 5, Iter 1060, Loss 0.27680447697639465\n",
      "Epoch 5, Iter 1070, Loss 0.24089495837688446\n",
      "Epoch 5, Iter 1080, Loss 0.252170205116272\n",
      "Epoch 5, Iter 1090, Loss 0.2530590295791626\n",
      "Epoch 5, Iter 1100, Loss 0.26658663153648376\n",
      "Epoch 5, Iter 1110, Loss 0.30937129259109497\n",
      "Epoch 5, Iter 1120, Loss 0.24497979879379272\n",
      "Epoch 5, Iter 1130, Loss 0.2708760201931\n",
      "Epoch 5, Iter 1140, Loss 0.26525580883026123\n",
      "Epoch 5, Iter 1150, Loss 0.25564271211624146\n",
      "Epoch 5, Iter 1160, Loss 0.24602991342544556\n",
      "Epoch 5, Iter 1170, Loss 0.2896465063095093\n",
      "Epoch 5, Iter 1180, Loss 0.25756803154945374\n",
      "Epoch 5, Iter 1190, Loss 0.2410888522863388\n",
      "Epoch 5, Iter 1200, Loss 0.28732946515083313\n",
      "Model saved as crnn_256_512_0.001_64_10_IAM.pth\n",
      "Epoch 6, Iter 0, Loss 0.2571769654750824\n",
      "Epoch 6, Iter 10, Loss 0.24870093166828156\n",
      "Epoch 6, Iter 20, Loss 0.2639961242675781\n",
      "Epoch 6, Iter 30, Loss 0.24431607127189636\n",
      "Epoch 6, Iter 40, Loss 0.2763676643371582\n",
      "Epoch 6, Iter 50, Loss 0.26764050126075745\n",
      "Epoch 6, Iter 60, Loss 0.23803991079330444\n",
      "Epoch 6, Iter 70, Loss 0.2610596716403961\n",
      "Epoch 6, Iter 80, Loss 0.25166448950767517\n",
      "Epoch 6, Iter 90, Loss 0.26196229457855225\n",
      "Epoch 6, Iter 100, Loss 0.24162374436855316\n",
      "Epoch 6, Iter 110, Loss 0.23828069865703583\n",
      "Epoch 6, Iter 120, Loss 0.25605517625808716\n",
      "Epoch 6, Iter 130, Loss 0.2614297568798065\n",
      "Epoch 6, Iter 140, Loss 0.2528122365474701\n",
      "Epoch 6, Iter 150, Loss 0.24290357530117035\n",
      "Epoch 6, Iter 160, Loss 0.26272085309028625\n",
      "Epoch 6, Iter 170, Loss 0.25326550006866455\n",
      "Epoch 6, Iter 180, Loss 0.247763529419899\n",
      "Epoch 6, Iter 190, Loss 0.2653338313102722\n",
      "Epoch 6, Iter 200, Loss 0.26705196499824524\n",
      "Epoch 6, Iter 210, Loss 0.2530476152896881\n",
      "Epoch 6, Iter 220, Loss 0.2455308735370636\n",
      "Epoch 6, Iter 230, Loss 0.2563551366329193\n",
      "Epoch 6, Iter 240, Loss 0.2750278413295746\n",
      "Epoch 6, Iter 250, Loss 0.2600279152393341\n",
      "Epoch 6, Iter 260, Loss 0.2591913640499115\n",
      "Epoch 6, Iter 270, Loss 0.23991981148719788\n",
      "Epoch 6, Iter 280, Loss 0.3034052848815918\n",
      "Epoch 6, Iter 290, Loss 0.23901773989200592\n",
      "Epoch 6, Iter 300, Loss 0.2598862051963806\n",
      "Epoch 6, Iter 310, Loss 0.24245521426200867\n",
      "Epoch 6, Iter 320, Loss 0.26082393527030945\n",
      "Epoch 6, Iter 330, Loss 0.23429331183433533\n",
      "Epoch 6, Iter 340, Loss 0.27775096893310547\n",
      "Epoch 6, Iter 350, Loss 0.2519567906856537\n",
      "Epoch 6, Iter 360, Loss 0.22355271875858307\n",
      "Epoch 6, Iter 370, Loss 0.2953126132488251\n",
      "Epoch 6, Iter 380, Loss 0.2721846401691437\n",
      "Epoch 6, Iter 390, Loss 0.23009586334228516\n",
      "Epoch 6, Iter 400, Loss 0.24128536880016327\n",
      "Epoch 6, Iter 410, Loss 0.2476431280374527\n",
      "Epoch 6, Iter 420, Loss 0.240217387676239\n",
      "Epoch 6, Iter 430, Loss 0.24027985334396362\n",
      "Epoch 6, Iter 440, Loss 0.24657845497131348\n",
      "Epoch 6, Iter 450, Loss 0.2793983221054077\n",
      "Epoch 6, Iter 460, Loss 0.2560109496116638\n",
      "Epoch 6, Iter 470, Loss 0.2574872374534607\n",
      "Epoch 6, Iter 480, Loss 0.24766699969768524\n",
      "Epoch 6, Iter 490, Loss 0.2639240622520447\n",
      "Epoch 6, Iter 500, Loss 0.2981579601764679\n",
      "Epoch 6, Iter 510, Loss 0.24404925107955933\n",
      "Epoch 6, Iter 520, Loss 0.2681974172592163\n",
      "Epoch 6, Iter 530, Loss 0.28194645047187805\n",
      "Epoch 6, Iter 540, Loss 0.24510347843170166\n",
      "Epoch 6, Iter 550, Loss 0.27432605624198914\n",
      "Epoch 6, Iter 560, Loss 0.2741474509239197\n",
      "Epoch 6, Iter 570, Loss 0.25951167941093445\n",
      "Epoch 6, Iter 580, Loss 0.2607724070549011\n",
      "Epoch 6, Iter 590, Loss 0.30712801218032837\n",
      "Epoch 6, Iter 600, Loss 0.2654125392436981\n",
      "Epoch 6, Iter 610, Loss 0.2308155596256256\n",
      "Epoch 6, Iter 620, Loss 0.22314952313899994\n",
      "Epoch 6, Iter 630, Loss 0.27114495635032654\n",
      "Epoch 6, Iter 640, Loss 0.31941089034080505\n",
      "Epoch 6, Iter 650, Loss 0.2728246748447418\n",
      "Epoch 6, Iter 660, Loss 0.23604242503643036\n",
      "Epoch 6, Iter 670, Loss 0.24564747512340546\n",
      "Epoch 6, Iter 680, Loss 0.25904619693756104\n",
      "Epoch 6, Iter 690, Loss 0.24658602476119995\n",
      "Epoch 6, Iter 700, Loss 0.27692967653274536\n",
      "Epoch 6, Iter 710, Loss 0.2470240443944931\n",
      "Epoch 6, Iter 720, Loss 0.26536378264427185\n",
      "Epoch 6, Iter 730, Loss 0.344439834356308\n",
      "Epoch 6, Iter 740, Loss 0.2419407069683075\n",
      "Epoch 6, Iter 750, Loss 0.2615850269794464\n",
      "Epoch 6, Iter 760, Loss 0.26771223545074463\n",
      "Epoch 6, Iter 770, Loss 0.26472342014312744\n",
      "Epoch 6, Iter 780, Loss 0.24341511726379395\n",
      "Epoch 6, Iter 790, Loss 0.25385257601737976\n",
      "Epoch 6, Iter 800, Loss 0.25820714235305786\n",
      "Epoch 6, Iter 810, Loss 0.2599455714225769\n",
      "Epoch 6, Iter 820, Loss 0.2534448504447937\n",
      "Epoch 6, Iter 830, Loss 0.24692608416080475\n",
      "Epoch 6, Iter 840, Loss 0.2606734037399292\n",
      "Epoch 6, Iter 850, Loss 0.25506311655044556\n",
      "Epoch 6, Iter 860, Loss 0.25184956192970276\n",
      "Epoch 6, Iter 870, Loss 0.26024624705314636\n",
      "Epoch 6, Iter 880, Loss 0.28338974714279175\n",
      "Epoch 6, Iter 890, Loss 0.2599090039730072\n",
      "Epoch 6, Iter 900, Loss 0.25762492418289185\n",
      "Epoch 6, Iter 910, Loss 0.2664167582988739\n",
      "Epoch 6, Iter 920, Loss 0.24470195174217224\n",
      "Epoch 6, Iter 930, Loss 0.2515254318714142\n",
      "Epoch 6, Iter 940, Loss 0.24010834097862244\n",
      "Epoch 6, Iter 950, Loss 0.25551581382751465\n",
      "Epoch 6, Iter 960, Loss 0.23736567795276642\n",
      "Epoch 6, Iter 970, Loss 0.23969265818595886\n",
      "Epoch 6, Iter 980, Loss 0.24203525483608246\n",
      "Epoch 6, Iter 990, Loss 0.2338714450597763\n",
      "Epoch 6, Iter 1000, Loss 0.2357710748910904\n",
      "Epoch 6, Iter 1010, Loss 0.2410135269165039\n",
      "Epoch 6, Iter 1020, Loss 0.2347380369901657\n",
      "Epoch 6, Iter 1030, Loss 0.25189438462257385\n",
      "Epoch 6, Iter 1040, Loss 0.2759503126144409\n",
      "Epoch 6, Iter 1050, Loss 0.28452664613723755\n",
      "Epoch 6, Iter 1060, Loss 0.26689961552619934\n",
      "Epoch 6, Iter 1070, Loss 0.24675066769123077\n",
      "Epoch 6, Iter 1080, Loss 0.2511143386363983\n",
      "Epoch 6, Iter 1090, Loss 0.2601388692855835\n",
      "Epoch 6, Iter 1100, Loss 0.24490146338939667\n",
      "Epoch 6, Iter 1110, Loss 0.2833690047264099\n",
      "Epoch 6, Iter 1120, Loss 0.2523461580276489\n",
      "Epoch 6, Iter 1130, Loss 0.2369145154953003\n",
      "Epoch 6, Iter 1140, Loss 0.2415875494480133\n",
      "Epoch 6, Iter 1150, Loss 0.24983549118041992\n",
      "Epoch 6, Iter 1160, Loss 0.2567535638809204\n",
      "Epoch 6, Iter 1170, Loss 0.24004583060741425\n",
      "Epoch 6, Iter 1180, Loss 0.24939408898353577\n",
      "Epoch 6, Iter 1190, Loss 0.25340887904167175\n",
      "Epoch 6, Iter 1200, Loss 0.2615204155445099\n",
      "Model saved as crnn_256_512_0.001_64_10_IAM.pth\n",
      "Epoch 7, Iter 0, Loss 0.3065498471260071\n",
      "Epoch 7, Iter 10, Loss 0.31866657733917236\n",
      "Epoch 7, Iter 20, Loss 0.24400252103805542\n",
      "Epoch 7, Iter 30, Loss 0.24903658032417297\n",
      "Epoch 7, Iter 40, Loss 0.2607322633266449\n",
      "Epoch 7, Iter 50, Loss 0.238759383559227\n",
      "Epoch 7, Iter 60, Loss 0.2809720039367676\n",
      "Epoch 7, Iter 70, Loss 0.23659323155879974\n",
      "Epoch 7, Iter 80, Loss 0.24794109165668488\n",
      "Epoch 7, Iter 90, Loss 0.2524738013744354\n",
      "Epoch 7, Iter 100, Loss 0.249286949634552\n",
      "Epoch 7, Iter 110, Loss 0.30914196372032166\n",
      "Epoch 7, Iter 120, Loss 0.23979604244232178\n",
      "Epoch 7, Iter 130, Loss 0.2571061849594116\n",
      "Epoch 7, Iter 140, Loss 0.22377713024616241\n",
      "Epoch 7, Iter 150, Loss 0.2534274458885193\n",
      "Epoch 7, Iter 160, Loss 0.23753713071346283\n",
      "Epoch 7, Iter 170, Loss 0.23096917569637299\n",
      "Epoch 7, Iter 180, Loss 0.2411552518606186\n",
      "Epoch 7, Iter 190, Loss 0.2460356205701828\n",
      "Epoch 7, Iter 200, Loss 0.25566771626472473\n",
      "Epoch 7, Iter 210, Loss 0.2982344627380371\n",
      "Epoch 7, Iter 220, Loss 0.2291860729455948\n",
      "Epoch 7, Iter 230, Loss 0.24284996092319489\n",
      "Epoch 7, Iter 240, Loss 0.23411105573177338\n",
      "Epoch 7, Iter 250, Loss 0.23898421227931976\n",
      "Epoch 7, Iter 260, Loss 0.24746178090572357\n",
      "Epoch 7, Iter 270, Loss 0.2737053334712982\n",
      "Epoch 7, Iter 280, Loss 0.2409062534570694\n",
      "Epoch 7, Iter 290, Loss 0.2787175178527832\n",
      "Epoch 7, Iter 300, Loss 0.24667158722877502\n",
      "Epoch 7, Iter 310, Loss 0.25397735834121704\n",
      "Epoch 7, Iter 320, Loss 0.24702149629592896\n",
      "Epoch 7, Iter 330, Loss 0.24889293313026428\n",
      "Epoch 7, Iter 340, Loss 0.25611412525177\n",
      "Epoch 7, Iter 350, Loss 0.2482369840145111\n",
      "Epoch 7, Iter 360, Loss 0.26929739117622375\n",
      "Epoch 7, Iter 370, Loss 0.23796683549880981\n",
      "Epoch 7, Iter 380, Loss 0.2589758336544037\n",
      "Epoch 7, Iter 390, Loss 0.25271323323249817\n",
      "Epoch 7, Iter 400, Loss 0.22707492113113403\n",
      "Epoch 7, Iter 410, Loss 0.25476938486099243\n",
      "Epoch 7, Iter 420, Loss 0.2381383329629898\n",
      "Epoch 7, Iter 430, Loss 0.23973767459392548\n",
      "Epoch 7, Iter 440, Loss 0.23365746438503265\n",
      "Epoch 7, Iter 450, Loss 0.2380591332912445\n",
      "Epoch 7, Iter 460, Loss 0.3107670545578003\n",
      "Epoch 7, Iter 470, Loss 0.24714285135269165\n",
      "Epoch 7, Iter 480, Loss 0.2795136868953705\n",
      "Epoch 7, Iter 490, Loss 0.23735380172729492\n",
      "Epoch 7, Iter 500, Loss 0.2239271104335785\n",
      "Epoch 7, Iter 510, Loss 0.24782858788967133\n",
      "Epoch 7, Iter 520, Loss 0.2588411867618561\n",
      "Epoch 7, Iter 530, Loss 0.27088862657546997\n",
      "Epoch 7, Iter 540, Loss 0.2534836530685425\n",
      "Epoch 7, Iter 550, Loss 0.2737174928188324\n",
      "Epoch 7, Iter 560, Loss 0.24693088233470917\n",
      "Epoch 7, Iter 570, Loss 0.2405518740415573\n",
      "Epoch 7, Iter 580, Loss 0.3298929035663605\n",
      "Epoch 7, Iter 590, Loss 0.2381010800600052\n",
      "Epoch 7, Iter 600, Loss 0.25805920362472534\n",
      "Epoch 7, Iter 610, Loss 0.23519964516162872\n",
      "Epoch 7, Iter 620, Loss 0.26513415575027466\n",
      "Epoch 7, Iter 630, Loss 0.2736208140850067\n",
      "Epoch 7, Iter 640, Loss 0.2217933088541031\n",
      "Epoch 7, Iter 650, Loss 0.2583068609237671\n",
      "Epoch 7, Iter 660, Loss 0.24352550506591797\n",
      "Epoch 7, Iter 670, Loss 0.23921677470207214\n",
      "Epoch 7, Iter 680, Loss 0.28678226470947266\n",
      "Epoch 7, Iter 690, Loss 0.24478432536125183\n",
      "Epoch 7, Iter 700, Loss 0.24174530804157257\n",
      "Epoch 7, Iter 710, Loss 0.24785897135734558\n",
      "Epoch 7, Iter 720, Loss 0.26353719830513\n",
      "Epoch 7, Iter 730, Loss 0.2511141300201416\n",
      "Epoch 7, Iter 740, Loss 0.22001391649246216\n",
      "Epoch 7, Iter 750, Loss 0.2585727870464325\n",
      "Epoch 7, Iter 760, Loss 0.2488894909620285\n",
      "Epoch 7, Iter 770, Loss 0.2689250111579895\n",
      "Epoch 7, Iter 780, Loss 0.2592082619667053\n",
      "Epoch 7, Iter 790, Loss 0.25439441204071045\n",
      "Epoch 7, Iter 800, Loss 0.25230786204338074\n",
      "Epoch 7, Iter 810, Loss 0.2992231249809265\n",
      "Epoch 7, Iter 820, Loss 0.23592452704906464\n",
      "Epoch 7, Iter 830, Loss 0.24908879399299622\n",
      "Epoch 7, Iter 840, Loss 0.25587356090545654\n",
      "Epoch 7, Iter 850, Loss 0.24915100634098053\n",
      "Epoch 7, Iter 860, Loss 0.24204257130622864\n",
      "Epoch 7, Iter 870, Loss 0.24336600303649902\n",
      "Epoch 7, Iter 880, Loss 0.25649046897888184\n",
      "Epoch 7, Iter 890, Loss 0.22660288214683533\n",
      "Epoch 7, Iter 900, Loss 0.2401486188173294\n",
      "Epoch 7, Iter 910, Loss 0.2783028185367584\n",
      "Epoch 7, Iter 920, Loss 0.26722121238708496\n",
      "Epoch 7, Iter 930, Loss 0.23317989706993103\n",
      "Epoch 7, Iter 940, Loss 0.25283440947532654\n",
      "Epoch 7, Iter 950, Loss 0.23373602330684662\n",
      "Epoch 7, Iter 960, Loss 0.25935760140419006\n",
      "Epoch 7, Iter 970, Loss 0.23910394310951233\n",
      "Epoch 7, Iter 980, Loss 0.2602243721485138\n",
      "Epoch 7, Iter 990, Loss 0.23138660192489624\n",
      "Epoch 7, Iter 1000, Loss 0.2560451328754425\n",
      "Epoch 7, Iter 1010, Loss 0.24748463928699493\n",
      "Epoch 7, Iter 1020, Loss 0.24586375057697296\n",
      "Epoch 7, Iter 1030, Loss 0.27623534202575684\n",
      "Epoch 7, Iter 1040, Loss 0.2657996714115143\n",
      "Epoch 7, Iter 1050, Loss 0.27408161759376526\n",
      "Epoch 7, Iter 1060, Loss 0.24949654936790466\n",
      "Epoch 7, Iter 1070, Loss 0.2586981952190399\n",
      "Epoch 7, Iter 1080, Loss 0.2469785064458847\n",
      "Epoch 7, Iter 1090, Loss 0.24086065590381622\n",
      "Epoch 7, Iter 1100, Loss 0.23649798333644867\n",
      "Epoch 7, Iter 1110, Loss 0.25675520300865173\n",
      "Epoch 7, Iter 1120, Loss 0.24723856151103973\n",
      "Epoch 7, Iter 1130, Loss 0.2520023286342621\n",
      "Epoch 7, Iter 1140, Loss 0.2896779775619507\n",
      "Epoch 7, Iter 1150, Loss 0.2498345524072647\n",
      "Epoch 7, Iter 1160, Loss 0.2337549328804016\n",
      "Epoch 7, Iter 1170, Loss 0.228104367852211\n",
      "Epoch 7, Iter 1180, Loss 0.2517956495285034\n",
      "Epoch 7, Iter 1190, Loss 0.24085263907909393\n",
      "Epoch 7, Iter 1200, Loss 0.24905535578727722\n",
      "Model saved as crnn_256_512_0.001_64_10_IAM.pth\n",
      "Epoch 8, Iter 0, Loss 0.2581017017364502\n",
      "Epoch 8, Iter 10, Loss 0.24536757171154022\n",
      "Epoch 8, Iter 20, Loss 0.2519820034503937\n",
      "Epoch 8, Iter 30, Loss 0.2866063117980957\n",
      "Epoch 8, Iter 40, Loss 0.24307014048099518\n",
      "Epoch 8, Iter 50, Loss 0.2648233473300934\n",
      "Epoch 8, Iter 60, Loss 0.2405223250389099\n",
      "Epoch 8, Iter 70, Loss 0.2611065208911896\n",
      "Epoch 8, Iter 80, Loss 0.26142027974128723\n",
      "Epoch 8, Iter 90, Loss 0.25937703251838684\n",
      "Epoch 8, Iter 100, Loss 0.24447055160999298\n",
      "Epoch 8, Iter 110, Loss 0.2371041625738144\n",
      "Epoch 8, Iter 120, Loss 0.22949334979057312\n",
      "Epoch 8, Iter 130, Loss 0.2450791746377945\n",
      "Epoch 8, Iter 140, Loss 0.23522767424583435\n",
      "Epoch 8, Iter 150, Loss 0.2271563559770584\n",
      "Epoch 8, Iter 160, Loss 0.2491597831249237\n",
      "Epoch 8, Iter 170, Loss 0.2625238001346588\n",
      "Epoch 8, Iter 180, Loss 0.2413778007030487\n",
      "Epoch 8, Iter 190, Loss 0.23905274271965027\n",
      "Epoch 8, Iter 200, Loss 0.247373566031456\n",
      "Epoch 8, Iter 210, Loss 0.23481231927871704\n",
      "Epoch 8, Iter 220, Loss 0.25221413373947144\n",
      "Epoch 8, Iter 230, Loss 0.2680419981479645\n",
      "Epoch 8, Iter 240, Loss 0.26391398906707764\n",
      "Epoch 8, Iter 250, Loss 0.25103089213371277\n",
      "Epoch 8, Iter 260, Loss 0.24365177750587463\n",
      "Epoch 8, Iter 270, Loss 0.25711125135421753\n",
      "Epoch 8, Iter 280, Loss 0.24331174790859222\n",
      "Epoch 8, Iter 290, Loss 0.264258474111557\n",
      "Epoch 8, Iter 300, Loss 0.24316765367984772\n",
      "Epoch 8, Iter 310, Loss 0.2703985869884491\n",
      "Epoch 8, Iter 320, Loss 0.23751215636730194\n",
      "Epoch 8, Iter 330, Loss 0.24178239703178406\n",
      "Epoch 8, Iter 340, Loss 0.22646161913871765\n",
      "Epoch 8, Iter 350, Loss 0.22881253063678741\n",
      "Epoch 8, Iter 360, Loss 0.23609121143817902\n",
      "Epoch 8, Iter 370, Loss 0.23255065083503723\n",
      "Epoch 8, Iter 380, Loss 0.22580236196517944\n",
      "Epoch 8, Iter 390, Loss 0.2532929480075836\n",
      "Epoch 8, Iter 400, Loss 0.2510910928249359\n",
      "Epoch 8, Iter 410, Loss 0.2232004702091217\n",
      "Epoch 8, Iter 420, Loss 0.21907705068588257\n",
      "Epoch 8, Iter 430, Loss 0.2323233038187027\n",
      "Epoch 8, Iter 440, Loss 0.2538391351699829\n",
      "Epoch 8, Iter 450, Loss 0.23061777651309967\n",
      "Epoch 8, Iter 460, Loss 0.24987567961215973\n",
      "Epoch 8, Iter 470, Loss 0.2888249158859253\n",
      "Epoch 8, Iter 480, Loss 0.2519350051879883\n",
      "Epoch 8, Iter 490, Loss 0.24549826979637146\n",
      "Epoch 8, Iter 500, Loss 0.24367068707942963\n",
      "Epoch 8, Iter 510, Loss 0.2346402257680893\n",
      "Epoch 8, Iter 520, Loss 0.3147631883621216\n",
      "Epoch 8, Iter 530, Loss 0.2420845776796341\n",
      "Epoch 8, Iter 540, Loss 0.2818866968154907\n",
      "Epoch 8, Iter 550, Loss 0.24690787494182587\n",
      "Epoch 8, Iter 560, Loss 0.22536984086036682\n",
      "Epoch 8, Iter 570, Loss 0.23872418701648712\n",
      "Epoch 8, Iter 580, Loss 0.2728366255760193\n",
      "Epoch 8, Iter 590, Loss 0.23390710353851318\n",
      "Epoch 8, Iter 600, Loss 0.2411230057477951\n",
      "Epoch 8, Iter 610, Loss 0.22618073225021362\n",
      "Epoch 8, Iter 620, Loss 0.2383699119091034\n",
      "Epoch 8, Iter 630, Loss 0.2419404834508896\n",
      "Epoch 8, Iter 640, Loss 0.28509679436683655\n",
      "Epoch 8, Iter 650, Loss 0.2527560889720917\n",
      "Epoch 8, Iter 660, Loss 0.24704168736934662\n",
      "Epoch 8, Iter 670, Loss 0.23968352377414703\n",
      "Epoch 8, Iter 680, Loss 0.24358907341957092\n",
      "Epoch 8, Iter 690, Loss 0.22204674780368805\n",
      "Epoch 8, Iter 700, Loss 0.23792648315429688\n",
      "Epoch 8, Iter 710, Loss 0.2853734791278839\n",
      "Epoch 8, Iter 720, Loss 0.2610263228416443\n",
      "Epoch 8, Iter 730, Loss 0.2548801600933075\n",
      "Epoch 8, Iter 740, Loss 0.24381794035434723\n",
      "Epoch 8, Iter 750, Loss 0.23880082368850708\n",
      "Epoch 8, Iter 760, Loss 0.2722617983818054\n",
      "Epoch 8, Iter 770, Loss 0.248509481549263\n",
      "Epoch 8, Iter 780, Loss 0.23045183718204498\n",
      "Epoch 8, Iter 790, Loss 0.25967124104499817\n",
      "Epoch 8, Iter 800, Loss 0.25148287415504456\n",
      "Epoch 8, Iter 810, Loss 0.24802961945533752\n",
      "Epoch 8, Iter 820, Loss 0.25571778416633606\n",
      "Epoch 8, Iter 830, Loss 0.22443002462387085\n",
      "Epoch 8, Iter 840, Loss 0.2289806753396988\n",
      "Epoch 8, Iter 850, Loss 0.25094684958457947\n",
      "Epoch 8, Iter 860, Loss 0.28311726450920105\n",
      "Epoch 8, Iter 870, Loss 0.23980823159217834\n",
      "Epoch 8, Iter 880, Loss 0.22696852684020996\n",
      "Epoch 8, Iter 890, Loss 0.2855015695095062\n",
      "Epoch 8, Iter 900, Loss 0.24774248898029327\n",
      "Epoch 8, Iter 910, Loss 0.24738237261772156\n",
      "Epoch 8, Iter 920, Loss 0.21668832004070282\n",
      "Epoch 8, Iter 930, Loss 0.25819697976112366\n",
      "Epoch 8, Iter 940, Loss 0.2501576840877533\n",
      "Epoch 8, Iter 950, Loss 0.239126056432724\n",
      "Epoch 8, Iter 960, Loss 0.2442416399717331\n",
      "Epoch 8, Iter 970, Loss 0.23642407357692719\n",
      "Epoch 8, Iter 980, Loss 0.2378588765859604\n",
      "Epoch 8, Iter 990, Loss 0.2395891696214676\n",
      "Epoch 8, Iter 1000, Loss 0.27697432041168213\n",
      "Epoch 8, Iter 1010, Loss 0.23545609414577484\n",
      "Epoch 8, Iter 1020, Loss 0.2388230413198471\n",
      "Epoch 8, Iter 1030, Loss 0.27657583355903625\n",
      "Epoch 8, Iter 1040, Loss 0.2643154263496399\n",
      "Epoch 8, Iter 1050, Loss 0.24850063025951385\n",
      "Epoch 8, Iter 1060, Loss 0.2444164752960205\n",
      "Epoch 8, Iter 1070, Loss 0.24998100101947784\n",
      "Epoch 8, Iter 1080, Loss 0.22537532448768616\n",
      "Epoch 8, Iter 1090, Loss 0.250674843788147\n",
      "Epoch 8, Iter 1100, Loss 0.24017517268657684\n",
      "Epoch 8, Iter 1110, Loss 0.26113590598106384\n",
      "Epoch 8, Iter 1120, Loss 0.2486114650964737\n",
      "Epoch 8, Iter 1130, Loss 0.2555999755859375\n",
      "Epoch 8, Iter 1140, Loss 0.22339856624603271\n",
      "Epoch 8, Iter 1150, Loss 0.27500930428504944\n",
      "Epoch 8, Iter 1160, Loss 0.25811806321144104\n",
      "Epoch 8, Iter 1170, Loss 0.2508980929851532\n",
      "Epoch 8, Iter 1180, Loss 0.23115815222263336\n",
      "Epoch 8, Iter 1190, Loss 0.262033611536026\n",
      "Epoch 8, Iter 1200, Loss 0.25864943861961365\n",
      "Model saved as crnn_256_512_0.001_64_10_IAM.pth\n",
      "Epoch 9, Iter 0, Loss 0.2557654082775116\n",
      "Epoch 9, Iter 10, Loss 0.2890594005584717\n",
      "Epoch 9, Iter 20, Loss 0.24153271317481995\n",
      "Epoch 9, Iter 30, Loss 0.22493290901184082\n",
      "Epoch 9, Iter 40, Loss 0.22173282504081726\n",
      "Epoch 9, Iter 50, Loss 0.22824959456920624\n",
      "Epoch 9, Iter 60, Loss 0.23253105580806732\n",
      "Epoch 9, Iter 70, Loss 0.23236139118671417\n",
      "Epoch 9, Iter 80, Loss 0.24897177517414093\n",
      "Epoch 9, Iter 90, Loss 0.2754981517791748\n",
      "Epoch 9, Iter 100, Loss 0.22004692256450653\n",
      "Epoch 9, Iter 110, Loss 0.2878135144710541\n",
      "Epoch 9, Iter 120, Loss 0.2681773900985718\n",
      "Epoch 9, Iter 130, Loss 0.23296284675598145\n",
      "Epoch 9, Iter 140, Loss 0.21348440647125244\n",
      "Epoch 9, Iter 150, Loss 0.2207164764404297\n",
      "Epoch 9, Iter 160, Loss 0.2628057599067688\n",
      "Epoch 9, Iter 170, Loss 0.22983600199222565\n",
      "Epoch 9, Iter 180, Loss 0.25158438086509705\n",
      "Epoch 9, Iter 190, Loss 0.25725120306015015\n",
      "Epoch 9, Iter 200, Loss 0.22868850827217102\n",
      "Epoch 9, Iter 210, Loss 0.23211178183555603\n",
      "Epoch 9, Iter 220, Loss 0.21588769555091858\n",
      "Epoch 9, Iter 230, Loss 0.24438048899173737\n",
      "Epoch 9, Iter 240, Loss 0.2644730508327484\n",
      "Epoch 9, Iter 250, Loss 0.251255601644516\n",
      "Epoch 9, Iter 260, Loss 0.27057117223739624\n",
      "Epoch 9, Iter 270, Loss 0.23602676391601562\n",
      "Epoch 9, Iter 280, Loss 0.22905482351779938\n",
      "Epoch 9, Iter 290, Loss 0.26204970479011536\n",
      "Epoch 9, Iter 300, Loss 0.2262546569108963\n",
      "Epoch 9, Iter 310, Loss 0.26502418518066406\n",
      "Epoch 9, Iter 320, Loss 0.2678168714046478\n",
      "Epoch 9, Iter 330, Loss 0.26137906312942505\n",
      "Epoch 9, Iter 340, Loss 0.25614625215530396\n",
      "Epoch 9, Iter 350, Loss 0.2594873011112213\n",
      "Epoch 9, Iter 360, Loss 0.25489354133605957\n",
      "Epoch 9, Iter 370, Loss 0.2770572900772095\n",
      "Epoch 9, Iter 380, Loss 0.29738232493400574\n",
      "Epoch 9, Iter 390, Loss 0.25194084644317627\n",
      "Epoch 9, Iter 400, Loss 0.2422584593296051\n",
      "Epoch 9, Iter 410, Loss 0.26294538378715515\n",
      "Epoch 9, Iter 420, Loss 0.23300305008888245\n",
      "Epoch 9, Iter 430, Loss 0.2471236288547516\n",
      "Epoch 9, Iter 440, Loss 0.2718908190727234\n",
      "Epoch 9, Iter 450, Loss 0.22448478639125824\n",
      "Epoch 9, Iter 460, Loss 0.21767796576023102\n",
      "Epoch 9, Iter 470, Loss 0.2584233582019806\n",
      "Epoch 9, Iter 480, Loss 0.23211701214313507\n",
      "Epoch 9, Iter 490, Loss 0.23497724533081055\n",
      "Epoch 9, Iter 500, Loss 0.23012775182724\n",
      "Epoch 9, Iter 510, Loss 0.26478883624076843\n",
      "Epoch 9, Iter 520, Loss 0.24258318543434143\n",
      "Epoch 9, Iter 530, Loss 0.2878643572330475\n",
      "Epoch 9, Iter 540, Loss 0.27084144949913025\n",
      "Epoch 9, Iter 550, Loss 0.2373984307050705\n",
      "Epoch 9, Iter 560, Loss 0.23681440949440002\n",
      "Epoch 9, Iter 570, Loss 0.26648950576782227\n",
      "Epoch 9, Iter 580, Loss 0.2608546018600464\n",
      "Epoch 9, Iter 590, Loss 0.2583695650100708\n",
      "Epoch 9, Iter 600, Loss 0.2315126359462738\n",
      "Epoch 9, Iter 610, Loss 0.2295895516872406\n",
      "Epoch 9, Iter 620, Loss 0.2702738344669342\n",
      "Epoch 9, Iter 630, Loss 0.2643667757511139\n",
      "Epoch 9, Iter 640, Loss 0.21544665098190308\n",
      "Epoch 9, Iter 650, Loss 0.23713289201259613\n",
      "Epoch 9, Iter 660, Loss 0.2519242763519287\n",
      "Epoch 9, Iter 670, Loss 0.252940833568573\n",
      "Epoch 9, Iter 680, Loss 0.2538490891456604\n",
      "Epoch 9, Iter 690, Loss 0.2344106137752533\n",
      "Epoch 9, Iter 700, Loss 0.24247828125953674\n",
      "Epoch 9, Iter 710, Loss 0.2591032385826111\n",
      "Epoch 9, Iter 720, Loss 0.2278997153043747\n",
      "Epoch 9, Iter 730, Loss 0.2423100620508194\n",
      "Epoch 9, Iter 740, Loss 0.25485727190971375\n",
      "Epoch 9, Iter 750, Loss 0.22435466945171356\n",
      "Epoch 9, Iter 760, Loss 0.2611386477947235\n",
      "Epoch 9, Iter 770, Loss 0.2215614765882492\n",
      "Epoch 9, Iter 780, Loss 0.25153985619544983\n",
      "Epoch 9, Iter 790, Loss 0.23942239582538605\n",
      "Epoch 9, Iter 800, Loss 0.2513188123703003\n",
      "Epoch 9, Iter 810, Loss 0.2492011934518814\n",
      "Epoch 9, Iter 820, Loss 0.24125885963439941\n",
      "Epoch 9, Iter 830, Loss 0.2443343997001648\n",
      "Epoch 9, Iter 840, Loss 0.25537559390068054\n",
      "Epoch 9, Iter 850, Loss 0.23355837166309357\n",
      "Epoch 9, Iter 860, Loss 0.25380969047546387\n",
      "Epoch 9, Iter 870, Loss 0.21170388162136078\n",
      "Epoch 9, Iter 880, Loss 0.22530753910541534\n",
      "Epoch 9, Iter 890, Loss 0.24668832123279572\n",
      "Epoch 9, Iter 900, Loss 0.21382087469100952\n",
      "Epoch 9, Iter 910, Loss 0.23178288340568542\n",
      "Epoch 9, Iter 920, Loss 0.2380160242319107\n",
      "Epoch 9, Iter 930, Loss 0.24415335059165955\n",
      "Epoch 9, Iter 940, Loss 0.2400646060705185\n",
      "Epoch 9, Iter 950, Loss 0.25888028740882874\n",
      "Epoch 9, Iter 960, Loss 0.2416662573814392\n",
      "Epoch 9, Iter 970, Loss 0.22790341079235077\n",
      "Epoch 9, Iter 980, Loss 0.30447638034820557\n",
      "Epoch 9, Iter 990, Loss 0.22533977031707764\n",
      "Epoch 9, Iter 1000, Loss 0.2491430789232254\n",
      "Epoch 9, Iter 1010, Loss 0.2509578466415405\n",
      "Epoch 9, Iter 1020, Loss 0.22536757588386536\n",
      "Epoch 9, Iter 1030, Loss 0.22079746425151825\n",
      "Epoch 9, Iter 1040, Loss 0.24301894009113312\n",
      "Epoch 9, Iter 1050, Loss 0.23371489346027374\n",
      "Epoch 9, Iter 1060, Loss 0.2320326417684555\n",
      "Epoch 9, Iter 1070, Loss 0.2386840581893921\n",
      "Epoch 9, Iter 1080, Loss 0.22752735018730164\n",
      "Epoch 9, Iter 1090, Loss 0.2390272170305252\n",
      "Epoch 9, Iter 1100, Loss 0.23059625923633575\n",
      "Epoch 9, Iter 1110, Loss 0.23760756850242615\n",
      "Epoch 9, Iter 1120, Loss 0.22714708745479584\n",
      "Epoch 9, Iter 1130, Loss 0.24422232806682587\n",
      "Epoch 9, Iter 1140, Loss 0.22661174833774567\n",
      "Epoch 9, Iter 1150, Loss 0.24803699553012848\n",
      "Epoch 9, Iter 1160, Loss 0.2399137318134308\n",
      "Epoch 9, Iter 1170, Loss 0.2615854740142822\n",
      "Epoch 9, Iter 1180, Loss 0.22645914554595947\n",
      "Epoch 9, Iter 1190, Loss 0.22427065670490265\n",
      "Epoch 9, Iter 1200, Loss 0.24053141474723816\n",
      "Model saved as crnn_256_512_0.001_64_10_IAM.pth\n"
     ]
    }
   ],
   "source": [
    "# 可能要调的超参数有：hidden_dim, io_dim, lr, batch_size, num_epochs, dataset_name\n",
    "def get_model_name(hidden_dim, io_dim, lr, batch_size, num_epochs, dataset_name):\n",
    "    return f\"crnn_{hidden_dim}_{io_dim}_{lr}_{batch_size}_{num_epochs}_{dataset_name}.pth\"\n",
    "\n",
    "\n",
    "# 下面是训练的代码，使用教师强制训练\n",
    "def train_crnn(model, dataloader, learning_rate, epochs, device):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9)\n",
    "    model.train()\n",
    "    model.to(device)\n",
    "    for epoch in range(epochs):\n",
    "        for i, (img, target) in enumerate(dataloader):\n",
    "            img, target = img.to(device), target.to(device)\n",
    "            output = model(img)\n",
    "            optimizer.zero_grad()\n",
    "            # h0, c0 = model.init_state(img)\n",
    "            # h0, c0 = h0.to(device), c0.to(device)\n",
    "            # output = crnn.embedding(torch.tensor([2] * img.size(0), dtype=torch.long).view(img.size(0), 1).to(device))\n",
    "            # output = model(output, (h0, c0))\n",
    "            loss = criterion(output.view(-1, crnn.num_classes), target.argmax(-1).view(-1))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            if i % 10 == 0:\n",
    "                print(f\"Epoch {epoch}, Iter {i}, Loss {loss.item()}\")\n",
    "        model_name = get_model_name(model.hidden_dim, model.io_dim, learning_rate, dataloader.batch_size, epoch, dataset.name)\n",
    "        torch.save(model.state_dict(), model_name)\n",
    "        print(f\"Model saved as {model_name}\")\n",
    "\n",
    "\n",
    "model = CRNN()\n",
    "dataloader = DataLoader(dataset, batch_size=64, shuffle=True, num_workers=8)\n",
    "train_crnn(model, dataloader, 0.001, 10, \"cuda:0\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
