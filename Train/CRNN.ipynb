{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这个记事本是用来搭建CRNN网络的，用来做recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch import nn\n",
    "from torchvision import models\n",
    "from utils import *\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "dataset_IAM = RecDataset(\"IAM\", \"train\")\n",
    "dataset_CVL = RecDataset(\"CVL\", \"train\")\n",
    "dataset = torch.utils.data.ConcatDataset([dataset_IAM, dataset_CVL])\n",
    "dataloader = DataLoader(dataset, batch_size=10, shuffle=True, num_workers=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.12/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/root/miniconda3/lib/python3.12/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "-------------------\n",
      "BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "-------------------\n",
      "ReLU(inplace=True)\n",
      "-------------------\n",
      "MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "-------------------\n",
      "Sequential(\n",
      "  (0): Bottleneck(\n",
      "    (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu): ReLU(inplace=True)\n",
      "    (downsample): Sequential(\n",
      "      (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (1): Bottleneck(\n",
      "    (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu): ReLU(inplace=True)\n",
      "  )\n",
      "  (2): Bottleneck(\n",
      "    (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu): ReLU(inplace=True)\n",
      "  )\n",
      ")\n",
      "-------------------\n",
      "Sequential(\n",
      "  (0): Bottleneck(\n",
      "    (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu): ReLU(inplace=True)\n",
      "    (downsample): Sequential(\n",
      "      (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (1): Bottleneck(\n",
      "    (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu): ReLU(inplace=True)\n",
      "  )\n",
      "  (2): Bottleneck(\n",
      "    (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu): ReLU(inplace=True)\n",
      "  )\n",
      "  (3): Bottleneck(\n",
      "    (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu): ReLU(inplace=True)\n",
      "  )\n",
      ")\n",
      "-------------------\n",
      "Sequential(\n",
      "  (0): Bottleneck(\n",
      "    (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu): ReLU(inplace=True)\n",
      "    (downsample): Sequential(\n",
      "      (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "      (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (1): Bottleneck(\n",
      "    (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu): ReLU(inplace=True)\n",
      "  )\n",
      "  (2): Bottleneck(\n",
      "    (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu): ReLU(inplace=True)\n",
      "  )\n",
      "  (3): Bottleneck(\n",
      "    (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu): ReLU(inplace=True)\n",
      "  )\n",
      "  (4): Bottleneck(\n",
      "    (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu): ReLU(inplace=True)\n",
      "  )\n",
      "  (5): Bottleneck(\n",
      "    (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu): ReLU(inplace=True)\n",
      "  )\n",
      ")\n",
      "-------------------\n",
      "Sequential(\n",
      "  (0): Bottleneck(\n",
      "    (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu): ReLU(inplace=True)\n",
      "    (downsample): Sequential(\n",
      "      (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "      (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (1): Bottleneck(\n",
      "    (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu): ReLU(inplace=True)\n",
      "  )\n",
      "  (2): Bottleneck(\n",
      "    (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu): ReLU(inplace=True)\n",
      "  )\n",
      ")\n",
      "-------------------\n",
      "AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "-------------------\n",
      "Linear(in_features=2048, out_features=1000, bias=True)\n",
      "-------------------\n"
     ]
    }
   ],
   "source": [
    "# test\n",
    "model = models.resnet50(pretrained=True)\n",
    "# model_cnn = nn.Sequential(*list(model.children())[:-1])\n",
    "for chi in model.children():\n",
    "    print(chi)\n",
    "    print('-------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里用resnet18除去最后的fc作为cnn的部分，lstm作为rnn的部分。<br>\n",
    "输入1x128x128图片<br>\n",
    "经过cnn部分，先是卷到了512x4x4，然后经过平均池化层变成512x1x1<br>\n",
    "然后展平，经过线性变换放入lstm的hidden和cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CRNN(nn.Module):\n",
    "    def __init__(self, num_classes=128, hidden_dim=512, io_dim=1024, device='cuda:0'):\n",
    "        super(CRNN, self).__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.io_dim = io_dim\n",
    "        self.device = device\n",
    "        self.max_len = 64  # max num of characters of the generated text\n",
    "        self.conv1 = nn.Conv2d(1, 3, 1)\n",
    "        self.cnn = models.resnet18(pretrained=True)\n",
    "        self.cnn = nn.Sequential(*list(self.cnn.children())[:-1])  # output dim is 512\n",
    "        self.rnn = nn.LSTM(io_dim, hidden_dim, 1, batch_first=True)\n",
    "        self.h0_fc = nn.Linear(512, hidden_dim)\n",
    "        self.c0_fc = nn.Linear(512, hidden_dim)\n",
    "        self.out_fc = nn.Linear(hidden_dim, num_classes)\n",
    "        self.embedding = nn.Embedding(num_classes, io_dim)\n",
    "        self.to(device)\n",
    "    \n",
    "    def init_state(self, img):\n",
    "        # 通过CNN卷出 lstm 的 hidden state 和 cell state\n",
    "        x = self.conv1(img)         # batch_size, 3, 64, 64\n",
    "        x = self.cnn(x)             # batch_size, 512, 1, 1\n",
    "        x = x.view(x.size(0), -1)   # batch_size, 512\n",
    "        x = x.unsqueeze(0)          # 1, batch_size, 512\n",
    "        h0 = self.h0_fc(x)          # 1, batch_size, hidden_dim\n",
    "        c0 = self.c0_fc(x)          # 1, batch_size, hidden_dim\n",
    "        return h0, c0\n",
    "    \n",
    "    def next_char(self, x, h_c_n):\n",
    "        # print(\"next char x shape: \", x.shape)\n",
    "        h_n, c_n = h_c_n\n",
    "        # x: the embedding of the last character\n",
    "        # h_n: the hidden state of the last character\n",
    "        # c_n: the cell state of the last character\n",
    "        x, (h_n, c_n) = self.rnn(x, (h_n, c_n))\n",
    "        # print(\"next char rnn output x shape: \", x.shape)\n",
    "        x = self.out_fc(x)\n",
    "        # print(\"next char output x shape: \", x.shape)\n",
    "        return x, (h_n, c_n)\n",
    "    \n",
    "    def forward(self, img):\n",
    "        batch_size = img.size(0)\n",
    "        h0, c0 = self.init_state(img)\n",
    "        x = 2  # the index of the start token\n",
    "        x = torch.tensor([x] * batch_size, dtype=torch.long).view(batch_size, 1).to(self.device)\n",
    "        x = self.embedding(x)\n",
    "        # print(\"after embedding x shape: \", x.shape)\n",
    "        h_c_n = (h0, c0)\n",
    "        temp = torch.zeros(batch_size, 1, self.num_classes).to(self.device)\n",
    "        temp[:, 0, 2] = 1\n",
    "        output = [temp]\n",
    "        for i in range(1, self.max_len):\n",
    "            x, h_c_n = self.next_char(x, h_c_n)\n",
    "            output.append(x)\n",
    "            x = x.argmax(dim=-1)\n",
    "            x = self.embedding(x)\n",
    "        output = torch.cat(output, dim=1)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.12/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 1, 128, 128])\n",
      "torch.Size([10, 64, 128])\n"
     ]
    }
   ],
   "source": [
    "# 测试CRNN进行预测时的形状是否符合要求\n",
    "\n",
    "crnn = CRNN()\n",
    "for step, (img, label) in enumerate(dataloader):\n",
    "    img, label = img.to(crnn.device), label.to(crnn.device)\n",
    "    print(img.shape)\n",
    "    output = crnn(img)\n",
    "    print(output.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 1, 128, 128])\n",
      "torch.Size([1, 10, 512]) torch.Size([1, 10, 512])\n"
     ]
    }
   ],
   "source": [
    "# 测试CRNN进行训练时的形状是否符合要求\n",
    "\n",
    "crnn = CRNN()\n",
    "for step, (img, label) in enumerate(dataloader):\n",
    "    img, label = img.to(crnn.device), label.to(crnn.device)\n",
    "    print(img.shape)\n",
    "    h_n, c_n = crnn.init_state(img)\n",
    "    print(h_n.shape, c_n.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.12/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/root/miniconda3/lib/python3.12/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "/root/miniconda3/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Iter 0, Loss 4.699872970581055\n",
      "Epoch 0, Iter 100, Loss 2.7987453937530518\n",
      "Epoch 0, Iter 200, Loss 2.5149643421173096\n",
      "Epoch 0, Iter 300, Loss 2.4094910621643066\n",
      "Epoch 0, Iter 400, Loss 2.2449092864990234\n",
      "Epoch 0, Iter 500, Loss 2.014798402786255\n",
      "Epoch 0, Iter 600, Loss 2.0866196155548096\n",
      "Epoch 0, Iter 700, Loss 1.862219214439392\n",
      "Epoch 0, Iter 800, Loss 1.9126718044281006\n",
      "Epoch 0, Iter 900, Loss 1.7945988178253174\n",
      "Epoch 0, Iter 1000, Loss 1.640978217124939\n",
      "Epoch 0, Iter 1100, Loss 1.5102792978286743\n",
      "Epoch 0, Iter 1200, Loss 1.6400631666183472\n",
      "Epoch 0, Iter 1300, Loss 1.5820311307907104\n",
      "Epoch 0, Iter 1400, Loss 1.6005780696868896\n",
      "Epoch 0, Iter 1500, Loss 1.595097541809082\n",
      "Epoch 0, Iter 1600, Loss 1.3952330350875854\n",
      "Epoch 0, Iter 1700, Loss 1.2971463203430176\n",
      "Epoch 0, Iter 1800, Loss 1.2627466917037964\n",
      "Epoch 0, Iter 1900, Loss 1.2941441535949707\n",
      "Epoch 0, Iter 2000, Loss 1.3147010803222656\n",
      "Epoch 0, Iter 2100, Loss 1.1827856302261353\n",
      "Epoch 0, Iter 2200, Loss 1.1354682445526123\n",
      "Epoch 0, Iter 2300, Loss 1.1381092071533203\n",
      "Epoch 0, Iter 2400, Loss 1.2433375120162964\n",
      "Epoch 0, Val Loss 1.173788070678711\n",
      "Model saved as crnn_512_1024_0.01_64_0_concat.pth\n",
      "Epoch 1, Iter 0, Loss 1.0044976472854614\n",
      "Epoch 1, Iter 100, Loss 1.2287521362304688\n",
      "Epoch 1, Iter 200, Loss 1.051601767539978\n",
      "Epoch 1, Iter 300, Loss 1.0574930906295776\n",
      "Epoch 1, Iter 400, Loss 1.0619007349014282\n",
      "Epoch 1, Iter 500, Loss 1.0906054973602295\n",
      "Epoch 1, Iter 600, Loss 1.1120887994766235\n",
      "Epoch 1, Iter 700, Loss 1.0972493886947632\n",
      "Epoch 1, Iter 800, Loss 1.073737621307373\n",
      "Epoch 1, Iter 900, Loss 1.0946152210235596\n",
      "Epoch 1, Iter 1000, Loss 0.9972342252731323\n",
      "Epoch 1, Iter 1100, Loss 0.9322819709777832\n",
      "Epoch 1, Iter 1200, Loss 1.0723620653152466\n",
      "Epoch 1, Iter 1300, Loss 1.0090266466140747\n",
      "Epoch 1, Iter 1400, Loss 1.000963807106018\n",
      "Epoch 1, Iter 1500, Loss 1.018961787223816\n",
      "Epoch 1, Iter 1600, Loss 0.9982019662857056\n",
      "Epoch 1, Iter 1700, Loss 1.0527750253677368\n",
      "Epoch 1, Iter 1800, Loss 1.0627063512802124\n",
      "Epoch 1, Iter 1900, Loss 0.8563344478607178\n",
      "Epoch 1, Iter 2000, Loss 0.9791532754898071\n",
      "Epoch 1, Iter 2100, Loss 0.9446344971656799\n",
      "Epoch 1, Iter 2200, Loss 0.9206578135490417\n",
      "Epoch 1, Iter 2300, Loss 1.0811957120895386\n",
      "Epoch 1, Iter 2400, Loss 0.9813905954360962\n",
      "Epoch 1, Val Loss 0.9844763875007629\n",
      "Model saved as crnn_512_1024_0.01_64_1_concat.pth\n",
      "Epoch 2, Iter 0, Loss 0.9512352347373962\n",
      "Epoch 2, Iter 100, Loss 0.8481729030609131\n",
      "Epoch 2, Iter 200, Loss 0.8918008208274841\n",
      "Epoch 2, Iter 300, Loss 1.0236226320266724\n",
      "Epoch 2, Iter 400, Loss 0.8823041319847107\n",
      "Epoch 2, Iter 500, Loss 0.8927241563796997\n",
      "Epoch 2, Iter 600, Loss 0.8526687622070312\n",
      "Epoch 2, Iter 700, Loss 0.9734471440315247\n",
      "Epoch 2, Iter 800, Loss 0.8911494612693787\n",
      "Epoch 2, Iter 900, Loss 0.9673645496368408\n",
      "Epoch 2, Iter 1000, Loss 1.0676226615905762\n",
      "Epoch 2, Iter 1100, Loss 0.963022768497467\n",
      "Epoch 2, Iter 1200, Loss 0.8481384515762329\n",
      "Epoch 2, Iter 1300, Loss 0.9317449927330017\n",
      "Epoch 2, Iter 1400, Loss 0.8558471202850342\n",
      "Epoch 2, Iter 1500, Loss 0.9443415999412537\n",
      "Epoch 2, Iter 1600, Loss 0.7451543211936951\n",
      "Epoch 2, Iter 1700, Loss 0.8402829766273499\n",
      "Epoch 2, Iter 1800, Loss 0.8506739735603333\n",
      "Epoch 2, Iter 1900, Loss 0.9091303944587708\n",
      "Epoch 2, Iter 2000, Loss 0.9308268427848816\n",
      "Epoch 2, Iter 2100, Loss 1.0080921649932861\n",
      "Epoch 2, Iter 2200, Loss 0.894766628742218\n",
      "Epoch 2, Iter 2300, Loss 0.8438102602958679\n",
      "Epoch 2, Iter 2400, Loss 0.8275728821754456\n",
      "Epoch 2, Val Loss 0.9230955839157104\n",
      "Model saved as crnn_512_1024_0.01_64_2_concat.pth\n",
      "Epoch 3, Iter 0, Loss 0.8367730975151062\n",
      "Epoch 3, Iter 100, Loss 0.7474372982978821\n",
      "Epoch 3, Iter 200, Loss 0.8156805634498596\n",
      "Epoch 3, Iter 300, Loss 0.8714712262153625\n",
      "Epoch 3, Iter 400, Loss 0.7937964797019958\n",
      "Epoch 3, Iter 500, Loss 0.7386032342910767\n",
      "Epoch 3, Iter 600, Loss 0.7101755738258362\n",
      "Epoch 3, Iter 700, Loss 0.7730867862701416\n",
      "Epoch 3, Iter 800, Loss 0.7918602228164673\n",
      "Epoch 3, Iter 900, Loss 0.7632796168327332\n",
      "Epoch 3, Iter 1000, Loss 0.8176136016845703\n",
      "Epoch 3, Iter 1100, Loss 0.7880274653434753\n",
      "Epoch 3, Iter 1200, Loss 0.7738922238349915\n",
      "Epoch 3, Iter 1300, Loss 0.7640413045883179\n",
      "Epoch 3, Iter 1400, Loss 0.7069149017333984\n",
      "Epoch 3, Iter 1500, Loss 0.8148480653762817\n",
      "Epoch 3, Iter 1600, Loss 0.9031561017036438\n",
      "Epoch 3, Iter 1700, Loss 0.7538071870803833\n",
      "Epoch 3, Iter 1800, Loss 0.8650796413421631\n",
      "Epoch 3, Iter 1900, Loss 0.8148499131202698\n",
      "Epoch 3, Iter 2000, Loss 0.7929847836494446\n",
      "Epoch 3, Iter 2100, Loss 0.8224824666976929\n",
      "Epoch 3, Iter 2200, Loss 0.8064704537391663\n",
      "Epoch 3, Iter 2300, Loss 0.7211715579032898\n",
      "Epoch 3, Iter 2400, Loss 0.8450083136558533\n",
      "Epoch 3, Val Loss 0.8858564496040344\n",
      "Model saved as crnn_512_1024_0.01_64_3_concat.pth\n",
      "Epoch 4, Iter 0, Loss 0.7451324462890625\n",
      "Epoch 4, Iter 100, Loss 0.7259986400604248\n",
      "Epoch 4, Iter 200, Loss 0.7467695474624634\n",
      "Epoch 4, Iter 300, Loss 0.6802439093589783\n",
      "Epoch 4, Iter 400, Loss 0.8343037962913513\n",
      "Epoch 4, Iter 500, Loss 0.7397634387016296\n",
      "Epoch 4, Iter 600, Loss 0.76587975025177\n",
      "Epoch 4, Iter 700, Loss 0.7558749914169312\n",
      "Epoch 4, Iter 800, Loss 0.8904268741607666\n",
      "Epoch 4, Iter 900, Loss 0.7817372679710388\n",
      "Epoch 4, Iter 1000, Loss 0.8348154425621033\n",
      "Epoch 4, Iter 1100, Loss 0.8119331002235413\n",
      "Epoch 4, Iter 1200, Loss 0.7974843978881836\n",
      "Epoch 4, Iter 1300, Loss 0.7998612523078918\n",
      "Epoch 4, Iter 1400, Loss 0.7286139130592346\n",
      "Epoch 4, Iter 1500, Loss 0.7742524147033691\n",
      "Epoch 4, Iter 1600, Loss 0.7823279500007629\n",
      "Epoch 4, Iter 1700, Loss 0.6505531072616577\n",
      "Epoch 4, Iter 1800, Loss 0.8139050006866455\n",
      "Epoch 4, Iter 1900, Loss 0.7454460859298706\n",
      "Epoch 4, Iter 2000, Loss 0.7539087533950806\n",
      "Epoch 4, Iter 2100, Loss 0.8488019108772278\n",
      "Epoch 4, Iter 2200, Loss 0.7235478758811951\n",
      "Epoch 4, Iter 2300, Loss 0.7614697217941284\n",
      "Epoch 4, Iter 2400, Loss 0.8007979393005371\n",
      "Epoch 4, Val Loss 0.885635495185852\n",
      "Model saved as crnn_512_1024_0.01_64_4_concat.pth\n",
      "Epoch 5, Iter 0, Loss 0.7773668169975281\n",
      "Epoch 5, Iter 100, Loss 0.6735051870346069\n",
      "Epoch 5, Iter 200, Loss 0.7049205303192139\n",
      "Epoch 5, Iter 300, Loss 0.7595945596694946\n",
      "Epoch 5, Iter 400, Loss 0.7419039011001587\n",
      "Epoch 5, Iter 500, Loss 0.699249267578125\n",
      "Epoch 5, Iter 600, Loss 0.709928035736084\n",
      "Epoch 5, Iter 700, Loss 0.7154501676559448\n",
      "Epoch 5, Iter 800, Loss 0.6634293794631958\n",
      "Epoch 5, Iter 900, Loss 0.732523500919342\n",
      "Epoch 5, Iter 1000, Loss 0.8045120239257812\n",
      "Epoch 5, Iter 1100, Loss 0.6838110685348511\n",
      "Epoch 5, Iter 1200, Loss 0.7142930030822754\n",
      "Epoch 5, Iter 1300, Loss 0.740450382232666\n",
      "Epoch 5, Iter 1400, Loss 0.6664451360702515\n",
      "Epoch 5, Iter 1500, Loss 0.6851823329925537\n",
      "Epoch 5, Iter 1600, Loss 0.7551421523094177\n",
      "Epoch 5, Iter 1700, Loss 0.7096320986747742\n",
      "Epoch 5, Iter 1800, Loss 0.740268886089325\n",
      "Epoch 5, Iter 1900, Loss 0.7649163603782654\n",
      "Epoch 5, Iter 2000, Loss 0.6952880024909973\n",
      "Epoch 5, Iter 2100, Loss 0.664237380027771\n",
      "Epoch 5, Iter 2200, Loss 0.7101499438285828\n",
      "Epoch 5, Iter 2300, Loss 0.7118300199508667\n",
      "Epoch 5, Iter 2400, Loss 0.7907702922821045\n",
      "Epoch 5, Val Loss 0.8633680939674377\n",
      "Model saved as crnn_512_1024_0.01_64_5_concat.pth\n",
      "Epoch 6, Iter 0, Loss 0.6912723183631897\n",
      "Epoch 6, Iter 100, Loss 0.6812650561332703\n",
      "Epoch 6, Iter 200, Loss 0.7197921872138977\n",
      "Epoch 6, Iter 300, Loss 0.7098588943481445\n",
      "Epoch 6, Iter 400, Loss 0.6585777401924133\n",
      "Epoch 6, Iter 500, Loss 0.7516196966171265\n",
      "Epoch 6, Iter 600, Loss 0.6702409386634827\n",
      "Epoch 6, Iter 700, Loss 0.6808294057846069\n",
      "Epoch 6, Iter 800, Loss 0.7328491806983948\n",
      "Epoch 6, Iter 900, Loss 0.6046391129493713\n",
      "Epoch 6, Iter 1000, Loss 0.7287841439247131\n",
      "Epoch 6, Iter 1100, Loss 0.6581361293792725\n",
      "Epoch 6, Iter 1200, Loss 0.6985920667648315\n",
      "Epoch 6, Iter 1300, Loss 0.6860924959182739\n",
      "Epoch 6, Iter 1400, Loss 0.6845388412475586\n",
      "Epoch 6, Iter 1500, Loss 0.7218404412269592\n",
      "Epoch 6, Iter 1600, Loss 0.811709463596344\n",
      "Epoch 6, Iter 1700, Loss 0.6696144938468933\n",
      "Epoch 6, Iter 1800, Loss 0.7923794984817505\n",
      "Epoch 6, Iter 1900, Loss 0.6471032500267029\n",
      "Epoch 6, Iter 2000, Loss 0.8778939247131348\n",
      "Epoch 6, Iter 2100, Loss 0.626716136932373\n",
      "Epoch 6, Iter 2200, Loss 0.703981339931488\n",
      "Epoch 6, Iter 2300, Loss 0.6960045099258423\n",
      "Epoch 6, Iter 2400, Loss 0.6409810185432434\n",
      "Epoch 6, Val Loss 0.8558743000030518\n",
      "Model saved as crnn_512_1024_0.01_64_6_concat.pth\n",
      "Epoch 7, Iter 0, Loss 0.6917866468429565\n",
      "Epoch 7, Iter 100, Loss 0.6550357937812805\n",
      "Epoch 7, Iter 200, Loss 0.7140138149261475\n",
      "Epoch 7, Iter 300, Loss 0.6766175627708435\n",
      "Epoch 7, Iter 400, Loss 0.641167938709259\n",
      "Epoch 7, Iter 500, Loss 0.6543654799461365\n",
      "Epoch 7, Iter 600, Loss 0.6541722416877747\n",
      "Epoch 7, Iter 700, Loss 0.6987073421478271\n",
      "Epoch 7, Iter 800, Loss 0.7099760174751282\n",
      "Epoch 7, Iter 900, Loss 0.5903094410896301\n",
      "Epoch 7, Iter 1000, Loss 0.6725497245788574\n",
      "Epoch 7, Iter 1100, Loss 0.6982675194740295\n",
      "Epoch 7, Iter 1200, Loss 0.6551539897918701\n",
      "Epoch 7, Iter 1300, Loss 0.6456422805786133\n",
      "Epoch 7, Iter 1400, Loss 0.6839749813079834\n",
      "Epoch 7, Iter 1500, Loss 0.6722283959388733\n",
      "Epoch 7, Iter 1600, Loss 0.6552234292030334\n",
      "Epoch 7, Iter 1700, Loss 0.7821468114852905\n",
      "Epoch 7, Iter 1800, Loss 0.6685475707054138\n",
      "Epoch 7, Iter 1900, Loss 0.7172432541847229\n",
      "Epoch 7, Iter 2000, Loss 0.6392026543617249\n",
      "Epoch 7, Iter 2100, Loss 0.6352857351303101\n",
      "Epoch 7, Iter 2200, Loss 0.7102295756340027\n",
      "Epoch 7, Iter 2300, Loss 0.7469110488891602\n",
      "Epoch 7, Iter 2400, Loss 0.6093575954437256\n",
      "Epoch 7, Val Loss 0.8726586699485779\n",
      "Model saved as crnn_512_1024_0.01_64_7_concat.pth\n",
      "Epoch 8, Iter 0, Loss 0.6148527264595032\n",
      "Epoch 8, Iter 100, Loss 0.6796013116836548\n",
      "Epoch 8, Iter 200, Loss 0.7407427430152893\n",
      "Epoch 8, Iter 300, Loss 0.709114134311676\n",
      "Epoch 8, Iter 400, Loss 0.6541376113891602\n",
      "Epoch 8, Iter 500, Loss 0.6992056965827942\n",
      "Epoch 8, Iter 600, Loss 0.6288543343544006\n",
      "Epoch 8, Iter 700, Loss 0.68187016248703\n",
      "Epoch 8, Iter 800, Loss 0.6634471416473389\n",
      "Epoch 8, Iter 900, Loss 0.7043834924697876\n",
      "Epoch 8, Iter 1000, Loss 0.6941480040550232\n",
      "Epoch 8, Iter 1100, Loss 0.6702298521995544\n",
      "Epoch 8, Iter 1200, Loss 0.7017757892608643\n",
      "Epoch 8, Iter 1300, Loss 0.5912584066390991\n",
      "Epoch 8, Iter 1400, Loss 0.6516461372375488\n",
      "Epoch 8, Iter 1500, Loss 0.6922304630279541\n",
      "Epoch 8, Iter 1600, Loss 0.6463048458099365\n",
      "Epoch 8, Iter 1700, Loss 0.7113245129585266\n",
      "Epoch 8, Iter 1800, Loss 0.6798273324966431\n",
      "Epoch 8, Iter 1900, Loss 0.6396331787109375\n",
      "Epoch 8, Iter 2000, Loss 0.6406991481781006\n",
      "Epoch 8, Iter 2100, Loss 0.7031717300415039\n",
      "Epoch 8, Iter 2200, Loss 0.6250576972961426\n",
      "Epoch 8, Iter 2300, Loss 0.6753690838813782\n",
      "Epoch 8, Iter 2400, Loss 0.6432124972343445\n",
      "Epoch 8, Val Loss 0.8477666974067688\n",
      "Model saved as crnn_512_1024_0.01_64_8_concat.pth\n",
      "Epoch 9, Iter 0, Loss 0.7196074724197388\n",
      "Epoch 9, Iter 100, Loss 0.6732690930366516\n",
      "Epoch 9, Iter 200, Loss 0.715542733669281\n",
      "Epoch 9, Iter 300, Loss 0.8079083561897278\n",
      "Epoch 9, Iter 400, Loss 0.646016538143158\n",
      "Epoch 9, Iter 500, Loss 0.6570200324058533\n",
      "Epoch 9, Iter 600, Loss 0.5995827913284302\n",
      "Epoch 9, Iter 700, Loss 0.660622775554657\n",
      "Epoch 9, Iter 800, Loss 0.6565453410148621\n",
      "Epoch 9, Iter 900, Loss 0.6332693696022034\n",
      "Epoch 9, Iter 1000, Loss 0.6378382444381714\n",
      "Epoch 9, Iter 1100, Loss 0.6490262150764465\n",
      "Epoch 9, Iter 1200, Loss 0.6338174939155579\n",
      "Epoch 9, Iter 1300, Loss 0.6371698379516602\n",
      "Epoch 9, Iter 1400, Loss 0.5812628865242004\n",
      "Epoch 9, Iter 1500, Loss 0.629788875579834\n",
      "Epoch 9, Iter 1600, Loss 0.6482405662536621\n",
      "Epoch 9, Iter 1700, Loss 0.6145994067192078\n",
      "Epoch 9, Iter 1800, Loss 0.6747534275054932\n",
      "Epoch 9, Iter 1900, Loss 0.6562951803207397\n",
      "Epoch 9, Iter 2000, Loss 0.6040459275245667\n",
      "Epoch 9, Iter 2100, Loss 0.6583492755889893\n",
      "Epoch 9, Iter 2200, Loss 0.6377971172332764\n",
      "Epoch 9, Iter 2300, Loss 0.6410706639289856\n",
      "Epoch 9, Iter 2400, Loss 0.702980101108551\n",
      "Epoch 9, Val Loss 0.8479127287864685\n",
      "Model saved as crnn_512_1024_0.01_64_9_concat.pth\n",
      "Epoch 10, Iter 0, Loss 0.6045663952827454\n",
      "Epoch 10, Iter 100, Loss 0.66862952709198\n",
      "Epoch 10, Iter 200, Loss 0.6277762651443481\n",
      "Epoch 10, Iter 300, Loss 0.641745388507843\n",
      "Epoch 10, Iter 400, Loss 0.6739733815193176\n",
      "Epoch 10, Iter 500, Loss 0.64421147108078\n",
      "Epoch 10, Iter 600, Loss 0.6625770926475525\n",
      "Epoch 10, Iter 700, Loss 0.6461025476455688\n",
      "Epoch 10, Iter 800, Loss 0.6135432124137878\n",
      "Epoch 10, Iter 900, Loss 0.6835547089576721\n",
      "Epoch 10, Iter 1000, Loss 0.7469442486763\n",
      "Epoch 10, Iter 1100, Loss 0.6164817214012146\n",
      "Epoch 10, Iter 1200, Loss 0.641041100025177\n",
      "Epoch 10, Iter 1300, Loss 0.6322289109230042\n",
      "Epoch 10, Iter 1400, Loss 0.6041855812072754\n",
      "Epoch 10, Iter 1500, Loss 0.6251066327095032\n",
      "Epoch 10, Iter 1600, Loss 0.637520432472229\n",
      "Epoch 10, Iter 1700, Loss 0.6332772970199585\n",
      "Epoch 10, Iter 1800, Loss 0.6060969233512878\n",
      "Epoch 10, Iter 1900, Loss 0.6734420657157898\n",
      "Epoch 10, Iter 2000, Loss 0.6253505349159241\n",
      "Epoch 10, Iter 2100, Loss 0.6758087277412415\n",
      "Epoch 10, Iter 2200, Loss 0.665839672088623\n",
      "Epoch 10, Iter 2300, Loss 0.690955400466919\n",
      "Epoch 10, Iter 2400, Loss 0.6484416127204895\n",
      "Epoch 10, Val Loss 0.8465003967285156\n",
      "Model saved as crnn_512_1024_0.01_64_10_concat.pth\n",
      "Epoch 11, Iter 0, Loss 0.6758331656455994\n",
      "Epoch 11, Iter 100, Loss 0.637638509273529\n",
      "Epoch 11, Iter 200, Loss 0.6380845904350281\n",
      "Epoch 11, Iter 300, Loss 0.6502414345741272\n",
      "Epoch 11, Iter 400, Loss 0.6608503460884094\n",
      "Epoch 11, Iter 500, Loss 0.560932457447052\n",
      "Epoch 11, Iter 600, Loss 0.5928613543510437\n",
      "Epoch 11, Iter 700, Loss 0.6479188203811646\n",
      "Epoch 11, Iter 800, Loss 0.6585907936096191\n",
      "Epoch 11, Iter 900, Loss 0.5904350876808167\n",
      "Epoch 11, Iter 1000, Loss 0.6074440479278564\n",
      "Epoch 11, Iter 1100, Loss 0.6194220185279846\n",
      "Epoch 11, Iter 1200, Loss 0.5757673978805542\n",
      "Epoch 11, Iter 1300, Loss 0.6236964464187622\n",
      "Epoch 11, Iter 1400, Loss 0.6267377138137817\n",
      "Epoch 11, Iter 1500, Loss 0.6391588449478149\n",
      "Epoch 11, Iter 1600, Loss 0.6639596819877625\n",
      "Epoch 11, Iter 1700, Loss 0.6692124605178833\n",
      "Epoch 11, Iter 1800, Loss 0.6656555533409119\n",
      "Epoch 11, Iter 1900, Loss 0.6299060583114624\n",
      "Epoch 11, Iter 2000, Loss 0.599873960018158\n",
      "Epoch 11, Iter 2100, Loss 0.6095358729362488\n",
      "Epoch 11, Iter 2200, Loss 0.680991530418396\n",
      "Epoch 11, Iter 2300, Loss 0.6281029582023621\n",
      "Epoch 11, Iter 2400, Loss 0.7326776385307312\n",
      "Epoch 11, Val Loss 0.8493291139602661\n",
      "Model saved as crnn_512_1024_0.01_64_11_concat.pth\n",
      "Epoch 12, Iter 0, Loss 0.6506684422492981\n",
      "Epoch 12, Iter 100, Loss 0.6220666170120239\n",
      "Epoch 12, Iter 200, Loss 0.6552325487136841\n",
      "Epoch 12, Iter 300, Loss 0.6833469867706299\n",
      "Epoch 12, Iter 400, Loss 0.6549312472343445\n",
      "Epoch 12, Iter 500, Loss 0.6041677594184875\n",
      "Epoch 12, Iter 600, Loss 0.5885763764381409\n",
      "Epoch 12, Iter 700, Loss 0.6145790815353394\n",
      "Epoch 12, Iter 800, Loss 0.6699811816215515\n",
      "Epoch 12, Iter 900, Loss 0.6374553442001343\n",
      "Epoch 12, Iter 1000, Loss 0.6613397598266602\n",
      "Epoch 12, Iter 1100, Loss 0.6576418876647949\n",
      "Epoch 12, Iter 1200, Loss 0.6095722913742065\n",
      "Epoch 12, Iter 1300, Loss 0.6233075261116028\n",
      "Epoch 12, Iter 1400, Loss 0.6350904107093811\n",
      "Epoch 12, Iter 1500, Loss 0.6594955921173096\n",
      "Epoch 12, Iter 1600, Loss 0.6598250865936279\n",
      "Epoch 12, Iter 1700, Loss 0.6218222975730896\n",
      "Epoch 12, Iter 1800, Loss 0.6170850992202759\n",
      "Epoch 12, Iter 1900, Loss 0.6186123490333557\n",
      "Epoch 12, Iter 2000, Loss 0.5734594464302063\n",
      "Epoch 12, Iter 2100, Loss 0.6179317831993103\n",
      "Epoch 12, Iter 2200, Loss 0.6414713859558105\n",
      "Epoch 12, Iter 2300, Loss 0.6720719933509827\n",
      "Epoch 12, Iter 2400, Loss 0.6048616170883179\n",
      "Epoch 12, Val Loss 0.853696346282959\n",
      "Model saved as crnn_512_1024_0.01_64_12_concat.pth\n",
      "Epoch 13, Iter 0, Loss 0.588347852230072\n",
      "Epoch 13, Iter 100, Loss 0.635410487651825\n",
      "Epoch 13, Iter 200, Loss 0.6291425228118896\n",
      "Epoch 13, Iter 300, Loss 0.6673722863197327\n",
      "Epoch 13, Iter 400, Loss 0.5936537981033325\n",
      "Epoch 13, Iter 500, Loss 0.5812081694602966\n",
      "Epoch 13, Iter 600, Loss 0.6172560453414917\n",
      "Epoch 13, Iter 700, Loss 0.6084866523742676\n",
      "Epoch 13, Iter 800, Loss 0.6494271159172058\n",
      "Epoch 13, Iter 900, Loss 0.6143536567687988\n",
      "Epoch 13, Iter 1000, Loss 0.5773719549179077\n",
      "Epoch 13, Iter 1100, Loss 0.6073648929595947\n",
      "Epoch 13, Iter 1200, Loss 0.6455620527267456\n",
      "Epoch 13, Iter 1300, Loss 0.626293420791626\n",
      "Epoch 13, Iter 1400, Loss 0.6861134171485901\n",
      "Epoch 13, Iter 1500, Loss 0.6344332098960876\n",
      "Epoch 13, Iter 1600, Loss 0.6163899302482605\n",
      "Epoch 13, Iter 1700, Loss 0.6704216599464417\n",
      "Epoch 13, Iter 1800, Loss 0.6372509598731995\n",
      "Epoch 13, Iter 1900, Loss 0.6642482280731201\n",
      "Epoch 13, Iter 2000, Loss 0.5997856855392456\n",
      "Epoch 13, Iter 2100, Loss 0.6656880974769592\n",
      "Epoch 13, Iter 2200, Loss 0.6030134558677673\n",
      "Epoch 13, Iter 2300, Loss 0.6103041768074036\n",
      "Epoch 13, Iter 2400, Loss 0.6623836755752563\n",
      "Epoch 13, Val Loss 0.8457669019699097\n",
      "Model saved as crnn_512_1024_0.01_64_13_concat.pth\n",
      "Epoch 14, Iter 0, Loss 0.6301112771034241\n",
      "Epoch 14, Iter 100, Loss 0.5864985585212708\n",
      "Epoch 14, Iter 200, Loss 0.5982652902603149\n",
      "Epoch 14, Iter 300, Loss 0.6778043508529663\n",
      "Epoch 14, Iter 400, Loss 0.609867513179779\n",
      "Epoch 14, Iter 500, Loss 0.5801709890365601\n",
      "Epoch 14, Iter 600, Loss 0.5866541266441345\n",
      "Epoch 14, Iter 700, Loss 0.6343980431556702\n",
      "Epoch 14, Iter 800, Loss 0.6418692469596863\n",
      "Epoch 14, Iter 900, Loss 0.6069754958152771\n",
      "Epoch 14, Iter 1000, Loss 0.686445415019989\n",
      "Epoch 14, Iter 1100, Loss 0.6041113138198853\n",
      "Epoch 14, Iter 1200, Loss 0.5624831318855286\n",
      "Epoch 14, Iter 1300, Loss 0.5987131595611572\n",
      "Epoch 14, Iter 1400, Loss 0.6232572793960571\n",
      "Epoch 14, Iter 1500, Loss 0.6606426239013672\n",
      "Epoch 14, Iter 1600, Loss 0.5955958366394043\n",
      "Epoch 14, Iter 1700, Loss 0.6270647048950195\n",
      "Epoch 14, Iter 1800, Loss 0.6185407042503357\n",
      "Epoch 14, Iter 1900, Loss 0.5840821266174316\n",
      "Epoch 14, Iter 2000, Loss 0.5808143019676208\n",
      "Epoch 14, Iter 2100, Loss 0.6066932678222656\n",
      "Epoch 14, Iter 2200, Loss 0.6123574376106262\n",
      "Epoch 14, Iter 2300, Loss 0.566926896572113\n",
      "Epoch 14, Iter 2400, Loss 0.5986703634262085\n",
      "Epoch 14, Val Loss 0.8549323678016663\n",
      "Model saved as crnn_512_1024_0.01_64_14_concat.pth\n",
      "Epoch 15, Iter 0, Loss 0.6530345678329468\n",
      "Epoch 15, Iter 100, Loss 0.6553548574447632\n",
      "Epoch 15, Iter 200, Loss 0.6295455098152161\n",
      "Epoch 15, Iter 300, Loss 0.6685270667076111\n",
      "Epoch 15, Iter 400, Loss 0.5967144966125488\n",
      "Epoch 15, Iter 500, Loss 0.5701010227203369\n",
      "Epoch 15, Iter 600, Loss 0.613340437412262\n",
      "Epoch 15, Iter 700, Loss 0.6304805874824524\n",
      "Epoch 15, Iter 800, Loss 0.600584089756012\n",
      "Epoch 15, Iter 900, Loss 0.6661034822463989\n",
      "Epoch 15, Iter 1000, Loss 0.6448329091072083\n",
      "Epoch 15, Iter 1100, Loss 0.59255450963974\n",
      "Epoch 15, Iter 1200, Loss 0.6319243311882019\n",
      "Epoch 15, Iter 1300, Loss 0.6872782111167908\n",
      "Epoch 15, Iter 1400, Loss 0.6094090342521667\n",
      "Epoch 15, Iter 1500, Loss 0.651249885559082\n",
      "Epoch 15, Iter 1600, Loss 0.609794020652771\n",
      "Epoch 15, Iter 1700, Loss 0.6382164359092712\n",
      "Epoch 15, Iter 1800, Loss 0.6482405662536621\n",
      "Epoch 15, Iter 1900, Loss 0.6380184292793274\n",
      "Epoch 15, Iter 2000, Loss 0.5990002155303955\n",
      "Epoch 15, Iter 2100, Loss 0.574888288974762\n",
      "Epoch 15, Iter 2200, Loss 0.5733826756477356\n",
      "Epoch 15, Iter 2300, Loss 0.5675932765007019\n",
      "Epoch 15, Iter 2400, Loss 0.6357318758964539\n",
      "Epoch 15, Val Loss 0.8615770936012268\n",
      "Model saved as crnn_512_1024_0.01_64_15_concat.pth\n",
      "Epoch 16, Iter 0, Loss 0.6507822275161743\n",
      "Epoch 16, Iter 100, Loss 0.5753784775733948\n",
      "Epoch 16, Iter 200, Loss 0.5960317850112915\n",
      "Epoch 16, Iter 300, Loss 0.6241257190704346\n",
      "Epoch 16, Iter 400, Loss 0.6203041672706604\n",
      "Epoch 16, Iter 500, Loss 0.6153162717819214\n",
      "Epoch 16, Iter 600, Loss 0.6107627749443054\n",
      "Epoch 16, Iter 700, Loss 0.5717652440071106\n",
      "Epoch 16, Iter 800, Loss 0.6239216923713684\n",
      "Epoch 16, Iter 900, Loss 0.6713333129882812\n",
      "Epoch 16, Iter 1000, Loss 0.6360583901405334\n",
      "Epoch 16, Iter 1100, Loss 0.6419217586517334\n",
      "Epoch 16, Iter 1200, Loss 0.6561548709869385\n",
      "Epoch 16, Iter 1300, Loss 0.6724357604980469\n",
      "Epoch 16, Iter 1400, Loss 0.5525926947593689\n",
      "Epoch 16, Iter 1500, Loss 0.633478045463562\n",
      "Epoch 16, Iter 1600, Loss 0.6300234198570251\n",
      "Epoch 16, Iter 1700, Loss 0.6225536465644836\n",
      "Epoch 16, Iter 1800, Loss 0.6314847469329834\n",
      "Epoch 16, Iter 1900, Loss 0.5760636329650879\n",
      "Epoch 16, Iter 2000, Loss 0.6118362545967102\n",
      "Epoch 16, Iter 2100, Loss 0.5558585524559021\n",
      "Epoch 16, Iter 2200, Loss 0.586672842502594\n",
      "Epoch 16, Iter 2300, Loss 0.5864340662956238\n",
      "Epoch 16, Iter 2400, Loss 0.6125914454460144\n",
      "Epoch 16, Val Loss 0.8598555326461792\n",
      "Model saved as crnn_512_1024_0.01_64_16_concat.pth\n",
      "Epoch 17, Iter 0, Loss 0.6070682406425476\n",
      "Epoch 17, Iter 100, Loss 0.6431978344917297\n",
      "Epoch 17, Iter 200, Loss 0.6138282418251038\n",
      "Epoch 17, Iter 300, Loss 0.6413614153862\n",
      "Epoch 17, Iter 400, Loss 0.611730694770813\n",
      "Epoch 17, Iter 500, Loss 0.6113258600234985\n",
      "Epoch 17, Iter 600, Loss 0.6117423176765442\n",
      "Epoch 17, Iter 700, Loss 0.6708548665046692\n",
      "Epoch 17, Iter 800, Loss 0.6578115224838257\n",
      "Epoch 17, Iter 900, Loss 0.5911479592323303\n",
      "Epoch 17, Iter 1000, Loss 0.6455566883087158\n",
      "Epoch 17, Iter 1100, Loss 0.6020717620849609\n",
      "Epoch 17, Iter 1200, Loss 0.6223856210708618\n",
      "Epoch 17, Iter 1300, Loss 0.6150726675987244\n",
      "Epoch 17, Iter 1400, Loss 0.5915257930755615\n",
      "Epoch 17, Iter 1500, Loss 0.570045530796051\n",
      "Epoch 17, Iter 1600, Loss 0.6130195260047913\n",
      "Epoch 17, Iter 1700, Loss 0.6113281846046448\n",
      "Epoch 17, Iter 1800, Loss 0.6288509368896484\n",
      "Epoch 17, Iter 1900, Loss 0.5762174129486084\n",
      "Epoch 17, Iter 2000, Loss 0.6077583432197571\n",
      "Epoch 17, Iter 2100, Loss 0.6439836621284485\n",
      "Epoch 17, Iter 2200, Loss 0.5748580694198608\n",
      "Epoch 17, Iter 2300, Loss 0.639358639717102\n",
      "Epoch 17, Iter 2400, Loss 0.5593205094337463\n",
      "Epoch 17, Val Loss 0.8559617400169373\n",
      "Model saved as crnn_512_1024_0.01_64_17_concat.pth\n",
      "Epoch 18, Iter 0, Loss 0.5673303008079529\n",
      "Epoch 18, Iter 100, Loss 0.6694633960723877\n",
      "Epoch 18, Iter 200, Loss 0.6264730095863342\n",
      "Epoch 18, Iter 300, Loss 0.6284030079841614\n",
      "Epoch 18, Iter 400, Loss 0.5747983455657959\n",
      "Epoch 18, Iter 500, Loss 0.6417452096939087\n",
      "Epoch 18, Iter 600, Loss 0.6388069987297058\n",
      "Epoch 18, Iter 700, Loss 0.6151157021522522\n",
      "Epoch 18, Iter 800, Loss 0.597088634967804\n",
      "Epoch 18, Iter 900, Loss 0.5660473108291626\n",
      "Epoch 18, Iter 1000, Loss 0.652784526348114\n",
      "Epoch 18, Iter 1100, Loss 0.6554211378097534\n",
      "Epoch 18, Iter 1200, Loss 0.6528633236885071\n",
      "Epoch 18, Iter 1300, Loss 0.6145048141479492\n",
      "Epoch 18, Iter 1400, Loss 0.6111957430839539\n",
      "Epoch 18, Iter 1500, Loss 0.6009539365768433\n",
      "Epoch 18, Iter 1600, Loss 0.6018794775009155\n",
      "Epoch 18, Iter 1700, Loss 0.6045120358467102\n",
      "Epoch 18, Iter 1800, Loss 0.6522094011306763\n",
      "Epoch 18, Iter 1900, Loss 0.6168304681777954\n",
      "Epoch 18, Iter 2000, Loss 0.6941896677017212\n",
      "Epoch 18, Iter 2100, Loss 0.6306263208389282\n",
      "Epoch 18, Iter 2200, Loss 0.5900970697402954\n",
      "Epoch 18, Iter 2300, Loss 0.5622552633285522\n",
      "Epoch 18, Iter 2400, Loss 0.6283344030380249\n",
      "Epoch 18, Val Loss 0.8686366081237793\n",
      "Model saved as crnn_512_1024_0.01_64_18_concat.pth\n",
      "Epoch 19, Iter 0, Loss 0.6585049629211426\n",
      "Epoch 19, Iter 100, Loss 0.5738875865936279\n",
      "Epoch 19, Iter 200, Loss 0.5889348983764648\n",
      "Epoch 19, Iter 300, Loss 0.6331639885902405\n",
      "Epoch 19, Iter 400, Loss 0.6024900674819946\n",
      "Epoch 19, Iter 500, Loss 0.6160846948623657\n",
      "Epoch 19, Iter 600, Loss 0.5977298617362976\n",
      "Epoch 19, Iter 700, Loss 0.590412437915802\n",
      "Epoch 19, Iter 800, Loss 0.5750525593757629\n",
      "Epoch 19, Iter 900, Loss 0.6338761448860168\n",
      "Epoch 19, Iter 1000, Loss 0.5913625955581665\n",
      "Epoch 19, Iter 1100, Loss 0.583446741104126\n",
      "Epoch 19, Iter 1200, Loss 0.6352846026420593\n",
      "Epoch 19, Iter 1300, Loss 0.5801433324813843\n",
      "Epoch 19, Iter 1400, Loss 0.6076280474662781\n",
      "Epoch 19, Iter 1500, Loss 0.6525123715400696\n",
      "Epoch 19, Iter 1600, Loss 0.607542872428894\n",
      "Epoch 19, Iter 1700, Loss 0.6547350883483887\n",
      "Epoch 19, Iter 1800, Loss 0.6292174458503723\n",
      "Epoch 19, Iter 1900, Loss 0.6069638729095459\n",
      "Epoch 19, Iter 2000, Loss 0.5725721716880798\n",
      "Epoch 19, Iter 2100, Loss 0.612091064453125\n",
      "Epoch 19, Iter 2200, Loss 0.6170769333839417\n",
      "Epoch 19, Iter 2300, Loss 0.6072553992271423\n",
      "Epoch 19, Iter 2400, Loss 0.581824004650116\n",
      "Epoch 19, Val Loss 0.8689414858818054\n",
      "Model saved as crnn_512_1024_0.01_64_19_concat.pth\n",
      "Epoch 20, Iter 0, Loss 0.6790762543678284\n",
      "Epoch 20, Iter 100, Loss 0.623894989490509\n",
      "Epoch 20, Iter 200, Loss 0.6993625164031982\n",
      "Epoch 20, Iter 300, Loss 0.6012663841247559\n",
      "Epoch 20, Iter 400, Loss 0.5910288095474243\n",
      "Epoch 20, Iter 500, Loss 0.6390034556388855\n",
      "Epoch 20, Iter 600, Loss 0.6008865833282471\n",
      "Epoch 20, Iter 700, Loss 0.6474499106407166\n",
      "Epoch 20, Iter 800, Loss 0.6354643106460571\n",
      "Epoch 20, Iter 900, Loss 0.6472802758216858\n",
      "Epoch 20, Iter 1000, Loss 0.6025301218032837\n",
      "Epoch 20, Iter 1100, Loss 0.6114392876625061\n",
      "Epoch 20, Iter 1200, Loss 0.6704134345054626\n",
      "Epoch 20, Iter 1300, Loss 0.6474003195762634\n",
      "Epoch 20, Iter 1400, Loss 0.6232044696807861\n",
      "Epoch 20, Iter 1500, Loss 0.5975939631462097\n",
      "Epoch 20, Iter 1600, Loss 0.6450964212417603\n",
      "Epoch 20, Iter 1700, Loss 0.5889536142349243\n",
      "Epoch 20, Iter 1800, Loss 0.6611738801002502\n",
      "Epoch 20, Iter 1900, Loss 0.6280126571655273\n",
      "Epoch 20, Iter 2000, Loss 0.5889927744865417\n",
      "Epoch 20, Iter 2100, Loss 0.5358752012252808\n",
      "Epoch 20, Iter 2200, Loss 0.5524362325668335\n",
      "Epoch 20, Iter 2300, Loss 0.6401688456535339\n",
      "Epoch 20, Iter 2400, Loss 0.6510143280029297\n",
      "Epoch 20, Val Loss 0.8563942313194275\n",
      "Model saved as crnn_512_1024_0.01_64_20_concat.pth\n",
      "Epoch 21, Iter 0, Loss 0.5831092000007629\n",
      "Epoch 21, Iter 100, Loss 0.5847952961921692\n",
      "Epoch 21, Iter 200, Loss 0.6113676428794861\n",
      "Epoch 21, Iter 300, Loss 0.6058963537216187\n",
      "Epoch 21, Iter 400, Loss 0.5859804153442383\n",
      "Epoch 21, Iter 500, Loss 0.5871113538742065\n",
      "Epoch 21, Iter 600, Loss 0.5719752311706543\n",
      "Epoch 21, Iter 700, Loss 0.5745508074760437\n",
      "Epoch 21, Iter 800, Loss 0.6318078637123108\n",
      "Epoch 21, Iter 900, Loss 0.5936183333396912\n",
      "Epoch 21, Iter 1000, Loss 0.674969494342804\n",
      "Epoch 21, Iter 1100, Loss 0.6395300626754761\n",
      "Epoch 21, Iter 1200, Loss 0.6320432424545288\n",
      "Epoch 21, Iter 1300, Loss 0.5570393800735474\n",
      "Epoch 21, Iter 1400, Loss 0.587837815284729\n",
      "Epoch 21, Iter 1500, Loss 0.5628660917282104\n",
      "Epoch 21, Iter 1600, Loss 0.5912174582481384\n",
      "Epoch 21, Iter 1700, Loss 0.6431692242622375\n",
      "Epoch 21, Iter 1800, Loss 0.595974862575531\n",
      "Epoch 21, Iter 1900, Loss 0.6097819805145264\n",
      "Epoch 21, Iter 2000, Loss 0.6518270373344421\n",
      "Epoch 21, Iter 2100, Loss 0.5767470598220825\n",
      "Epoch 21, Iter 2200, Loss 0.560043215751648\n",
      "Epoch 21, Iter 2300, Loss 0.6277288794517517\n",
      "Epoch 21, Iter 2400, Loss 0.6465176343917847\n",
      "Epoch 21, Val Loss 0.8580848574638367\n",
      "Model saved as crnn_512_1024_0.01_64_21_concat.pth\n",
      "Epoch 22, Iter 0, Loss 0.5554181933403015\n",
      "Epoch 22, Iter 100, Loss 0.5758383274078369\n",
      "Epoch 22, Iter 200, Loss 0.6128886938095093\n",
      "Epoch 22, Iter 300, Loss 0.6131219267845154\n",
      "Epoch 22, Iter 400, Loss 0.5676417350769043\n",
      "Epoch 22, Iter 500, Loss 0.6137827634811401\n",
      "Epoch 22, Iter 600, Loss 0.5905666947364807\n",
      "Epoch 22, Iter 700, Loss 0.5742635130882263\n",
      "Epoch 22, Iter 800, Loss 0.6090167164802551\n",
      "Epoch 22, Iter 900, Loss 0.6399385929107666\n",
      "Epoch 22, Iter 1000, Loss 0.5804741978645325\n",
      "Epoch 22, Iter 1100, Loss 0.5949422121047974\n",
      "Epoch 22, Iter 1200, Loss 0.5932761430740356\n",
      "Epoch 22, Iter 1300, Loss 0.5843108892440796\n",
      "Epoch 22, Iter 1400, Loss 0.5792089700698853\n",
      "Epoch 22, Iter 1500, Loss 0.6454316973686218\n",
      "Epoch 22, Iter 1600, Loss 0.6508514285087585\n",
      "Epoch 22, Iter 1700, Loss 0.5951377153396606\n",
      "Epoch 22, Iter 1800, Loss 0.5948066711425781\n",
      "Epoch 22, Iter 1900, Loss 0.5880730748176575\n",
      "Epoch 22, Iter 2000, Loss 0.5558553338050842\n",
      "Epoch 22, Iter 2100, Loss 0.6040589809417725\n",
      "Epoch 22, Iter 2200, Loss 0.5842088460922241\n",
      "Epoch 22, Iter 2300, Loss 0.6558700799942017\n",
      "Epoch 22, Iter 2400, Loss 0.609371542930603\n",
      "Epoch 22, Val Loss 0.8593382835388184\n",
      "Model saved as crnn_512_1024_0.01_64_22_concat.pth\n",
      "Epoch 23, Iter 0, Loss 0.5657876133918762\n",
      "Epoch 23, Iter 100, Loss 0.640825629234314\n",
      "Epoch 23, Iter 200, Loss 0.599844753742218\n",
      "Epoch 23, Iter 300, Loss 0.565614640712738\n",
      "Epoch 23, Iter 400, Loss 0.6269124746322632\n",
      "Epoch 23, Iter 500, Loss 0.6031307578086853\n",
      "Epoch 23, Iter 600, Loss 0.5851126909255981\n",
      "Epoch 23, Iter 700, Loss 0.5833082795143127\n",
      "Epoch 23, Iter 800, Loss 0.6358641386032104\n",
      "Epoch 23, Iter 900, Loss 0.5857049822807312\n",
      "Epoch 23, Iter 1000, Loss 0.597057580947876\n",
      "Epoch 23, Iter 1100, Loss 0.5837134122848511\n",
      "Epoch 23, Iter 1200, Loss 0.6037470698356628\n",
      "Epoch 23, Iter 1300, Loss 0.5788565874099731\n",
      "Epoch 23, Iter 1400, Loss 0.6196189522743225\n",
      "Epoch 23, Iter 1500, Loss 0.5644901990890503\n",
      "Epoch 23, Iter 1600, Loss 0.5731328129768372\n",
      "Epoch 23, Iter 1700, Loss 0.5949323177337646\n",
      "Epoch 23, Iter 1800, Loss 0.5907907485961914\n",
      "Epoch 23, Iter 1900, Loss 0.5775651335716248\n",
      "Epoch 23, Iter 2000, Loss 0.5822582840919495\n",
      "Epoch 23, Iter 2100, Loss 0.5740256905555725\n",
      "Epoch 23, Iter 2200, Loss 0.6253458261489868\n",
      "Epoch 23, Iter 2300, Loss 0.6017546057701111\n",
      "Epoch 23, Iter 2400, Loss 0.6359303593635559\n",
      "Epoch 23, Val Loss 0.8601731061935425\n",
      "Model saved as crnn_512_1024_0.01_64_23_concat.pth\n",
      "Epoch 24, Iter 0, Loss 0.5758480429649353\n",
      "Epoch 24, Iter 100, Loss 0.6075577139854431\n",
      "Epoch 24, Iter 200, Loss 0.63804692029953\n",
      "Epoch 24, Iter 300, Loss 0.5636709928512573\n",
      "Epoch 24, Iter 400, Loss 0.6179521679878235\n",
      "Epoch 24, Iter 500, Loss 0.5847504734992981\n",
      "Epoch 24, Iter 600, Loss 0.5877138376235962\n",
      "Epoch 24, Iter 700, Loss 0.6211325526237488\n",
      "Epoch 24, Iter 800, Loss 0.5858088731765747\n",
      "Epoch 24, Iter 900, Loss 0.6336135268211365\n",
      "Epoch 24, Iter 1000, Loss 0.5830994844436646\n",
      "Epoch 24, Iter 1100, Loss 0.640460193157196\n",
      "Epoch 24, Iter 1200, Loss 0.5412299036979675\n",
      "Epoch 24, Iter 1300, Loss 0.5945513844490051\n",
      "Epoch 24, Iter 1400, Loss 0.6210622191429138\n",
      "Epoch 24, Iter 1500, Loss 0.6687076687812805\n",
      "Epoch 24, Iter 1600, Loss 0.6292656064033508\n",
      "Epoch 24, Iter 1700, Loss 0.5942806005477905\n",
      "Epoch 24, Iter 1800, Loss 0.5987699031829834\n",
      "Epoch 24, Iter 1900, Loss 0.5961111783981323\n",
      "Epoch 24, Iter 2000, Loss 0.575477659702301\n",
      "Epoch 24, Iter 2100, Loss 0.5946557521820068\n",
      "Epoch 24, Iter 2200, Loss 0.6551185846328735\n",
      "Epoch 24, Iter 2300, Loss 0.6139273047447205\n",
      "Epoch 24, Iter 2400, Loss 0.5612897872924805\n",
      "Epoch 24, Val Loss 0.8624158501625061\n",
      "Model saved as crnn_512_1024_0.01_64_24_concat.pth\n",
      "Epoch 25, Iter 0, Loss 0.5856845378875732\n",
      "Epoch 25, Iter 100, Loss 0.6367727518081665\n",
      "Epoch 25, Iter 200, Loss 0.6540760397911072\n",
      "Epoch 25, Iter 300, Loss 0.6013582348823547\n",
      "Epoch 25, Iter 400, Loss 0.5314526557922363\n",
      "Epoch 25, Iter 500, Loss 0.5570529103279114\n",
      "Epoch 25, Iter 600, Loss 0.6183488368988037\n",
      "Epoch 25, Iter 700, Loss 0.6028328537940979\n",
      "Epoch 25, Iter 800, Loss 0.6199910640716553\n",
      "Epoch 25, Iter 900, Loss 0.5758699774742126\n",
      "Epoch 25, Iter 1000, Loss 0.5900223255157471\n",
      "Epoch 25, Iter 1100, Loss 0.6162149310112\n",
      "Epoch 25, Iter 1200, Loss 0.6009047031402588\n",
      "Epoch 25, Iter 1300, Loss 0.5838433504104614\n",
      "Epoch 25, Iter 1400, Loss 0.633603572845459\n",
      "Epoch 25, Iter 1500, Loss 0.6189652681350708\n",
      "Epoch 25, Iter 1600, Loss 0.6104879379272461\n",
      "Epoch 25, Iter 1700, Loss 0.587497353553772\n",
      "Epoch 25, Iter 1800, Loss 0.6095227003097534\n",
      "Epoch 25, Iter 1900, Loss 0.6274279356002808\n",
      "Epoch 25, Iter 2000, Loss 0.5701718926429749\n",
      "Epoch 25, Iter 2100, Loss 0.5999715924263\n",
      "Epoch 25, Iter 2200, Loss 0.6417685747146606\n",
      "Epoch 25, Iter 2300, Loss 0.5891506671905518\n",
      "Epoch 25, Iter 2400, Loss 0.6133671402931213\n",
      "Epoch 25, Val Loss 0.8648799061775208\n",
      "Model saved as crnn_512_1024_0.01_64_25_concat.pth\n",
      "Epoch 26, Iter 0, Loss 0.641445517539978\n",
      "Epoch 26, Iter 100, Loss 0.6341824531555176\n",
      "Epoch 26, Iter 200, Loss 0.6224765181541443\n",
      "Epoch 26, Iter 300, Loss 0.6153568029403687\n",
      "Epoch 26, Iter 400, Loss 0.588321328163147\n",
      "Epoch 26, Iter 500, Loss 0.6259934306144714\n",
      "Epoch 26, Iter 600, Loss 0.5806072354316711\n",
      "Epoch 26, Iter 700, Loss 0.6412878036499023\n",
      "Epoch 26, Iter 800, Loss 0.6490307450294495\n",
      "Epoch 26, Iter 900, Loss 0.6251416802406311\n",
      "Epoch 26, Iter 1000, Loss 0.6145007014274597\n",
      "Epoch 26, Iter 1100, Loss 0.5903674364089966\n",
      "Epoch 26, Iter 1200, Loss 0.6123942136764526\n",
      "Epoch 26, Iter 1300, Loss 0.5860438942909241\n",
      "Epoch 26, Iter 1400, Loss 0.6156075596809387\n",
      "Epoch 26, Iter 1500, Loss 0.64999920129776\n",
      "Epoch 26, Iter 1600, Loss 0.6156752109527588\n",
      "Epoch 26, Iter 1700, Loss 0.5989691615104675\n",
      "Epoch 26, Iter 1800, Loss 0.6287409067153931\n",
      "Epoch 26, Iter 1900, Loss 0.620684802532196\n",
      "Epoch 26, Iter 2000, Loss 0.5594572424888611\n",
      "Epoch 26, Iter 2100, Loss 0.5858990550041199\n",
      "Epoch 26, Iter 2200, Loss 0.6119867563247681\n",
      "Epoch 26, Iter 2300, Loss 0.5942712426185608\n",
      "Epoch 26, Iter 2400, Loss 0.611416757106781\n",
      "Epoch 26, Val Loss 0.8651416301727295\n",
      "Model saved as crnn_512_1024_0.01_64_26_concat.pth\n",
      "Epoch 27, Iter 0, Loss 0.6066411137580872\n",
      "Epoch 27, Iter 100, Loss 0.582336962223053\n",
      "Epoch 27, Iter 200, Loss 0.6396812200546265\n",
      "Epoch 27, Iter 300, Loss 0.612042248249054\n",
      "Epoch 27, Iter 400, Loss 0.5919749140739441\n",
      "Epoch 27, Iter 500, Loss 0.5711589455604553\n",
      "Epoch 27, Iter 600, Loss 0.6901922225952148\n",
      "Epoch 27, Iter 700, Loss 0.5770987272262573\n",
      "Epoch 27, Iter 800, Loss 0.6153208613395691\n",
      "Epoch 27, Iter 900, Loss 0.5965554118156433\n",
      "Epoch 27, Iter 1000, Loss 0.6230408549308777\n",
      "Epoch 27, Iter 1100, Loss 0.5854654312133789\n",
      "Epoch 27, Iter 1200, Loss 0.5451219081878662\n",
      "Epoch 27, Iter 1300, Loss 0.6119481325149536\n",
      "Epoch 27, Iter 1400, Loss 0.591501772403717\n",
      "Epoch 27, Iter 1500, Loss 0.550162136554718\n",
      "Epoch 27, Iter 1600, Loss 0.6585077047348022\n",
      "Epoch 27, Iter 1700, Loss 0.6165511608123779\n",
      "Epoch 27, Iter 1800, Loss 0.5968139171600342\n",
      "Epoch 27, Iter 1900, Loss 0.5947582125663757\n",
      "Epoch 27, Iter 2000, Loss 0.6059232950210571\n",
      "Epoch 27, Iter 2100, Loss 0.5761843919754028\n",
      "Epoch 27, Iter 2200, Loss 0.6233744621276855\n",
      "Epoch 27, Iter 2300, Loss 0.5767446160316467\n",
      "Epoch 27, Iter 2400, Loss 0.5906959176063538\n",
      "Epoch 27, Val Loss 0.8660063743591309\n",
      "Model saved as crnn_512_1024_0.01_64_27_concat.pth\n",
      "Epoch 28, Iter 0, Loss 0.6419075131416321\n",
      "Epoch 28, Iter 100, Loss 0.591524064540863\n",
      "Epoch 28, Iter 200, Loss 0.5833307504653931\n",
      "Epoch 28, Iter 300, Loss 0.5821068286895752\n",
      "Epoch 28, Iter 400, Loss 0.6382761001586914\n",
      "Epoch 28, Iter 500, Loss 0.5752231478691101\n",
      "Epoch 28, Iter 600, Loss 0.6348847150802612\n",
      "Epoch 28, Iter 700, Loss 0.671481728553772\n",
      "Epoch 28, Iter 800, Loss 0.6600348353385925\n",
      "Epoch 28, Iter 900, Loss 0.6144989132881165\n",
      "Epoch 28, Iter 1000, Loss 0.6195240020751953\n",
      "Epoch 28, Iter 1100, Loss 0.5738406181335449\n",
      "Epoch 28, Iter 1200, Loss 0.5500282645225525\n",
      "Epoch 28, Iter 1300, Loss 0.607064962387085\n",
      "Epoch 28, Iter 1400, Loss 0.569802463054657\n",
      "Epoch 28, Iter 1500, Loss 0.5760005712509155\n",
      "Epoch 28, Iter 1600, Loss 0.6365357041358948\n",
      "Epoch 28, Iter 1700, Loss 0.6386151909828186\n",
      "Epoch 28, Iter 1800, Loss 0.6118936538696289\n",
      "Epoch 28, Iter 1900, Loss 0.6060152649879456\n",
      "Epoch 28, Iter 2000, Loss 0.6265395283699036\n",
      "Epoch 28, Iter 2100, Loss 0.6250251531600952\n",
      "Epoch 28, Iter 2200, Loss 0.6076784729957581\n",
      "Epoch 28, Iter 2300, Loss 0.6191447973251343\n",
      "Epoch 28, Iter 2400, Loss 0.5384281277656555\n",
      "Epoch 28, Val Loss 0.8669956922531128\n",
      "Model saved as crnn_512_1024_0.01_64_28_concat.pth\n",
      "Epoch 29, Iter 0, Loss 0.5985733866691589\n",
      "Epoch 29, Iter 100, Loss 0.6281105279922485\n",
      "Epoch 29, Iter 200, Loss 0.6015337705612183\n",
      "Epoch 29, Iter 300, Loss 0.5907869338989258\n",
      "Epoch 29, Iter 400, Loss 0.6115180253982544\n",
      "Epoch 29, Iter 500, Loss 0.6155797243118286\n",
      "Epoch 29, Iter 600, Loss 0.5981114506721497\n",
      "Epoch 29, Iter 700, Loss 0.5943014621734619\n",
      "Epoch 29, Iter 800, Loss 0.5976868867874146\n",
      "Epoch 29, Iter 900, Loss 0.5893451571464539\n",
      "Epoch 29, Iter 1000, Loss 0.6071187853813171\n",
      "Epoch 29, Iter 1100, Loss 0.580385684967041\n",
      "Epoch 29, Iter 1200, Loss 0.5922276973724365\n",
      "Epoch 29, Iter 1300, Loss 0.6376639008522034\n",
      "Epoch 29, Iter 1400, Loss 0.5954515933990479\n",
      "Epoch 29, Iter 1500, Loss 0.6288127303123474\n",
      "Epoch 29, Iter 1600, Loss 0.6458503007888794\n",
      "Epoch 29, Iter 1700, Loss 0.6164181232452393\n",
      "Epoch 29, Iter 1800, Loss 0.6145089864730835\n",
      "Epoch 29, Iter 1900, Loss 0.5848084688186646\n",
      "Epoch 29, Iter 2000, Loss 0.6352629661560059\n",
      "Epoch 29, Iter 2100, Loss 0.5688971281051636\n",
      "Epoch 29, Iter 2200, Loss 0.5941147804260254\n",
      "Epoch 29, Iter 2300, Loss 0.5834981799125671\n",
      "Epoch 29, Iter 2400, Loss 0.6489822864532471\n",
      "Epoch 29, Val Loss 0.8675349354743958\n",
      "Model saved as crnn_512_1024_0.01_64_29_concat.pth\n",
      "Epoch 30, Iter 0, Loss 0.5808162093162537\n",
      "Epoch 30, Iter 100, Loss 0.6238813400268555\n",
      "Epoch 30, Iter 200, Loss 0.6144945621490479\n",
      "Epoch 30, Iter 300, Loss 0.6298882961273193\n",
      "Epoch 30, Iter 400, Loss 0.5952595472335815\n",
      "Epoch 30, Iter 500, Loss 0.6251764297485352\n",
      "Epoch 30, Iter 600, Loss 0.6242902278900146\n",
      "Epoch 30, Iter 700, Loss 0.5857677459716797\n",
      "Epoch 30, Iter 800, Loss 0.5925516486167908\n",
      "Epoch 30, Iter 900, Loss 0.557583749294281\n",
      "Epoch 30, Iter 1000, Loss 0.5728171467781067\n",
      "Epoch 30, Iter 1100, Loss 0.6209284067153931\n",
      "Epoch 30, Iter 1200, Loss 0.565337598323822\n",
      "Epoch 30, Iter 1300, Loss 0.6202062368392944\n",
      "Epoch 30, Iter 1400, Loss 0.600888729095459\n",
      "Epoch 30, Iter 1500, Loss 0.6684293746948242\n",
      "Epoch 30, Iter 1600, Loss 0.6051722764968872\n",
      "Epoch 30, Iter 1700, Loss 0.5979061722755432\n",
      "Epoch 30, Iter 1800, Loss 0.6471394896507263\n",
      "Epoch 30, Iter 1900, Loss 0.6204901933670044\n",
      "Epoch 30, Iter 2000, Loss 0.5699732899665833\n",
      "Epoch 30, Iter 2100, Loss 0.6345109939575195\n",
      "Epoch 30, Iter 2200, Loss 0.6109891533851624\n",
      "Epoch 30, Iter 2300, Loss 0.6050561666488647\n",
      "Epoch 30, Iter 2400, Loss 0.6083217859268188\n",
      "Epoch 30, Val Loss 0.8679599761962891\n",
      "Model saved as crnn_512_1024_0.01_64_30_concat.pth\n",
      "Epoch 31, Iter 0, Loss 0.5905823111534119\n",
      "Epoch 31, Iter 100, Loss 0.6764192581176758\n",
      "Epoch 31, Iter 200, Loss 0.6116649508476257\n",
      "Epoch 31, Iter 300, Loss 0.6349757313728333\n",
      "Epoch 31, Iter 400, Loss 0.6412469744682312\n",
      "Epoch 31, Iter 500, Loss 0.6687926650047302\n",
      "Epoch 31, Iter 600, Loss 0.6067370772361755\n",
      "Epoch 31, Iter 700, Loss 0.6352214217185974\n",
      "Epoch 31, Iter 800, Loss 0.5554091334342957\n",
      "Epoch 31, Iter 900, Loss 0.6311975121498108\n",
      "Epoch 31, Iter 1000, Loss 0.606448769569397\n",
      "Epoch 31, Iter 1100, Loss 0.6305697560310364\n",
      "Epoch 31, Iter 1200, Loss 0.6067715883255005\n",
      "Epoch 31, Iter 1300, Loss 0.5648183226585388\n",
      "Epoch 31, Iter 1400, Loss 0.567155659198761\n",
      "Epoch 31, Iter 1500, Loss 0.6290537118911743\n",
      "Epoch 31, Iter 1600, Loss 0.6740292906761169\n",
      "Epoch 31, Iter 1700, Loss 0.5990752577781677\n",
      "Epoch 31, Iter 1800, Loss 0.6500288248062134\n",
      "Epoch 31, Iter 1900, Loss 0.5959795117378235\n",
      "Epoch 31, Iter 2000, Loss 0.6015394926071167\n",
      "Epoch 31, Iter 2100, Loss 0.6187445521354675\n",
      "Epoch 31, Iter 2200, Loss 0.5962610244750977\n",
      "Epoch 31, Iter 2300, Loss 0.638357400894165\n",
      "Epoch 31, Iter 2400, Loss 0.6345608234405518\n",
      "Epoch 31, Val Loss 0.8685983419418335\n",
      "Model saved as crnn_512_1024_0.01_64_31_concat.pth\n",
      "Epoch 32, Iter 0, Loss 0.6016217470169067\n",
      "Epoch 32, Iter 100, Loss 0.610373318195343\n",
      "Epoch 32, Iter 200, Loss 0.608817994594574\n",
      "Epoch 32, Iter 300, Loss 0.5909906029701233\n",
      "Epoch 32, Iter 400, Loss 0.5483359694480896\n",
      "Epoch 32, Iter 500, Loss 0.6626452207565308\n",
      "Epoch 32, Iter 600, Loss 0.6052942276000977\n",
      "Epoch 32, Iter 700, Loss 0.5705166459083557\n",
      "Epoch 32, Iter 800, Loss 0.6267364621162415\n",
      "Epoch 32, Iter 900, Loss 0.6029159426689148\n",
      "Epoch 32, Iter 1000, Loss 0.6001759767532349\n",
      "Epoch 32, Iter 1100, Loss 0.5768231749534607\n",
      "Epoch 32, Iter 1200, Loss 0.619684100151062\n",
      "Epoch 32, Iter 1300, Loss 0.5847864151000977\n",
      "Epoch 32, Iter 1400, Loss 0.6000588536262512\n",
      "Epoch 32, Iter 1500, Loss 0.6543440818786621\n",
      "Epoch 32, Iter 1600, Loss 0.5843780040740967\n",
      "Epoch 32, Iter 1700, Loss 0.580501914024353\n",
      "Epoch 32, Iter 1800, Loss 0.5660032629966736\n",
      "Epoch 32, Iter 1900, Loss 0.6322267055511475\n",
      "Epoch 32, Iter 2000, Loss 0.6114966869354248\n",
      "Epoch 32, Iter 2100, Loss 0.6251926422119141\n",
      "Epoch 32, Iter 2200, Loss 0.5860472321510315\n",
      "Epoch 32, Iter 2300, Loss 0.606614351272583\n",
      "Epoch 32, Iter 2400, Loss 0.6137217879295349\n",
      "Epoch 32, Val Loss 0.8689265251159668\n",
      "Model saved as crnn_512_1024_0.01_64_32_concat.pth\n",
      "Epoch 33, Iter 0, Loss 0.6697456240653992\n",
      "Epoch 33, Iter 100, Loss 0.5925533771514893\n",
      "Epoch 33, Iter 200, Loss 0.6163061857223511\n",
      "Epoch 33, Iter 300, Loss 0.6451476216316223\n",
      "Epoch 33, Iter 400, Loss 0.6264094710350037\n",
      "Epoch 33, Iter 500, Loss 0.5693977475166321\n",
      "Epoch 33, Iter 600, Loss 0.6252809762954712\n",
      "Epoch 33, Iter 700, Loss 0.6144516468048096\n",
      "Epoch 33, Iter 800, Loss 0.6068864464759827\n",
      "Epoch 33, Iter 900, Loss 0.6087974309921265\n",
      "Epoch 33, Iter 1000, Loss 0.6049004793167114\n",
      "Epoch 33, Iter 1100, Loss 0.6088439226150513\n",
      "Epoch 33, Iter 1200, Loss 0.6264821887016296\n",
      "Epoch 33, Iter 1300, Loss 0.5804091095924377\n",
      "Epoch 33, Iter 1400, Loss 0.6356932520866394\n",
      "Epoch 33, Iter 1500, Loss 0.6040840744972229\n",
      "Epoch 33, Iter 1600, Loss 0.5838624238967896\n",
      "Epoch 33, Iter 1700, Loss 0.6150749921798706\n",
      "Epoch 33, Iter 1800, Loss 0.6180229187011719\n",
      "Epoch 33, Iter 1900, Loss 0.6285420656204224\n",
      "Epoch 33, Iter 2000, Loss 0.6144628524780273\n",
      "Epoch 33, Iter 2100, Loss 0.6179507970809937\n",
      "Epoch 33, Iter 2200, Loss 0.6126987934112549\n",
      "Epoch 33, Iter 2300, Loss 0.5779234170913696\n",
      "Epoch 33, Iter 2400, Loss 0.6071597337722778\n",
      "Epoch 33, Val Loss 0.8698765635490417\n",
      "Model saved as crnn_512_1024_0.01_64_33_concat.pth\n",
      "Epoch 34, Iter 0, Loss 0.5869331955909729\n",
      "Epoch 34, Iter 100, Loss 0.6622884273529053\n",
      "Epoch 34, Iter 200, Loss 0.6499714255332947\n",
      "Epoch 34, Iter 300, Loss 0.6015135049819946\n",
      "Epoch 34, Iter 400, Loss 0.610276460647583\n",
      "Epoch 34, Iter 500, Loss 0.6008395552635193\n",
      "Epoch 34, Iter 600, Loss 0.5811420679092407\n",
      "Epoch 34, Iter 700, Loss 0.619760274887085\n",
      "Epoch 34, Iter 800, Loss 0.5907596945762634\n",
      "Epoch 34, Iter 900, Loss 0.5714860558509827\n",
      "Epoch 34, Iter 1000, Loss 0.6620633006095886\n",
      "Epoch 34, Iter 1100, Loss 0.5662922263145447\n",
      "Epoch 34, Iter 1200, Loss 0.6177864670753479\n",
      "Epoch 34, Iter 1300, Loss 0.6165043115615845\n",
      "Epoch 34, Iter 1400, Loss 0.6199167370796204\n",
      "Epoch 34, Iter 1500, Loss 0.5709369778633118\n",
      "Epoch 34, Iter 1600, Loss 0.596370279788971\n",
      "Epoch 34, Iter 1700, Loss 0.6240719556808472\n",
      "Epoch 34, Iter 1800, Loss 0.5869622826576233\n",
      "Epoch 34, Iter 1900, Loss 0.572160542011261\n",
      "Epoch 34, Iter 2000, Loss 0.6154347062110901\n",
      "Epoch 34, Iter 2100, Loss 0.5688908696174622\n",
      "Epoch 34, Iter 2200, Loss 0.5736074447631836\n",
      "Epoch 34, Iter 2300, Loss 0.6153428554534912\n",
      "Epoch 34, Iter 2400, Loss 0.5887069702148438\n",
      "Epoch 34, Val Loss 0.8701580762863159\n",
      "Model saved as crnn_512_1024_0.01_64_34_concat.pth\n",
      "Epoch 35, Iter 0, Loss 0.6335915327072144\n",
      "Epoch 35, Iter 100, Loss 0.6078971028327942\n",
      "Epoch 35, Iter 200, Loss 0.5838846564292908\n",
      "Epoch 35, Iter 300, Loss 0.5871037244796753\n",
      "Epoch 35, Iter 400, Loss 0.5775029063224792\n",
      "Epoch 35, Iter 500, Loss 0.6335513591766357\n",
      "Epoch 35, Iter 600, Loss 0.5724312663078308\n",
      "Epoch 35, Iter 700, Loss 0.6544866561889648\n",
      "Epoch 35, Iter 800, Loss 0.5712777376174927\n",
      "Epoch 35, Iter 900, Loss 0.5955698490142822\n",
      "Epoch 35, Iter 1000, Loss 0.5694053173065186\n",
      "Epoch 35, Iter 1100, Loss 0.5693838000297546\n",
      "Epoch 35, Iter 1200, Loss 0.601742148399353\n",
      "Epoch 35, Iter 1300, Loss 0.5981614589691162\n",
      "Epoch 35, Iter 1400, Loss 0.6375588178634644\n",
      "Epoch 35, Iter 1500, Loss 0.5626099109649658\n",
      "Epoch 35, Iter 1600, Loss 0.5632774829864502\n",
      "Epoch 35, Iter 1700, Loss 0.6403605937957764\n",
      "Epoch 35, Iter 1800, Loss 0.5955977439880371\n",
      "Epoch 35, Iter 1900, Loss 0.6425508260726929\n",
      "Epoch 35, Iter 2000, Loss 0.6354325413703918\n",
      "Epoch 35, Iter 2100, Loss 0.6225693821907043\n",
      "Epoch 35, Iter 2200, Loss 0.5336548686027527\n",
      "Epoch 35, Iter 2300, Loss 0.6269898414611816\n",
      "Epoch 35, Iter 2400, Loss 0.6641610264778137\n",
      "Epoch 35, Val Loss 0.8701810240745544\n",
      "Model saved as crnn_512_1024_0.01_64_35_concat.pth\n",
      "Epoch 36, Iter 0, Loss 0.5640607476234436\n",
      "Epoch 36, Iter 100, Loss 0.558159589767456\n",
      "Epoch 36, Iter 200, Loss 0.5852975845336914\n",
      "Epoch 36, Iter 300, Loss 0.5849432945251465\n",
      "Epoch 36, Iter 400, Loss 0.6338241100311279\n",
      "Epoch 36, Iter 500, Loss 0.6760985851287842\n",
      "Epoch 36, Iter 600, Loss 0.5844373106956482\n",
      "Epoch 36, Iter 700, Loss 0.6318100690841675\n",
      "Epoch 36, Iter 800, Loss 0.579509973526001\n",
      "Epoch 36, Iter 900, Loss 0.557693600654602\n",
      "Epoch 36, Iter 1000, Loss 0.5871849060058594\n",
      "Epoch 36, Iter 1100, Loss 0.6178364157676697\n",
      "Epoch 36, Iter 1200, Loss 0.6180235743522644\n",
      "Epoch 36, Iter 1300, Loss 0.6734591126441956\n",
      "Epoch 36, Iter 1400, Loss 0.572802722454071\n",
      "Epoch 36, Iter 1500, Loss 0.5903961658477783\n",
      "Epoch 36, Iter 1600, Loss 0.6094585657119751\n",
      "Epoch 36, Iter 1700, Loss 0.6511272192001343\n",
      "Epoch 36, Iter 1800, Loss 0.5698678493499756\n",
      "Epoch 36, Iter 1900, Loss 0.5974956154823303\n",
      "Epoch 36, Iter 2000, Loss 0.6502895951271057\n",
      "Epoch 36, Iter 2100, Loss 0.5808857679367065\n",
      "Epoch 36, Iter 2200, Loss 0.6146296262741089\n",
      "Epoch 36, Iter 2300, Loss 0.5959312915802002\n",
      "Epoch 36, Iter 2400, Loss 0.611818253993988\n",
      "Epoch 36, Val Loss 0.8697133660316467\n",
      "Model saved as crnn_512_1024_0.01_64_36_concat.pth\n",
      "Epoch 37, Iter 0, Loss 0.5661887526512146\n",
      "Epoch 37, Iter 100, Loss 0.6320604085922241\n",
      "Epoch 37, Iter 200, Loss 0.6437466740608215\n",
      "Epoch 37, Iter 300, Loss 0.6335993409156799\n",
      "Epoch 37, Iter 400, Loss 0.5880967974662781\n",
      "Epoch 37, Iter 500, Loss 0.5793007016181946\n",
      "Epoch 37, Iter 600, Loss 0.5978123545646667\n",
      "Epoch 37, Iter 700, Loss 0.6747008562088013\n",
      "Epoch 37, Iter 800, Loss 0.5725077986717224\n",
      "Epoch 37, Iter 900, Loss 0.6002852320671082\n",
      "Epoch 37, Iter 1000, Loss 0.6025778651237488\n",
      "Epoch 37, Iter 1100, Loss 0.6185858249664307\n",
      "Epoch 37, Iter 1200, Loss 0.6250934600830078\n",
      "Epoch 37, Iter 1300, Loss 0.587864875793457\n",
      "Epoch 37, Iter 1400, Loss 0.5465443134307861\n",
      "Epoch 37, Iter 1500, Loss 0.6133818626403809\n",
      "Epoch 37, Iter 1600, Loss 0.607847273349762\n",
      "Epoch 37, Iter 1700, Loss 0.6355828046798706\n",
      "Epoch 37, Iter 1800, Loss 0.5818271040916443\n",
      "Epoch 37, Iter 1900, Loss 0.6013882160186768\n",
      "Epoch 37, Iter 2000, Loss 0.583253800868988\n",
      "Epoch 37, Iter 2100, Loss 0.5815650820732117\n",
      "Epoch 37, Iter 2200, Loss 0.6098127961158752\n",
      "Epoch 37, Iter 2300, Loss 0.6068527102470398\n",
      "Epoch 37, Iter 2400, Loss 0.6857293844223022\n",
      "Epoch 37, Val Loss 0.8706047534942627\n",
      "Model saved as crnn_512_1024_0.01_64_37_concat.pth\n",
      "Epoch 38, Iter 0, Loss 0.625396728515625\n",
      "Epoch 38, Iter 100, Loss 0.5996044278144836\n",
      "Epoch 38, Iter 200, Loss 0.6488451361656189\n",
      "Epoch 38, Iter 300, Loss 0.6241638660430908\n",
      "Epoch 38, Iter 400, Loss 0.6503069400787354\n",
      "Epoch 38, Iter 500, Loss 0.6518841981887817\n",
      "Epoch 38, Iter 600, Loss 0.587895929813385\n",
      "Epoch 38, Iter 700, Loss 0.6223207116127014\n",
      "Epoch 38, Iter 800, Loss 0.6179211139678955\n",
      "Epoch 38, Iter 900, Loss 0.6115249991416931\n",
      "Epoch 38, Iter 1000, Loss 0.6679108738899231\n",
      "Epoch 38, Iter 1100, Loss 0.6211293339729309\n",
      "Epoch 38, Iter 1200, Loss 0.56545490026474\n",
      "Epoch 38, Iter 1300, Loss 0.6072774529457092\n",
      "Epoch 38, Iter 1400, Loss 0.5799333453178406\n",
      "Epoch 38, Iter 1500, Loss 0.5860651731491089\n",
      "Epoch 38, Iter 1600, Loss 0.6029613018035889\n",
      "Epoch 38, Iter 1700, Loss 0.6161936521530151\n",
      "Epoch 38, Iter 1800, Loss 0.6350056529045105\n",
      "Epoch 38, Iter 1900, Loss 0.5564314723014832\n",
      "Epoch 38, Iter 2000, Loss 0.5962259769439697\n",
      "Epoch 38, Iter 2100, Loss 0.545084536075592\n",
      "Epoch 38, Iter 2200, Loss 0.5832709670066833\n",
      "Epoch 38, Iter 2300, Loss 0.5898405909538269\n",
      "Epoch 38, Iter 2400, Loss 0.5939879417419434\n",
      "Epoch 38, Val Loss 0.8709163665771484\n",
      "Model saved as crnn_512_1024_0.01_64_38_concat.pth\n",
      "Epoch 39, Iter 0, Loss 0.6750242114067078\n",
      "Epoch 39, Iter 100, Loss 0.575118899345398\n",
      "Epoch 39, Iter 200, Loss 0.5899909138679504\n",
      "Epoch 39, Iter 300, Loss 0.624880850315094\n",
      "Epoch 39, Iter 400, Loss 0.6095368266105652\n",
      "Epoch 39, Iter 500, Loss 0.5831555128097534\n",
      "Epoch 39, Iter 600, Loss 0.5889570116996765\n",
      "Epoch 39, Iter 700, Loss 0.6595052480697632\n",
      "Epoch 39, Iter 800, Loss 0.5935120582580566\n",
      "Epoch 39, Iter 900, Loss 0.6271304488182068\n",
      "Epoch 39, Iter 1000, Loss 0.6131318807601929\n",
      "Epoch 39, Iter 1100, Loss 0.6517055630683899\n",
      "Epoch 39, Iter 1200, Loss 0.6207752823829651\n",
      "Epoch 39, Iter 1300, Loss 0.6299117207527161\n",
      "Epoch 39, Iter 1400, Loss 0.6388841867446899\n",
      "Epoch 39, Iter 1500, Loss 0.6398290395736694\n",
      "Epoch 39, Iter 1600, Loss 0.5902398228645325\n",
      "Epoch 39, Iter 1700, Loss 0.6404168009757996\n",
      "Epoch 39, Iter 1800, Loss 0.5515305399894714\n",
      "Epoch 39, Iter 1900, Loss 0.5947321653366089\n",
      "Epoch 39, Iter 2000, Loss 0.5882754325866699\n",
      "Epoch 39, Iter 2100, Loss 0.5947370529174805\n",
      "Epoch 39, Iter 2200, Loss 0.5911656618118286\n",
      "Epoch 39, Iter 2300, Loss 0.570652186870575\n",
      "Epoch 39, Iter 2400, Loss 0.6221988201141357\n",
      "Epoch 39, Val Loss 0.8717706799507141\n",
      "Model saved as crnn_512_1024_0.01_64_39_concat.pth\n",
      "Epoch 40, Iter 0, Loss 0.583193838596344\n",
      "Epoch 40, Iter 100, Loss 0.6370081901550293\n",
      "Epoch 40, Iter 200, Loss 0.6401234269142151\n",
      "Epoch 40, Iter 300, Loss 0.5734509229660034\n",
      "Epoch 40, Iter 400, Loss 0.6078900694847107\n",
      "Epoch 40, Iter 500, Loss 0.6165297627449036\n",
      "Epoch 40, Iter 600, Loss 0.6580522656440735\n",
      "Epoch 40, Iter 700, Loss 0.5863593816757202\n",
      "Epoch 40, Iter 800, Loss 0.5982970595359802\n",
      "Epoch 40, Iter 900, Loss 0.5913239121437073\n",
      "Epoch 40, Iter 1000, Loss 0.6123877763748169\n",
      "Epoch 40, Iter 1100, Loss 0.6040428280830383\n",
      "Epoch 40, Iter 1200, Loss 0.5958456993103027\n",
      "Epoch 40, Iter 1300, Loss 0.6712582111358643\n",
      "Epoch 40, Iter 1400, Loss 0.625331461429596\n",
      "Epoch 40, Iter 1500, Loss 0.6014014482498169\n",
      "Epoch 40, Iter 1600, Loss 0.6521364450454712\n",
      "Epoch 40, Iter 1700, Loss 0.5724655985832214\n",
      "Epoch 40, Iter 1800, Loss 0.5791068077087402\n",
      "Epoch 40, Iter 1900, Loss 0.6265700459480286\n",
      "Epoch 40, Iter 2000, Loss 0.6011138558387756\n",
      "Epoch 40, Iter 2100, Loss 0.5557413697242737\n",
      "Epoch 40, Iter 2200, Loss 0.6149601936340332\n",
      "Epoch 40, Iter 2300, Loss 0.600017786026001\n",
      "Epoch 40, Iter 2400, Loss 0.5988929271697998\n",
      "Epoch 40, Val Loss 0.8710290789604187\n",
      "Model saved as crnn_512_1024_0.01_64_40_concat.pth\n",
      "Epoch 41, Iter 0, Loss 0.6199246048927307\n",
      "Epoch 41, Iter 100, Loss 0.5912555456161499\n",
      "Epoch 41, Iter 200, Loss 0.6056104302406311\n",
      "Epoch 41, Iter 300, Loss 0.5823208689689636\n",
      "Epoch 41, Iter 400, Loss 0.57121342420578\n",
      "Epoch 41, Iter 500, Loss 0.6843590140342712\n",
      "Epoch 41, Iter 600, Loss 0.5734917521476746\n",
      "Epoch 41, Iter 700, Loss 0.6556516885757446\n",
      "Epoch 41, Iter 800, Loss 0.6440101861953735\n",
      "Epoch 41, Iter 900, Loss 0.6359384655952454\n",
      "Epoch 41, Iter 1000, Loss 0.5630818009376526\n",
      "Epoch 41, Iter 1100, Loss 0.6219354867935181\n",
      "Epoch 41, Iter 1200, Loss 0.5916262865066528\n",
      "Epoch 41, Iter 1300, Loss 0.6160175204277039\n",
      "Epoch 41, Iter 1400, Loss 0.6355656981468201\n",
      "Epoch 41, Iter 1500, Loss 0.6367972493171692\n",
      "Epoch 41, Iter 1600, Loss 0.6300312876701355\n",
      "Epoch 41, Iter 1700, Loss 0.6469120979309082\n",
      "Epoch 41, Iter 1800, Loss 0.6376581192016602\n",
      "Epoch 41, Iter 1900, Loss 0.6145297884941101\n",
      "Epoch 41, Iter 2000, Loss 0.5735891461372375\n",
      "Epoch 41, Iter 2100, Loss 0.6437954306602478\n",
      "Epoch 41, Iter 2200, Loss 0.620961606502533\n",
      "Epoch 41, Iter 2300, Loss 0.6242915391921997\n",
      "Epoch 41, Iter 2400, Loss 0.590590238571167\n",
      "Epoch 41, Val Loss 0.871221661567688\n",
      "Model saved as crnn_512_1024_0.01_64_41_concat.pth\n",
      "Epoch 42, Iter 0, Loss 0.6238671541213989\n",
      "Epoch 42, Iter 100, Loss 0.6272639632225037\n",
      "Epoch 42, Iter 200, Loss 0.5876463651657104\n",
      "Epoch 42, Iter 300, Loss 0.6384714841842651\n",
      "Epoch 42, Iter 400, Loss 0.5538911819458008\n",
      "Epoch 42, Iter 500, Loss 0.6056092381477356\n",
      "Epoch 42, Iter 600, Loss 0.592941403388977\n",
      "Epoch 42, Iter 700, Loss 0.5695478916168213\n",
      "Epoch 42, Iter 800, Loss 0.6020351052284241\n",
      "Epoch 42, Iter 900, Loss 0.5813425779342651\n",
      "Epoch 42, Iter 1000, Loss 0.6961405277252197\n",
      "Epoch 42, Iter 1100, Loss 0.574761152267456\n",
      "Epoch 42, Iter 1200, Loss 0.5525357723236084\n",
      "Epoch 42, Iter 1300, Loss 0.6059056520462036\n",
      "Epoch 42, Iter 1400, Loss 0.6066570281982422\n",
      "Epoch 42, Iter 1500, Loss 0.6261453032493591\n",
      "Epoch 42, Iter 1600, Loss 0.624674916267395\n",
      "Epoch 42, Iter 1700, Loss 0.6201806664466858\n",
      "Epoch 42, Iter 1800, Loss 0.5464338660240173\n",
      "Epoch 42, Iter 1900, Loss 0.6037040948867798\n",
      "Epoch 42, Iter 2000, Loss 0.6024258732795715\n",
      "Epoch 42, Iter 2100, Loss 0.5721667408943176\n",
      "Epoch 42, Iter 2200, Loss 0.5900590419769287\n",
      "Epoch 42, Iter 2300, Loss 0.5899767875671387\n",
      "Epoch 42, Iter 2400, Loss 0.577438235282898\n",
      "Epoch 42, Val Loss 0.8721522092819214\n",
      "Model saved as crnn_512_1024_0.01_64_42_concat.pth\n",
      "Epoch 43, Iter 0, Loss 0.6069583296775818\n",
      "Epoch 43, Iter 100, Loss 0.6507850885391235\n",
      "Epoch 43, Iter 200, Loss 0.5960580706596375\n",
      "Epoch 43, Iter 300, Loss 0.5808056592941284\n",
      "Epoch 43, Iter 400, Loss 0.6268261671066284\n",
      "Epoch 43, Iter 500, Loss 0.6186749935150146\n",
      "Epoch 43, Iter 600, Loss 0.5552526712417603\n",
      "Epoch 43, Iter 700, Loss 0.5785505175590515\n",
      "Epoch 43, Iter 800, Loss 0.6048995852470398\n",
      "Epoch 43, Iter 900, Loss 0.6225534081459045\n",
      "Epoch 43, Iter 1000, Loss 0.636721134185791\n",
      "Epoch 43, Iter 1100, Loss 0.5622254014015198\n",
      "Epoch 43, Iter 1200, Loss 0.6126077771186829\n",
      "Epoch 43, Iter 1300, Loss 0.5968238115310669\n",
      "Epoch 43, Iter 1400, Loss 0.6275753378868103\n",
      "Epoch 43, Iter 1500, Loss 0.6130417585372925\n",
      "Epoch 43, Iter 1600, Loss 0.6103625893592834\n",
      "Epoch 43, Iter 1700, Loss 0.674726665019989\n",
      "Epoch 43, Iter 1800, Loss 0.608669102191925\n",
      "Epoch 43, Iter 1900, Loss 0.6177918314933777\n",
      "Epoch 43, Iter 2000, Loss 0.6400712132453918\n",
      "Epoch 43, Iter 2100, Loss 0.6197056770324707\n",
      "Epoch 43, Iter 2200, Loss 0.6851320266723633\n",
      "Epoch 43, Iter 2300, Loss 0.5812843441963196\n",
      "Epoch 43, Iter 2400, Loss 0.5747426152229309\n",
      "Epoch 43, Val Loss 0.871819257736206\n",
      "Model saved as crnn_512_1024_0.01_64_43_concat.pth\n",
      "Epoch 44, Iter 0, Loss 0.5729420781135559\n",
      "Epoch 44, Iter 100, Loss 0.6079698801040649\n",
      "Epoch 44, Iter 200, Loss 0.6292743682861328\n",
      "Epoch 44, Iter 300, Loss 0.6455040574073792\n",
      "Epoch 44, Iter 400, Loss 0.5892932415008545\n",
      "Epoch 44, Iter 500, Loss 0.6144051551818848\n",
      "Epoch 44, Iter 600, Loss 0.596094012260437\n",
      "Epoch 44, Iter 700, Loss 0.6062731146812439\n",
      "Epoch 44, Iter 800, Loss 0.57985520362854\n",
      "Epoch 44, Iter 900, Loss 0.566917896270752\n",
      "Epoch 44, Iter 1000, Loss 0.6157620549201965\n",
      "Epoch 44, Iter 1100, Loss 0.6503509879112244\n",
      "Epoch 44, Iter 1200, Loss 0.568727433681488\n",
      "Epoch 44, Iter 1300, Loss 0.631854772567749\n",
      "Epoch 44, Iter 1400, Loss 0.5868747234344482\n",
      "Epoch 44, Iter 1500, Loss 0.6268330812454224\n",
      "Epoch 44, Iter 1600, Loss 0.6046229600906372\n",
      "Epoch 44, Iter 1700, Loss 0.5571790933609009\n",
      "Epoch 44, Iter 1800, Loss 0.6380938291549683\n",
      "Epoch 44, Iter 1900, Loss 0.56178218126297\n",
      "Epoch 44, Iter 2000, Loss 0.5960012674331665\n",
      "Epoch 44, Iter 2100, Loss 0.6080554723739624\n",
      "Epoch 44, Iter 2200, Loss 0.6305440068244934\n",
      "Epoch 44, Iter 2300, Loss 0.6133648753166199\n",
      "Epoch 44, Iter 2400, Loss 0.6112037897109985\n",
      "Epoch 44, Val Loss 0.872116208076477\n",
      "Model saved as crnn_512_1024_0.01_64_44_concat.pth\n",
      "Epoch 45, Iter 0, Loss 0.6567426323890686\n",
      "Epoch 45, Iter 100, Loss 0.5828105807304382\n",
      "Epoch 45, Iter 200, Loss 0.5565606951713562\n",
      "Epoch 45, Iter 300, Loss 0.6219404339790344\n",
      "Epoch 45, Iter 400, Loss 0.6245981454849243\n",
      "Epoch 45, Iter 500, Loss 0.5743988156318665\n",
      "Epoch 45, Iter 600, Loss 0.5586559772491455\n",
      "Epoch 45, Iter 700, Loss 0.5879360437393188\n",
      "Epoch 45, Iter 800, Loss 0.63213711977005\n",
      "Epoch 45, Iter 900, Loss 0.6287257075309753\n",
      "Epoch 45, Iter 1000, Loss 0.6237088441848755\n",
      "Epoch 45, Iter 1100, Loss 0.6092706918716431\n",
      "Epoch 45, Iter 1200, Loss 0.6304352879524231\n",
      "Epoch 45, Iter 1300, Loss 0.581827700138092\n",
      "Epoch 45, Iter 1400, Loss 0.6012400388717651\n",
      "Epoch 45, Iter 1500, Loss 0.6507222056388855\n",
      "Epoch 45, Iter 1600, Loss 0.596631646156311\n",
      "Epoch 45, Iter 1700, Loss 0.6315754652023315\n",
      "Epoch 45, Iter 1800, Loss 0.6002164483070374\n",
      "Epoch 45, Iter 1900, Loss 0.593135416507721\n",
      "Epoch 45, Iter 2000, Loss 0.6012636423110962\n",
      "Epoch 45, Iter 2100, Loss 0.5971038341522217\n",
      "Epoch 45, Iter 2200, Loss 0.5814733505249023\n",
      "Epoch 45, Iter 2300, Loss 0.5846892595291138\n",
      "Epoch 45, Iter 2400, Loss 0.6114016175270081\n",
      "Epoch 45, Val Loss 0.8722454905509949\n",
      "Model saved as crnn_512_1024_0.01_64_45_concat.pth\n",
      "Epoch 46, Iter 0, Loss 0.5387899875640869\n",
      "Epoch 46, Iter 100, Loss 0.6440730690956116\n",
      "Epoch 46, Iter 200, Loss 0.6090728640556335\n",
      "Epoch 46, Iter 300, Loss 0.5655192732810974\n",
      "Epoch 46, Iter 400, Loss 0.6337329149246216\n",
      "Epoch 46, Iter 500, Loss 0.6054690480232239\n",
      "Epoch 46, Iter 600, Loss 0.5862521529197693\n",
      "Epoch 46, Iter 700, Loss 0.56221604347229\n",
      "Epoch 46, Iter 800, Loss 0.6088812947273254\n",
      "Epoch 46, Iter 900, Loss 0.6346908211708069\n",
      "Epoch 46, Iter 1000, Loss 0.6365482211112976\n",
      "Epoch 46, Iter 1100, Loss 0.6055310368537903\n",
      "Epoch 46, Iter 1200, Loss 0.5970313549041748\n",
      "Epoch 46, Iter 1300, Loss 0.5630308389663696\n",
      "Epoch 46, Iter 1400, Loss 0.5558322072029114\n",
      "Epoch 46, Iter 1500, Loss 0.6222941875457764\n",
      "Epoch 46, Iter 1600, Loss 0.5531830191612244\n",
      "Epoch 46, Iter 1700, Loss 0.6134319305419922\n",
      "Epoch 46, Iter 1800, Loss 0.5588644742965698\n",
      "Epoch 46, Iter 1900, Loss 0.594404935836792\n",
      "Epoch 46, Iter 2000, Loss 0.5992947220802307\n",
      "Epoch 46, Iter 2100, Loss 0.5998125076293945\n",
      "Epoch 46, Iter 2200, Loss 0.6116940379142761\n",
      "Epoch 46, Iter 2300, Loss 0.5800015330314636\n",
      "Epoch 46, Iter 2400, Loss 0.6318318247795105\n",
      "Epoch 46, Val Loss 0.871343195438385\n",
      "Model saved as crnn_512_1024_0.01_64_46_concat.pth\n",
      "Epoch 47, Iter 0, Loss 0.6607428789138794\n",
      "Epoch 47, Iter 100, Loss 0.6086190938949585\n",
      "Epoch 47, Iter 200, Loss 0.6105610728263855\n",
      "Epoch 47, Iter 300, Loss 0.5738325715065002\n",
      "Epoch 47, Iter 400, Loss 0.6677707433700562\n",
      "Epoch 47, Iter 500, Loss 0.5804842114448547\n",
      "Epoch 47, Iter 600, Loss 0.6256101727485657\n",
      "Epoch 47, Iter 700, Loss 0.63173508644104\n",
      "Epoch 47, Iter 800, Loss 0.6596128344535828\n",
      "Epoch 47, Iter 900, Loss 0.6262671947479248\n",
      "Epoch 47, Iter 1000, Loss 0.6113545298576355\n",
      "Epoch 47, Iter 1100, Loss 0.577053427696228\n",
      "Epoch 47, Iter 1200, Loss 0.6263637542724609\n",
      "Epoch 47, Iter 1300, Loss 0.6710051894187927\n",
      "Epoch 47, Iter 1400, Loss 0.6090553402900696\n",
      "Epoch 47, Iter 1500, Loss 0.607431948184967\n",
      "Epoch 47, Iter 1600, Loss 0.5989978909492493\n",
      "Epoch 47, Iter 1700, Loss 0.5606351494789124\n",
      "Epoch 47, Iter 1800, Loss 0.6166371703147888\n",
      "Epoch 47, Iter 1900, Loss 0.5923413634300232\n",
      "Epoch 47, Iter 2000, Loss 0.5997321009635925\n",
      "Epoch 47, Iter 2100, Loss 0.5685616731643677\n",
      "Epoch 47, Iter 2200, Loss 0.5835952758789062\n",
      "Epoch 47, Iter 2300, Loss 0.659141480922699\n",
      "Epoch 47, Iter 2400, Loss 0.6435703635215759\n",
      "Epoch 47, Val Loss 0.8726772665977478\n",
      "Model saved as crnn_512_1024_0.01_64_47_concat.pth\n",
      "Epoch 48, Iter 0, Loss 0.5669968128204346\n",
      "Epoch 48, Iter 100, Loss 0.5229550004005432\n",
      "Epoch 48, Iter 200, Loss 0.5998503565788269\n",
      "Epoch 48, Iter 300, Loss 0.6085989475250244\n",
      "Epoch 48, Iter 400, Loss 0.6262693405151367\n",
      "Epoch 48, Iter 500, Loss 0.6061177253723145\n",
      "Epoch 48, Iter 600, Loss 0.5808834433555603\n",
      "Epoch 48, Iter 700, Loss 0.5810732245445251\n",
      "Epoch 48, Iter 800, Loss 0.6539052724838257\n",
      "Epoch 48, Iter 900, Loss 0.6360433101654053\n",
      "Epoch 48, Iter 1000, Loss 0.615807294845581\n",
      "Epoch 48, Iter 1100, Loss 0.5885927081108093\n",
      "Epoch 48, Iter 1200, Loss 0.633932888507843\n",
      "Epoch 48, Iter 1300, Loss 0.5870349407196045\n",
      "Epoch 48, Iter 1400, Loss 0.6390365362167358\n",
      "Epoch 48, Iter 1500, Loss 0.5885228514671326\n",
      "Epoch 48, Iter 1600, Loss 0.5858411192893982\n",
      "Epoch 48, Iter 1700, Loss 0.6084035634994507\n",
      "Epoch 48, Iter 1800, Loss 0.587952733039856\n",
      "Epoch 48, Iter 1900, Loss 0.6036397814750671\n",
      "Epoch 48, Iter 2000, Loss 0.6604360938072205\n",
      "Epoch 48, Iter 2100, Loss 0.5953794121742249\n",
      "Epoch 48, Iter 2200, Loss 0.5957856178283691\n",
      "Epoch 48, Iter 2300, Loss 0.7070293426513672\n",
      "Epoch 48, Iter 2400, Loss 0.6590239405632019\n",
      "Epoch 48, Val Loss 0.8718864917755127\n",
      "Model saved as crnn_512_1024_0.01_64_48_concat.pth\n",
      "Epoch 49, Iter 0, Loss 0.6092543601989746\n",
      "Epoch 49, Iter 100, Loss 0.6041795611381531\n",
      "Epoch 49, Iter 200, Loss 0.6042096018791199\n",
      "Epoch 49, Iter 300, Loss 0.6063690781593323\n",
      "Epoch 49, Iter 400, Loss 0.6570155024528503\n",
      "Epoch 49, Iter 500, Loss 0.5654199719429016\n",
      "Epoch 49, Iter 600, Loss 0.5958459973335266\n",
      "Epoch 49, Iter 700, Loss 0.6132471561431885\n",
      "Epoch 49, Iter 800, Loss 0.6223528385162354\n",
      "Epoch 49, Iter 900, Loss 0.624214231967926\n",
      "Epoch 49, Iter 1000, Loss 0.5833254456520081\n",
      "Epoch 49, Iter 1100, Loss 0.6343072652816772\n",
      "Epoch 49, Iter 1200, Loss 0.6162763237953186\n",
      "Epoch 49, Iter 1300, Loss 0.6023539304733276\n",
      "Epoch 49, Iter 1400, Loss 0.6360293626785278\n",
      "Epoch 49, Iter 1500, Loss 0.6534885168075562\n",
      "Epoch 49, Iter 1600, Loss 0.5843613743782043\n",
      "Epoch 49, Iter 1700, Loss 0.6172817349433899\n",
      "Epoch 49, Iter 1800, Loss 0.6349624991416931\n",
      "Epoch 49, Iter 1900, Loss 0.5767151117324829\n",
      "Epoch 49, Iter 2000, Loss 0.6431552171707153\n",
      "Epoch 49, Iter 2100, Loss 0.571964681148529\n",
      "Epoch 49, Iter 2200, Loss 0.655823290348053\n",
      "Epoch 49, Iter 2300, Loss 0.6260906457901001\n",
      "Epoch 49, Iter 2400, Loss 0.6175416707992554\n",
      "Epoch 49, Val Loss 0.8712196350097656\n",
      "Model saved as crnn_512_1024_0.01_64_49_concat.pth\n",
      "Epoch 50, Iter 0, Loss 0.5774014592170715\n",
      "Epoch 50, Iter 100, Loss 0.5891415476799011\n",
      "Epoch 50, Iter 200, Loss 0.6402459144592285\n",
      "Epoch 50, Iter 300, Loss 0.6106683015823364\n",
      "Epoch 50, Iter 400, Loss 0.604712724685669\n",
      "Epoch 50, Iter 500, Loss 0.6665690541267395\n",
      "Epoch 50, Iter 600, Loss 0.6022569537162781\n",
      "Epoch 50, Iter 700, Loss 0.5952403545379639\n",
      "Epoch 50, Iter 800, Loss 0.6015284061431885\n",
      "Epoch 50, Iter 900, Loss 0.603678286075592\n",
      "Epoch 50, Iter 1000, Loss 0.6202082633972168\n",
      "Epoch 50, Iter 1100, Loss 0.5782018899917603\n",
      "Epoch 50, Iter 1200, Loss 0.5878488421440125\n",
      "Epoch 50, Iter 1300, Loss 0.6055759787559509\n",
      "Epoch 50, Iter 1400, Loss 0.5901128053665161\n",
      "Epoch 50, Iter 1500, Loss 0.6371358036994934\n",
      "Epoch 50, Iter 1600, Loss 0.6150968670845032\n",
      "Epoch 50, Iter 1700, Loss 0.5870086550712585\n",
      "Epoch 50, Iter 1800, Loss 0.5999261140823364\n",
      "Epoch 50, Iter 1900, Loss 0.5889449715614319\n",
      "Epoch 50, Iter 2000, Loss 0.5541319847106934\n",
      "Epoch 50, Iter 2100, Loss 0.6010231971740723\n",
      "Epoch 50, Iter 2200, Loss 0.6496518850326538\n",
      "Epoch 50, Iter 2300, Loss 0.6387799978256226\n",
      "Epoch 50, Iter 2400, Loss 0.578704833984375\n",
      "Epoch 50, Val Loss 0.8722218871116638\n",
      "Model saved as crnn_512_1024_0.01_64_50_concat.pth\n",
      "Epoch 51, Iter 0, Loss 0.5884016156196594\n",
      "Epoch 51, Iter 100, Loss 0.6457760334014893\n",
      "Epoch 51, Iter 200, Loss 0.580269992351532\n",
      "Epoch 51, Iter 300, Loss 0.6710686087608337\n",
      "Epoch 51, Iter 400, Loss 0.6209895610809326\n",
      "Epoch 51, Iter 500, Loss 0.6230250597000122\n",
      "Epoch 51, Iter 600, Loss 0.5836105942726135\n",
      "Epoch 51, Iter 700, Loss 0.6160378456115723\n",
      "Epoch 51, Iter 800, Loss 0.6258912086486816\n",
      "Epoch 51, Iter 900, Loss 0.6580175757408142\n",
      "Epoch 51, Iter 1000, Loss 0.5799620151519775\n",
      "Epoch 51, Iter 1100, Loss 0.6058825850486755\n",
      "Epoch 51, Iter 1200, Loss 0.6259145140647888\n",
      "Epoch 51, Iter 1300, Loss 0.6210405826568604\n",
      "Epoch 51, Iter 1400, Loss 0.6009535789489746\n",
      "Epoch 51, Iter 1500, Loss 0.592882513999939\n",
      "Epoch 51, Iter 1600, Loss 0.575687825679779\n",
      "Epoch 51, Iter 1700, Loss 0.5719529986381531\n",
      "Epoch 51, Iter 1800, Loss 0.6245325207710266\n",
      "Epoch 51, Iter 1900, Loss 0.6440178155899048\n",
      "Epoch 51, Iter 2000, Loss 0.6140024065971375\n",
      "Epoch 51, Iter 2100, Loss 0.6251990795135498\n",
      "Epoch 51, Iter 2200, Loss 0.5772963762283325\n",
      "Epoch 51, Iter 2300, Loss 0.6316588521003723\n",
      "Epoch 51, Iter 2400, Loss 0.5789755582809448\n",
      "Epoch 51, Val Loss 0.8728401064872742\n",
      "Model saved as crnn_512_1024_0.01_64_51_concat.pth\n",
      "Epoch 52, Iter 0, Loss 0.6012187600135803\n",
      "Epoch 52, Iter 100, Loss 0.6918476819992065\n",
      "Epoch 52, Iter 200, Loss 0.6607051491737366\n",
      "Epoch 52, Iter 300, Loss 0.5608215928077698\n",
      "Epoch 52, Iter 400, Loss 0.6412181854248047\n",
      "Epoch 52, Iter 500, Loss 0.5844241380691528\n",
      "Epoch 52, Iter 600, Loss 0.589954137802124\n",
      "Epoch 52, Iter 700, Loss 0.6183826327323914\n",
      "Epoch 52, Iter 800, Loss 0.6306717395782471\n",
      "Epoch 52, Iter 900, Loss 0.6550067067146301\n",
      "Epoch 52, Iter 1000, Loss 0.5622537732124329\n",
      "Epoch 52, Iter 1100, Loss 0.6185663938522339\n",
      "Epoch 52, Iter 1200, Loss 0.5805714726448059\n",
      "Epoch 52, Iter 1300, Loss 0.6042424440383911\n",
      "Epoch 52, Iter 1400, Loss 0.569449782371521\n",
      "Epoch 52, Iter 1500, Loss 0.5889174938201904\n",
      "Epoch 52, Iter 1600, Loss 0.6221457123756409\n",
      "Epoch 52, Iter 1700, Loss 0.5723031759262085\n",
      "Epoch 52, Iter 1800, Loss 0.6465871334075928\n",
      "Epoch 52, Iter 1900, Loss 0.5311026573181152\n",
      "Epoch 52, Iter 2000, Loss 0.6197723150253296\n",
      "Epoch 52, Iter 2100, Loss 0.6218405365943909\n",
      "Epoch 52, Iter 2200, Loss 0.579827070236206\n",
      "Epoch 52, Iter 2300, Loss 0.5847728252410889\n",
      "Epoch 52, Iter 2400, Loss 0.6439977884292603\n",
      "Epoch 52, Val Loss 0.8729938864707947\n",
      "Model saved as crnn_512_1024_0.01_64_52_concat.pth\n",
      "Epoch 53, Iter 0, Loss 0.6204767227172852\n",
      "Epoch 53, Iter 100, Loss 0.6458172798156738\n",
      "Epoch 53, Iter 200, Loss 0.5447772741317749\n",
      "Epoch 53, Iter 300, Loss 0.5982237458229065\n",
      "Epoch 53, Iter 400, Loss 0.6206738948822021\n",
      "Epoch 53, Iter 500, Loss 0.6831226348876953\n",
      "Epoch 53, Iter 600, Loss 0.6516150832176208\n",
      "Epoch 53, Iter 700, Loss 0.5735661387443542\n",
      "Epoch 53, Iter 800, Loss 0.6032463312149048\n",
      "Epoch 53, Iter 900, Loss 0.5862913131713867\n",
      "Epoch 53, Iter 1000, Loss 0.6553128957748413\n",
      "Epoch 53, Iter 1100, Loss 0.6119535565376282\n",
      "Epoch 53, Iter 1200, Loss 0.5692175030708313\n",
      "Epoch 53, Iter 1300, Loss 0.6121163368225098\n",
      "Epoch 53, Iter 1400, Loss 0.5886772274971008\n",
      "Epoch 53, Iter 1500, Loss 0.5732720494270325\n",
      "Epoch 53, Iter 1600, Loss 0.6548892259597778\n",
      "Epoch 53, Iter 1700, Loss 0.672878623008728\n",
      "Epoch 53, Iter 1800, Loss 0.5752896070480347\n",
      "Epoch 53, Iter 1900, Loss 0.6279230713844299\n",
      "Epoch 53, Iter 2000, Loss 0.5983563661575317\n",
      "Epoch 53, Iter 2100, Loss 0.6061722040176392\n",
      "Epoch 53, Iter 2200, Loss 0.6371549367904663\n",
      "Epoch 53, Iter 2300, Loss 0.6363012194633484\n",
      "Epoch 53, Iter 2400, Loss 0.5958009958267212\n",
      "Epoch 53, Val Loss 0.8721181750297546\n",
      "Model saved as crnn_512_1024_0.01_64_53_concat.pth\n",
      "Epoch 54, Iter 0, Loss 0.5727018713951111\n",
      "Epoch 54, Iter 100, Loss 0.6353585124015808\n",
      "Epoch 54, Iter 200, Loss 0.6366907954216003\n",
      "Epoch 54, Iter 300, Loss 0.6138457655906677\n",
      "Epoch 54, Iter 400, Loss 0.5728068947792053\n",
      "Epoch 54, Iter 500, Loss 0.6218632459640503\n",
      "Epoch 54, Iter 600, Loss 0.5753573775291443\n",
      "Epoch 54, Iter 700, Loss 0.601245105266571\n",
      "Epoch 54, Iter 800, Loss 0.604741096496582\n",
      "Epoch 54, Iter 900, Loss 0.6210368275642395\n",
      "Epoch 54, Iter 1000, Loss 0.6319869160652161\n",
      "Epoch 54, Iter 1100, Loss 0.5503455400466919\n",
      "Epoch 54, Iter 1200, Loss 0.5748255848884583\n",
      "Epoch 54, Iter 1300, Loss 0.6059921979904175\n",
      "Epoch 54, Iter 1400, Loss 0.611865758895874\n",
      "Epoch 54, Iter 1500, Loss 0.599842369556427\n",
      "Epoch 54, Iter 1600, Loss 0.579667866230011\n",
      "Epoch 54, Iter 1700, Loss 0.6103433966636658\n",
      "Epoch 54, Iter 1800, Loss 0.6032809615135193\n",
      "Epoch 54, Iter 1900, Loss 0.604167640209198\n",
      "Epoch 54, Iter 2000, Loss 0.5637994408607483\n",
      "Epoch 54, Iter 2100, Loss 0.5987700819969177\n",
      "Epoch 54, Iter 2200, Loss 0.6461630463600159\n",
      "Epoch 54, Iter 2300, Loss 0.608137309551239\n",
      "Epoch 54, Iter 2400, Loss 0.5707404613494873\n",
      "Epoch 54, Val Loss 0.8726342916488647\n",
      "Model saved as crnn_512_1024_0.01_64_54_concat.pth\n",
      "Epoch 55, Iter 0, Loss 0.6258745193481445\n",
      "Epoch 55, Iter 100, Loss 0.6365370750427246\n",
      "Epoch 55, Iter 200, Loss 0.6133459210395813\n",
      "Epoch 55, Iter 300, Loss 0.5821471214294434\n",
      "Epoch 55, Iter 400, Loss 0.550595223903656\n",
      "Epoch 55, Iter 500, Loss 0.6322717666625977\n",
      "Epoch 55, Iter 600, Loss 0.6303903460502625\n",
      "Epoch 55, Iter 700, Loss 0.6223883032798767\n",
      "Epoch 55, Iter 800, Loss 0.5793418288230896\n",
      "Epoch 55, Iter 900, Loss 0.5817647576332092\n",
      "Epoch 55, Iter 1000, Loss 0.6133705377578735\n",
      "Epoch 55, Iter 1100, Loss 0.5917333364486694\n",
      "Epoch 55, Iter 1200, Loss 0.5780540704727173\n",
      "Epoch 55, Iter 1300, Loss 0.650984525680542\n",
      "Epoch 55, Iter 1400, Loss 0.5975009202957153\n",
      "Epoch 55, Iter 1500, Loss 0.5930282473564148\n",
      "Epoch 55, Iter 1600, Loss 0.5795643925666809\n",
      "Epoch 55, Iter 1700, Loss 0.5666370987892151\n",
      "Epoch 55, Iter 1800, Loss 0.6377860903739929\n",
      "Epoch 55, Iter 1900, Loss 0.5814852714538574\n",
      "Epoch 55, Iter 2000, Loss 0.6092162728309631\n",
      "Epoch 55, Iter 2100, Loss 0.5823521018028259\n",
      "Epoch 55, Iter 2200, Loss 0.6360206007957458\n",
      "Epoch 55, Iter 2300, Loss 0.6565837860107422\n",
      "Epoch 55, Iter 2400, Loss 0.6117975115776062\n",
      "Epoch 55, Val Loss 0.8724472522735596\n",
      "Model saved as crnn_512_1024_0.01_64_55_concat.pth\n",
      "Epoch 56, Iter 0, Loss 0.6066493391990662\n",
      "Epoch 56, Iter 100, Loss 0.600940465927124\n",
      "Epoch 56, Iter 200, Loss 0.6024782061576843\n",
      "Epoch 56, Iter 300, Loss 0.636784553527832\n",
      "Epoch 56, Iter 400, Loss 0.5794628262519836\n",
      "Epoch 56, Iter 500, Loss 0.6310659646987915\n",
      "Epoch 56, Iter 600, Loss 0.5792917013168335\n",
      "Epoch 56, Iter 700, Loss 0.6086536049842834\n",
      "Epoch 56, Iter 800, Loss 0.6518890857696533\n",
      "Epoch 56, Iter 900, Loss 0.6584896445274353\n",
      "Epoch 56, Iter 1000, Loss 0.6468567848205566\n",
      "Epoch 56, Iter 1100, Loss 0.6128098368644714\n",
      "Epoch 56, Iter 1200, Loss 0.5554440021514893\n",
      "Epoch 56, Iter 1300, Loss 0.6130701303482056\n",
      "Epoch 56, Iter 1400, Loss 0.6170843243598938\n",
      "Epoch 56, Iter 1500, Loss 0.6453675627708435\n",
      "Epoch 56, Iter 1600, Loss 0.6173244714736938\n",
      "Epoch 56, Iter 1700, Loss 0.5855157971382141\n",
      "Epoch 56, Iter 1800, Loss 0.6529638171195984\n",
      "Epoch 56, Iter 1900, Loss 0.617891252040863\n",
      "Epoch 56, Iter 2000, Loss 0.5833262801170349\n",
      "Epoch 56, Iter 2100, Loss 0.5939081907272339\n",
      "Epoch 56, Iter 2200, Loss 0.6047210693359375\n",
      "Epoch 56, Iter 2300, Loss 0.5643260478973389\n",
      "Epoch 56, Iter 2400, Loss 0.5916716456413269\n",
      "Epoch 56, Val Loss 0.8725284934043884\n",
      "Model saved as crnn_512_1024_0.01_64_56_concat.pth\n",
      "Epoch 57, Iter 0, Loss 0.5724524855613708\n",
      "Epoch 57, Iter 100, Loss 0.6634133458137512\n",
      "Epoch 57, Iter 200, Loss 0.5747156143188477\n",
      "Epoch 57, Iter 300, Loss 0.6494723558425903\n",
      "Epoch 57, Iter 400, Loss 0.517094075679779\n",
      "Epoch 57, Iter 500, Loss 0.6320341229438782\n",
      "Epoch 57, Iter 600, Loss 0.6633892059326172\n",
      "Epoch 57, Iter 700, Loss 0.6049227118492126\n",
      "Epoch 57, Iter 800, Loss 0.6279011964797974\n",
      "Epoch 57, Iter 900, Loss 0.6305834650993347\n",
      "Epoch 57, Iter 1000, Loss 0.6307916045188904\n",
      "Epoch 57, Iter 1100, Loss 0.5844024419784546\n",
      "Epoch 57, Iter 1200, Loss 0.6540873050689697\n",
      "Epoch 57, Iter 1300, Loss 0.6231133937835693\n",
      "Epoch 57, Iter 1400, Loss 0.5872375965118408\n",
      "Epoch 57, Iter 1500, Loss 0.6135140061378479\n",
      "Epoch 57, Iter 1600, Loss 0.5803269743919373\n",
      "Epoch 57, Iter 1700, Loss 0.6136080622673035\n",
      "Epoch 57, Iter 1800, Loss 0.6349738240242004\n",
      "Epoch 57, Iter 1900, Loss 0.6222103238105774\n",
      "Epoch 57, Iter 2000, Loss 0.643567681312561\n",
      "Epoch 57, Iter 2100, Loss 0.5607289671897888\n",
      "Epoch 57, Iter 2200, Loss 0.5254799723625183\n",
      "Epoch 57, Iter 2300, Loss 0.6031730771064758\n",
      "Epoch 57, Iter 2400, Loss 0.5733826160430908\n",
      "Epoch 57, Val Loss 0.8725449442863464\n",
      "Model saved as crnn_512_1024_0.01_64_57_concat.pth\n",
      "Epoch 58, Iter 0, Loss 0.5702981352806091\n",
      "Epoch 58, Iter 100, Loss 0.575373113155365\n",
      "Epoch 58, Iter 200, Loss 0.6507154107093811\n",
      "Epoch 58, Iter 300, Loss 0.6331714391708374\n",
      "Epoch 58, Iter 400, Loss 0.6226981282234192\n",
      "Epoch 58, Iter 500, Loss 0.5704379677772522\n",
      "Epoch 58, Iter 600, Loss 0.5843510031700134\n",
      "Epoch 58, Iter 700, Loss 0.6361351609230042\n",
      "Epoch 58, Iter 800, Loss 0.5492660999298096\n",
      "Epoch 58, Iter 900, Loss 0.5779752731323242\n",
      "Epoch 58, Iter 1000, Loss 0.5575553178787231\n",
      "Epoch 58, Iter 1100, Loss 0.6660285592079163\n",
      "Epoch 58, Iter 1200, Loss 0.644879162311554\n",
      "Epoch 58, Iter 1300, Loss 0.5426698923110962\n",
      "Epoch 58, Iter 1400, Loss 0.6119109392166138\n",
      "Epoch 58, Iter 1500, Loss 0.6453703045845032\n",
      "Epoch 58, Iter 1600, Loss 0.6519030332565308\n",
      "Epoch 58, Iter 1700, Loss 0.6514024138450623\n",
      "Epoch 58, Iter 1800, Loss 0.6446384191513062\n",
      "Epoch 58, Iter 1900, Loss 0.6266117691993713\n",
      "Epoch 58, Iter 2000, Loss 0.5937063694000244\n",
      "Epoch 58, Iter 2100, Loss 0.6284762620925903\n",
      "Epoch 58, Iter 2200, Loss 0.6210544109344482\n",
      "Epoch 58, Iter 2300, Loss 0.5616089701652527\n",
      "Epoch 58, Iter 2400, Loss 0.6106966733932495\n",
      "Epoch 58, Val Loss 0.8719325661659241\n",
      "Model saved as crnn_512_1024_0.01_64_58_concat.pth\n",
      "Epoch 59, Iter 0, Loss 0.5737737417221069\n",
      "Epoch 59, Iter 100, Loss 0.5887717008590698\n",
      "Epoch 59, Iter 200, Loss 0.5626619458198547\n",
      "Epoch 59, Iter 300, Loss 0.6001107096672058\n",
      "Epoch 59, Iter 400, Loss 0.6298394799232483\n",
      "Epoch 59, Iter 500, Loss 0.5816243290901184\n",
      "Epoch 59, Iter 600, Loss 0.5134167075157166\n",
      "Epoch 59, Iter 700, Loss 0.6055133938789368\n",
      "Epoch 59, Iter 800, Loss 0.5953069925308228\n",
      "Epoch 59, Iter 900, Loss 0.6025264859199524\n",
      "Epoch 59, Iter 1000, Loss 0.6397168636322021\n",
      "Epoch 59, Iter 1100, Loss 0.5688788890838623\n",
      "Epoch 59, Iter 1200, Loss 0.5665298700332642\n",
      "Epoch 59, Iter 1300, Loss 0.5943688750267029\n",
      "Epoch 59, Iter 1400, Loss 0.6227853894233704\n",
      "Epoch 59, Iter 1500, Loss 0.6065552830696106\n",
      "Epoch 59, Iter 1600, Loss 0.6130358576774597\n",
      "Epoch 59, Iter 1700, Loss 0.5777486562728882\n",
      "Epoch 59, Iter 1800, Loss 0.6526590585708618\n",
      "Epoch 59, Iter 1900, Loss 0.6041606664657593\n",
      "Epoch 59, Iter 2000, Loss 0.5783383846282959\n",
      "Epoch 59, Iter 2100, Loss 0.6191906929016113\n",
      "Epoch 59, Iter 2200, Loss 0.6125431060791016\n",
      "Epoch 59, Iter 2300, Loss 0.6154246926307678\n",
      "Epoch 59, Iter 2400, Loss 0.5625163912773132\n",
      "Epoch 59, Val Loss 0.8724529147148132\n",
      "Model saved as crnn_512_1024_0.01_64_59_concat.pth\n",
      "Epoch 60, Iter 0, Loss 0.6230781674385071\n",
      "Epoch 60, Iter 100, Loss 0.6178346276283264\n",
      "Epoch 60, Iter 200, Loss 0.5834039449691772\n",
      "Epoch 60, Iter 300, Loss 0.6284940242767334\n",
      "Epoch 60, Iter 400, Loss 0.6147675514221191\n",
      "Epoch 60, Iter 500, Loss 0.6294771432876587\n",
      "Epoch 60, Iter 600, Loss 0.584230899810791\n",
      "Epoch 60, Iter 700, Loss 0.5909673571586609\n",
      "Epoch 60, Iter 800, Loss 0.5534276366233826\n",
      "Epoch 60, Iter 900, Loss 0.6144970655441284\n",
      "Epoch 60, Iter 1000, Loss 0.6057795286178589\n",
      "Epoch 60, Iter 1100, Loss 0.6355767846107483\n",
      "Epoch 60, Iter 1200, Loss 0.6289613842964172\n",
      "Epoch 60, Iter 1300, Loss 0.6133025884628296\n",
      "Epoch 60, Iter 1400, Loss 0.598256528377533\n",
      "Epoch 60, Iter 1500, Loss 0.5758920311927795\n",
      "Epoch 60, Iter 1600, Loss 0.5652337074279785\n",
      "Epoch 60, Iter 1700, Loss 0.6574766635894775\n",
      "Epoch 60, Iter 1800, Loss 0.6145654916763306\n",
      "Epoch 60, Iter 1900, Loss 0.6160925626754761\n",
      "Epoch 60, Iter 2000, Loss 0.6148359179496765\n",
      "Epoch 60, Iter 2100, Loss 0.6089473366737366\n",
      "Epoch 60, Iter 2200, Loss 0.6159633994102478\n",
      "Epoch 60, Iter 2300, Loss 0.6150697469711304\n",
      "Epoch 60, Iter 2400, Loss 0.6217430233955383\n",
      "Epoch 60, Val Loss 0.8723470568656921\n",
      "Model saved as crnn_512_1024_0.01_64_60_concat.pth\n",
      "Epoch 61, Iter 0, Loss 0.6797369718551636\n",
      "Epoch 61, Iter 100, Loss 0.563167154788971\n",
      "Epoch 61, Iter 200, Loss 0.6029970645904541\n",
      "Epoch 61, Iter 300, Loss 0.5814471244812012\n",
      "Epoch 61, Iter 400, Loss 0.5832394957542419\n",
      "Epoch 61, Iter 500, Loss 0.606696367263794\n",
      "Epoch 61, Iter 600, Loss 0.5857207179069519\n",
      "Epoch 61, Iter 700, Loss 0.6483662724494934\n",
      "Epoch 61, Iter 800, Loss 0.5871033668518066\n",
      "Epoch 61, Iter 900, Loss 0.6507872939109802\n",
      "Epoch 61, Iter 1000, Loss 0.587665319442749\n",
      "Epoch 61, Iter 1100, Loss 0.5947136878967285\n",
      "Epoch 61, Iter 1200, Loss 0.5811958312988281\n",
      "Epoch 61, Iter 1300, Loss 0.6461426615715027\n",
      "Epoch 61, Iter 1400, Loss 0.6555374264717102\n",
      "Epoch 61, Iter 1500, Loss 0.6478327512741089\n",
      "Epoch 61, Iter 1600, Loss 0.6148929595947266\n",
      "Epoch 61, Iter 1700, Loss 0.607490062713623\n",
      "Epoch 61, Iter 1800, Loss 0.5997061729431152\n",
      "Epoch 61, Iter 1900, Loss 0.6368361711502075\n",
      "Epoch 61, Iter 2000, Loss 0.647373378276825\n",
      "Epoch 61, Iter 2100, Loss 0.5805526971817017\n",
      "Epoch 61, Iter 2200, Loss 0.6065094470977783\n",
      "Epoch 61, Iter 2300, Loss 0.6118335127830505\n",
      "Epoch 61, Iter 2400, Loss 0.6632732152938843\n",
      "Epoch 61, Val Loss 0.8724074959754944\n",
      "Model saved as crnn_512_1024_0.01_64_61_concat.pth\n",
      "Epoch 62, Iter 0, Loss 0.6090620160102844\n",
      "Epoch 62, Iter 100, Loss 0.5718812942504883\n",
      "Epoch 62, Iter 200, Loss 0.6193211078643799\n",
      "Epoch 62, Iter 300, Loss 0.5994902849197388\n",
      "Epoch 62, Iter 400, Loss 0.6091156005859375\n",
      "Epoch 62, Iter 500, Loss 0.6016939282417297\n",
      "Epoch 62, Iter 600, Loss 0.6040810346603394\n",
      "Epoch 62, Iter 700, Loss 0.5623581409454346\n",
      "Epoch 62, Iter 800, Loss 0.5871628522872925\n",
      "Epoch 62, Iter 900, Loss 0.6052755117416382\n",
      "Epoch 62, Iter 1000, Loss 0.5945510864257812\n",
      "Epoch 62, Iter 1100, Loss 0.6497738361358643\n",
      "Epoch 62, Iter 1200, Loss 0.6095795035362244\n",
      "Epoch 62, Iter 1300, Loss 0.639481782913208\n",
      "Epoch 62, Iter 1400, Loss 0.578318178653717\n",
      "Epoch 62, Iter 1500, Loss 0.5980005860328674\n",
      "Epoch 62, Iter 1600, Loss 0.5573875308036804\n",
      "Epoch 62, Iter 1700, Loss 0.5957558155059814\n",
      "Epoch 62, Iter 1800, Loss 0.6394573450088501\n",
      "Epoch 62, Iter 1900, Loss 0.5881725549697876\n",
      "Epoch 62, Iter 2000, Loss 0.6162420511245728\n",
      "Epoch 62, Iter 2100, Loss 0.600543200969696\n",
      "Epoch 62, Iter 2200, Loss 0.589853048324585\n",
      "Epoch 62, Iter 2300, Loss 0.5763607621192932\n",
      "Epoch 62, Iter 2400, Loss 0.5914410352706909\n",
      "Epoch 62, Val Loss 0.8724538087844849\n",
      "Model saved as crnn_512_1024_0.01_64_62_concat.pth\n",
      "Epoch 63, Iter 0, Loss 0.5886731147766113\n",
      "Epoch 63, Iter 100, Loss 0.6551565527915955\n",
      "Epoch 63, Iter 200, Loss 0.5915932059288025\n",
      "Epoch 63, Iter 300, Loss 0.5995917916297913\n",
      "Epoch 63, Iter 400, Loss 0.6860500574111938\n",
      "Epoch 63, Iter 500, Loss 0.5441826581954956\n",
      "Epoch 63, Iter 600, Loss 0.5871252417564392\n",
      "Epoch 63, Iter 700, Loss 0.5908445715904236\n",
      "Epoch 63, Iter 800, Loss 0.6381644010543823\n",
      "Epoch 63, Iter 900, Loss 0.6661748290061951\n",
      "Epoch 63, Iter 1000, Loss 0.6056686639785767\n",
      "Epoch 63, Iter 1100, Loss 0.6099298596382141\n",
      "Epoch 63, Iter 1200, Loss 0.5670555830001831\n",
      "Epoch 63, Iter 1300, Loss 0.5549013614654541\n",
      "Epoch 63, Iter 1400, Loss 0.5823654532432556\n",
      "Epoch 63, Iter 1500, Loss 0.6368093490600586\n",
      "Epoch 63, Iter 1600, Loss 0.5874731540679932\n",
      "Epoch 63, Iter 1700, Loss 0.5890166163444519\n",
      "Epoch 63, Iter 1800, Loss 0.6255084276199341\n",
      "Epoch 63, Iter 1900, Loss 0.6488829851150513\n",
      "Epoch 63, Iter 2000, Loss 0.5954771041870117\n",
      "Epoch 63, Iter 2100, Loss 0.6056320667266846\n",
      "Epoch 63, Iter 2200, Loss 0.6031298637390137\n",
      "Epoch 63, Iter 2300, Loss 0.6000962853431702\n",
      "Epoch 63, Iter 2400, Loss 0.5451847910881042\n",
      "Epoch 63, Val Loss 0.872718095779419\n",
      "Model saved as crnn_512_1024_0.01_64_63_concat.pth\n",
      "Epoch 64, Iter 0, Loss 0.6485282778739929\n",
      "Epoch 64, Iter 100, Loss 0.6233070492744446\n",
      "Epoch 64, Iter 200, Loss 0.6547790169715881\n",
      "Epoch 64, Iter 300, Loss 0.6011410355567932\n",
      "Epoch 64, Iter 400, Loss 0.6023083925247192\n",
      "Epoch 64, Iter 500, Loss 0.579356849193573\n",
      "Epoch 64, Iter 600, Loss 0.6554720401763916\n",
      "Epoch 64, Iter 700, Loss 0.5912888646125793\n",
      "Epoch 64, Iter 800, Loss 0.6348705291748047\n",
      "Epoch 64, Iter 900, Loss 0.5971629023551941\n",
      "Epoch 64, Iter 1000, Loss 0.5892454385757446\n",
      "Epoch 64, Iter 1100, Loss 0.5850359201431274\n",
      "Epoch 64, Iter 1200, Loss 0.6209750175476074\n",
      "Epoch 64, Iter 1300, Loss 0.5602311491966248\n",
      "Epoch 64, Iter 1400, Loss 0.6210018396377563\n",
      "Epoch 64, Iter 1500, Loss 0.6082751750946045\n",
      "Epoch 64, Iter 1600, Loss 0.6054068803787231\n",
      "Epoch 64, Iter 1700, Loss 0.6017861366271973\n",
      "Epoch 64, Iter 1800, Loss 0.578709602355957\n",
      "Epoch 64, Iter 1900, Loss 0.603358268737793\n",
      "Epoch 64, Iter 2000, Loss 0.6149224042892456\n",
      "Epoch 64, Iter 2100, Loss 0.6178663372993469\n",
      "Epoch 64, Iter 2200, Loss 0.60355544090271\n",
      "Epoch 64, Iter 2300, Loss 0.5728173851966858\n",
      "Epoch 64, Iter 2400, Loss 0.6377719640731812\n",
      "Epoch 64, Val Loss 0.8729979991912842\n",
      "Model saved as crnn_512_1024_0.01_64_64_concat.pth\n",
      "Epoch 65, Iter 0, Loss 0.571992039680481\n",
      "Epoch 65, Iter 100, Loss 0.6020350456237793\n",
      "Epoch 65, Iter 200, Loss 0.5742921829223633\n",
      "Epoch 65, Iter 300, Loss 0.5491646528244019\n",
      "Epoch 65, Iter 400, Loss 0.6357535123825073\n",
      "Epoch 65, Iter 500, Loss 0.612572431564331\n",
      "Epoch 65, Iter 600, Loss 0.6826211214065552\n",
      "Epoch 65, Iter 700, Loss 0.5849164128303528\n",
      "Epoch 65, Iter 800, Loss 0.5726764798164368\n",
      "Epoch 65, Iter 900, Loss 0.601479709148407\n",
      "Epoch 65, Iter 1000, Loss 0.621783435344696\n",
      "Epoch 65, Iter 1100, Loss 0.6177588701248169\n",
      "Epoch 65, Iter 1200, Loss 0.6106346845626831\n",
      "Epoch 65, Iter 1300, Loss 0.591015636920929\n",
      "Epoch 65, Iter 1400, Loss 0.5809832215309143\n",
      "Epoch 65, Iter 1500, Loss 0.5586257576942444\n",
      "Epoch 65, Iter 1600, Loss 0.5774043798446655\n",
      "Epoch 65, Iter 1700, Loss 0.6587814092636108\n",
      "Epoch 65, Iter 1800, Loss 0.6500466465950012\n",
      "Epoch 65, Iter 1900, Loss 0.6402025818824768\n",
      "Epoch 65, Iter 2000, Loss 0.6069247126579285\n",
      "Epoch 65, Iter 2100, Loss 0.5962536334991455\n",
      "Epoch 65, Iter 2200, Loss 0.6142758727073669\n",
      "Epoch 65, Iter 2300, Loss 0.593034029006958\n",
      "Epoch 65, Iter 2400, Loss 0.6029519438743591\n",
      "Epoch 65, Val Loss 0.8727277517318726\n",
      "Model saved as crnn_512_1024_0.01_64_65_concat.pth\n",
      "Epoch 66, Iter 0, Loss 0.6099974513053894\n",
      "Epoch 66, Iter 100, Loss 0.6270437240600586\n",
      "Epoch 66, Iter 200, Loss 0.6211576461791992\n",
      "Epoch 66, Iter 300, Loss 0.6260334849357605\n",
      "Epoch 66, Iter 400, Loss 0.6287881135940552\n",
      "Epoch 66, Iter 500, Loss 0.5722723007202148\n",
      "Epoch 66, Iter 600, Loss 0.5385969281196594\n",
      "Epoch 66, Iter 700, Loss 0.5934318900108337\n",
      "Epoch 66, Iter 800, Loss 0.5849299430847168\n",
      "Epoch 66, Iter 900, Loss 0.5735723376274109\n",
      "Epoch 66, Iter 1000, Loss 0.6163281202316284\n",
      "Epoch 66, Iter 1100, Loss 0.6238555312156677\n",
      "Epoch 66, Iter 1200, Loss 0.5943586230278015\n",
      "Epoch 66, Iter 1300, Loss 0.6375792622566223\n",
      "Epoch 66, Iter 1400, Loss 0.6407169103622437\n",
      "Epoch 66, Iter 1500, Loss 0.6550505757331848\n",
      "Epoch 66, Iter 1600, Loss 0.6373459696769714\n",
      "Epoch 66, Iter 1700, Loss 0.6037170886993408\n",
      "Epoch 66, Iter 1800, Loss 0.6085535883903503\n",
      "Epoch 66, Iter 1900, Loss 0.6043371558189392\n",
      "Epoch 66, Iter 2000, Loss 0.6007577776908875\n",
      "Epoch 66, Iter 2100, Loss 0.5587599873542786\n",
      "Epoch 66, Iter 2200, Loss 0.6189485192298889\n",
      "Epoch 66, Iter 2300, Loss 0.6030033230781555\n",
      "Epoch 66, Iter 2400, Loss 0.5950071215629578\n",
      "Epoch 66, Val Loss 0.8719195127487183\n",
      "Model saved as crnn_512_1024_0.01_64_66_concat.pth\n",
      "Epoch 67, Iter 0, Loss 0.5311479568481445\n",
      "Epoch 67, Iter 100, Loss 0.5460140109062195\n",
      "Epoch 67, Iter 200, Loss 0.6229982972145081\n",
      "Epoch 67, Iter 300, Loss 0.5767118334770203\n",
      "Epoch 67, Iter 400, Loss 0.6326350569725037\n",
      "Epoch 67, Iter 500, Loss 0.5581579208374023\n",
      "Epoch 67, Iter 600, Loss 0.6460985541343689\n",
      "Epoch 67, Iter 700, Loss 0.5864185094833374\n",
      "Epoch 67, Iter 800, Loss 0.6029005646705627\n",
      "Epoch 67, Iter 900, Loss 0.5915428400039673\n",
      "Epoch 67, Iter 1000, Loss 0.6322818398475647\n",
      "Epoch 67, Iter 1100, Loss 0.6209427714347839\n",
      "Epoch 67, Iter 1200, Loss 0.5652197599411011\n",
      "Epoch 67, Iter 1300, Loss 0.5808331370353699\n",
      "Epoch 67, Iter 1400, Loss 0.6500478982925415\n",
      "Epoch 67, Iter 1500, Loss 0.6390454173088074\n",
      "Epoch 67, Iter 1600, Loss 0.5510365962982178\n",
      "Epoch 67, Iter 1700, Loss 0.6294650435447693\n",
      "Epoch 67, Iter 1800, Loss 0.6193954944610596\n",
      "Epoch 67, Iter 1900, Loss 0.6137990355491638\n",
      "Epoch 67, Iter 2000, Loss 0.5755797624588013\n",
      "Epoch 67, Iter 2100, Loss 0.602173924446106\n",
      "Epoch 67, Iter 2200, Loss 0.5719954371452332\n",
      "Epoch 67, Iter 2300, Loss 0.5742308497428894\n",
      "Epoch 67, Iter 2400, Loss 0.6012216210365295\n",
      "Epoch 67, Val Loss 0.872755229473114\n",
      "Model saved as crnn_512_1024_0.01_64_67_concat.pth\n",
      "Epoch 68, Iter 0, Loss 0.615404486656189\n",
      "Epoch 68, Iter 100, Loss 0.657490611076355\n",
      "Epoch 68, Iter 200, Loss 0.6260352730751038\n",
      "Epoch 68, Iter 300, Loss 0.6077125668525696\n",
      "Epoch 68, Iter 400, Loss 0.634690523147583\n",
      "Epoch 68, Iter 500, Loss 0.6401723623275757\n",
      "Epoch 68, Iter 600, Loss 0.6452721357345581\n",
      "Epoch 68, Iter 700, Loss 0.5619716644287109\n",
      "Epoch 68, Iter 800, Loss 0.6267064809799194\n",
      "Epoch 68, Iter 900, Loss 0.5974269509315491\n",
      "Epoch 68, Iter 1000, Loss 0.6426903605461121\n",
      "Epoch 68, Iter 1100, Loss 0.6052965521812439\n",
      "Epoch 68, Iter 1200, Loss 0.6042687296867371\n",
      "Epoch 68, Iter 1300, Loss 0.6100569367408752\n",
      "Epoch 68, Iter 1400, Loss 0.590006947517395\n",
      "Epoch 68, Iter 1500, Loss 0.6228643655776978\n",
      "Epoch 68, Iter 1600, Loss 0.6061833500862122\n",
      "Epoch 68, Iter 1700, Loss 0.6070051789283752\n",
      "Epoch 68, Iter 1800, Loss 0.6101701259613037\n",
      "Epoch 68, Iter 1900, Loss 0.6415438055992126\n",
      "Epoch 68, Iter 2000, Loss 0.6091703772544861\n",
      "Epoch 68, Iter 2100, Loss 0.5774097442626953\n",
      "Epoch 68, Iter 2200, Loss 0.6408094763755798\n",
      "Epoch 68, Iter 2300, Loss 0.6699009537696838\n",
      "Epoch 68, Iter 2400, Loss 0.634514331817627\n",
      "Epoch 68, Val Loss 0.8732681274414062\n",
      "Model saved as crnn_512_1024_0.01_64_68_concat.pth\n",
      "Epoch 69, Iter 0, Loss 0.6317541003227234\n",
      "Epoch 69, Iter 100, Loss 0.6153030395507812\n",
      "Epoch 69, Iter 200, Loss 0.6361093521118164\n",
      "Epoch 69, Iter 300, Loss 0.6319450736045837\n",
      "Epoch 69, Iter 400, Loss 0.6396957039833069\n",
      "Epoch 69, Iter 500, Loss 0.5833378434181213\n",
      "Epoch 69, Iter 600, Loss 0.5626057982444763\n",
      "Epoch 69, Iter 700, Loss 0.6145787239074707\n",
      "Epoch 69, Iter 800, Loss 0.6223456263542175\n",
      "Epoch 69, Iter 900, Loss 0.650420069694519\n",
      "Epoch 69, Iter 1000, Loss 0.6733095049858093\n",
      "Epoch 69, Iter 1100, Loss 0.5949123501777649\n",
      "Epoch 69, Iter 1200, Loss 0.6294245719909668\n",
      "Epoch 69, Iter 1300, Loss 0.5776629447937012\n",
      "Epoch 69, Iter 1400, Loss 0.5902354717254639\n",
      "Epoch 69, Iter 1500, Loss 0.5845875144004822\n",
      "Epoch 69, Iter 1600, Loss 0.5795875787734985\n",
      "Epoch 69, Iter 1700, Loss 0.5927695035934448\n",
      "Epoch 69, Iter 1800, Loss 0.6290520429611206\n",
      "Epoch 69, Iter 1900, Loss 0.5961817502975464\n",
      "Epoch 69, Iter 2000, Loss 0.667229175567627\n",
      "Epoch 69, Iter 2100, Loss 0.6519734263420105\n",
      "Epoch 69, Iter 2200, Loss 0.6270830631256104\n",
      "Epoch 69, Iter 2300, Loss 0.6358662843704224\n",
      "Epoch 69, Iter 2400, Loss 0.5846527814865112\n",
      "Epoch 69, Val Loss 0.8720019459724426\n",
      "Model saved as crnn_512_1024_0.01_64_69_concat.pth\n",
      "Epoch 70, Iter 0, Loss 0.5907999873161316\n",
      "Epoch 70, Iter 100, Loss 0.5739355087280273\n",
      "Epoch 70, Iter 200, Loss 0.6163548827171326\n",
      "Epoch 70, Iter 300, Loss 0.5720117688179016\n",
      "Epoch 70, Iter 400, Loss 0.6140730381011963\n",
      "Epoch 70, Iter 500, Loss 0.6012123823165894\n",
      "Epoch 70, Iter 600, Loss 0.5998252630233765\n",
      "Epoch 70, Iter 700, Loss 0.5613986253738403\n",
      "Epoch 70, Iter 800, Loss 0.6681050658226013\n",
      "Epoch 70, Iter 900, Loss 0.5743622779846191\n",
      "Epoch 70, Iter 1000, Loss 0.5866569876670837\n",
      "Epoch 70, Iter 1100, Loss 0.6638252139091492\n",
      "Epoch 70, Iter 1200, Loss 0.5757752060890198\n",
      "Epoch 70, Iter 1300, Loss 0.6043494939804077\n",
      "Epoch 70, Iter 1400, Loss 0.5889490842819214\n",
      "Epoch 70, Iter 1500, Loss 0.6205378770828247\n",
      "Epoch 70, Iter 1600, Loss 0.5850567817687988\n",
      "Epoch 70, Iter 1700, Loss 0.6413752436637878\n",
      "Epoch 70, Iter 1800, Loss 0.6136284470558167\n",
      "Epoch 70, Iter 1900, Loss 0.6044133305549622\n",
      "Epoch 70, Iter 2000, Loss 0.6274963617324829\n",
      "Epoch 70, Iter 2100, Loss 0.5917980670928955\n",
      "Epoch 70, Iter 2200, Loss 0.5692734718322754\n",
      "Epoch 70, Iter 2300, Loss 0.5787094831466675\n",
      "Epoch 70, Iter 2400, Loss 0.6332412362098694\n",
      "Epoch 70, Val Loss 0.8720783591270447\n",
      "Model saved as crnn_512_1024_0.01_64_70_concat.pth\n",
      "Epoch 71, Iter 0, Loss 0.6240513324737549\n",
      "Epoch 71, Iter 100, Loss 0.5502923727035522\n",
      "Epoch 71, Iter 200, Loss 0.6021848917007446\n",
      "Epoch 71, Iter 300, Loss 0.6341633796691895\n",
      "Epoch 71, Iter 400, Loss 0.6253950595855713\n",
      "Epoch 71, Iter 500, Loss 0.5983102321624756\n",
      "Epoch 71, Iter 600, Loss 0.6134128570556641\n",
      "Epoch 71, Iter 700, Loss 0.6398857235908508\n",
      "Epoch 71, Iter 800, Loss 0.583655834197998\n",
      "Epoch 71, Iter 900, Loss 0.5629270672798157\n",
      "Epoch 71, Iter 1000, Loss 0.596627950668335\n",
      "Epoch 71, Iter 1100, Loss 0.6324078440666199\n",
      "Epoch 71, Iter 1200, Loss 0.6485708355903625\n",
      "Epoch 71, Iter 1300, Loss 0.646879255771637\n",
      "Epoch 71, Iter 1400, Loss 0.6522321105003357\n",
      "Epoch 71, Iter 1500, Loss 0.5979399085044861\n",
      "Epoch 71, Iter 1600, Loss 0.6415969729423523\n",
      "Epoch 71, Iter 1700, Loss 0.6116832494735718\n",
      "Epoch 71, Iter 1800, Loss 0.5876688361167908\n",
      "Epoch 71, Iter 1900, Loss 0.5486428737640381\n",
      "Epoch 71, Iter 2000, Loss 0.6009157299995422\n",
      "Epoch 71, Iter 2100, Loss 0.6284677982330322\n",
      "Epoch 71, Iter 2200, Loss 0.6330751180648804\n",
      "Epoch 71, Iter 2300, Loss 0.5922482013702393\n",
      "Epoch 71, Iter 2400, Loss 0.6031407713890076\n",
      "Epoch 71, Val Loss 0.8723781108856201\n",
      "Model saved as crnn_512_1024_0.01_64_71_concat.pth\n",
      "Epoch 72, Iter 0, Loss 0.5707924365997314\n",
      "Epoch 72, Iter 100, Loss 0.5727802515029907\n",
      "Epoch 72, Iter 200, Loss 0.661551296710968\n",
      "Epoch 72, Iter 300, Loss 0.5968003869056702\n",
      "Epoch 72, Iter 400, Loss 0.596157431602478\n",
      "Epoch 72, Iter 500, Loss 0.6048938632011414\n",
      "Epoch 72, Iter 600, Loss 0.5703017115592957\n",
      "Epoch 72, Iter 700, Loss 0.6622856855392456\n",
      "Epoch 72, Iter 800, Loss 0.5666221380233765\n",
      "Epoch 72, Iter 900, Loss 0.5762733221054077\n",
      "Epoch 72, Iter 1000, Loss 0.6113875508308411\n",
      "Epoch 72, Iter 1100, Loss 0.6116976737976074\n",
      "Epoch 72, Iter 1200, Loss 0.5891832113265991\n",
      "Epoch 72, Iter 1300, Loss 0.6642318367958069\n",
      "Epoch 72, Iter 1400, Loss 0.6025824546813965\n",
      "Epoch 72, Iter 1500, Loss 0.609491765499115\n",
      "Epoch 72, Iter 1600, Loss 0.5914605855941772\n",
      "Epoch 72, Iter 1700, Loss 0.6436126828193665\n",
      "Epoch 72, Iter 1800, Loss 0.5873444080352783\n",
      "Epoch 72, Iter 1900, Loss 0.5618936419487\n",
      "Epoch 72, Iter 2000, Loss 0.5649358630180359\n",
      "Epoch 72, Iter 2100, Loss 0.6011017560958862\n",
      "Epoch 72, Iter 2200, Loss 0.5915618538856506\n",
      "Epoch 72, Iter 2300, Loss 0.6164805293083191\n",
      "Epoch 72, Iter 2400, Loss 0.6652026176452637\n",
      "Epoch 72, Val Loss 0.8721530437469482\n",
      "Model saved as crnn_512_1024_0.01_64_72_concat.pth\n",
      "Epoch 73, Iter 0, Loss 0.6348152756690979\n",
      "Epoch 73, Iter 100, Loss 0.6102715730667114\n",
      "Epoch 73, Iter 200, Loss 0.6088865399360657\n",
      "Epoch 73, Iter 300, Loss 0.6025856137275696\n",
      "Epoch 73, Iter 400, Loss 0.5872904658317566\n",
      "Epoch 73, Iter 500, Loss 0.5616287589073181\n",
      "Epoch 73, Iter 600, Loss 0.5873371362686157\n",
      "Epoch 73, Iter 700, Loss 0.6014876961708069\n",
      "Epoch 73, Iter 800, Loss 0.6627002358436584\n",
      "Epoch 73, Iter 900, Loss 0.6710659861564636\n",
      "Epoch 73, Iter 1000, Loss 0.5639039278030396\n",
      "Epoch 73, Iter 1100, Loss 0.6250773072242737\n",
      "Epoch 73, Iter 1200, Loss 0.6147847771644592\n",
      "Epoch 73, Iter 1300, Loss 0.5750271677970886\n",
      "Epoch 73, Iter 1400, Loss 0.5512151718139648\n",
      "Epoch 73, Iter 1500, Loss 0.5863617062568665\n",
      "Epoch 73, Iter 1600, Loss 0.626864492893219\n",
      "Epoch 73, Iter 1700, Loss 0.6412116289138794\n",
      "Epoch 73, Iter 1800, Loss 0.6325970888137817\n",
      "Epoch 73, Iter 1900, Loss 0.6700140237808228\n",
      "Epoch 73, Iter 2000, Loss 0.5469100475311279\n",
      "Epoch 73, Iter 2100, Loss 0.6434063911437988\n",
      "Epoch 73, Iter 2200, Loss 0.6102297306060791\n",
      "Epoch 73, Iter 2300, Loss 0.595710277557373\n",
      "Epoch 73, Iter 2400, Loss 0.6098721027374268\n",
      "Epoch 73, Val Loss 0.8729705810546875\n",
      "Model saved as crnn_512_1024_0.01_64_73_concat.pth\n",
      "Epoch 74, Iter 0, Loss 0.5913007259368896\n",
      "Epoch 74, Iter 100, Loss 0.6160675287246704\n",
      "Epoch 74, Iter 200, Loss 0.6159170866012573\n",
      "Epoch 74, Iter 300, Loss 0.6051811575889587\n",
      "Epoch 74, Iter 400, Loss 0.5585312247276306\n",
      "Epoch 74, Iter 500, Loss 0.6194274425506592\n",
      "Epoch 74, Iter 600, Loss 0.5760613679885864\n",
      "Epoch 74, Iter 700, Loss 0.6084142327308655\n",
      "Epoch 74, Iter 800, Loss 0.6129484176635742\n",
      "Epoch 74, Iter 900, Loss 0.6005105376243591\n",
      "Epoch 74, Iter 1000, Loss 0.598218560218811\n",
      "Epoch 74, Iter 1100, Loss 0.5247865915298462\n",
      "Epoch 74, Iter 1200, Loss 0.5729295611381531\n",
      "Epoch 74, Iter 1300, Loss 0.6417185068130493\n",
      "Epoch 74, Iter 1400, Loss 0.6710001826286316\n",
      "Epoch 74, Iter 1500, Loss 0.5925648808479309\n",
      "Epoch 74, Iter 1600, Loss 0.5859507322311401\n",
      "Epoch 74, Iter 1700, Loss 0.5773505568504333\n",
      "Epoch 74, Iter 1800, Loss 0.5644039511680603\n",
      "Epoch 74, Iter 1900, Loss 0.5918092131614685\n",
      "Epoch 74, Iter 2000, Loss 0.6251732707023621\n",
      "Epoch 74, Iter 2100, Loss 0.6242669820785522\n",
      "Epoch 74, Iter 2200, Loss 0.6597050428390503\n",
      "Epoch 74, Iter 2300, Loss 0.6589530110359192\n",
      "Epoch 74, Iter 2400, Loss 0.6497939825057983\n",
      "Epoch 74, Val Loss 0.8732038736343384\n",
      "Model saved as crnn_512_1024_0.01_64_74_concat.pth\n",
      "Epoch 75, Iter 0, Loss 0.6213177442550659\n",
      "Epoch 75, Iter 100, Loss 0.6101125478744507\n",
      "Epoch 75, Iter 200, Loss 0.5958591103553772\n",
      "Epoch 75, Iter 300, Loss 0.5519375205039978\n",
      "Epoch 75, Iter 400, Loss 0.5811904668807983\n",
      "Epoch 75, Iter 500, Loss 0.5949139595031738\n",
      "Epoch 75, Iter 600, Loss 0.6243087649345398\n",
      "Epoch 75, Iter 700, Loss 0.6553858518600464\n",
      "Epoch 75, Iter 800, Loss 0.6060408353805542\n",
      "Epoch 75, Iter 900, Loss 0.590872049331665\n",
      "Epoch 75, Iter 1000, Loss 0.5809334516525269\n",
      "Epoch 75, Iter 1100, Loss 0.6448702216148376\n",
      "Epoch 75, Iter 1200, Loss 0.5447850227355957\n",
      "Epoch 75, Iter 1300, Loss 0.6160392165184021\n",
      "Epoch 75, Iter 1400, Loss 0.617428183555603\n",
      "Epoch 75, Iter 1500, Loss 0.5961570143699646\n",
      "Epoch 75, Iter 1600, Loss 0.6092852354049683\n",
      "Epoch 75, Iter 1700, Loss 0.6407839059829712\n",
      "Epoch 75, Iter 1800, Loss 0.6058588624000549\n",
      "Epoch 75, Iter 1900, Loss 0.6435611248016357\n",
      "Epoch 75, Iter 2000, Loss 0.6099843978881836\n",
      "Epoch 75, Iter 2100, Loss 0.6006293296813965\n",
      "Epoch 75, Iter 2200, Loss 0.5994383692741394\n",
      "Epoch 75, Iter 2300, Loss 0.5842085480690002\n",
      "Epoch 75, Iter 2400, Loss 0.595649003982544\n",
      "Epoch 75, Val Loss 0.8726763725280762\n",
      "Model saved as crnn_512_1024_0.01_64_75_concat.pth\n",
      "Epoch 76, Iter 0, Loss 0.6287209987640381\n",
      "Epoch 76, Iter 100, Loss 0.576347291469574\n",
      "Epoch 76, Iter 200, Loss 0.6300866603851318\n",
      "Epoch 76, Iter 300, Loss 0.5982105731964111\n",
      "Epoch 76, Iter 400, Loss 0.6115871667861938\n",
      "Epoch 76, Iter 500, Loss 0.6541422605514526\n",
      "Epoch 76, Iter 600, Loss 0.6025074124336243\n",
      "Epoch 76, Iter 700, Loss 0.6363956928253174\n",
      "Epoch 76, Iter 800, Loss 0.5602367520332336\n",
      "Epoch 76, Iter 900, Loss 0.6163944602012634\n",
      "Epoch 76, Iter 1000, Loss 0.5820616483688354\n",
      "Epoch 76, Iter 1100, Loss 0.5903087854385376\n",
      "Epoch 76, Iter 1200, Loss 0.6273232102394104\n",
      "Epoch 76, Iter 1300, Loss 0.6384418606758118\n",
      "Epoch 76, Iter 1400, Loss 0.5551673173904419\n",
      "Epoch 76, Iter 1500, Loss 0.6161629557609558\n",
      "Epoch 76, Iter 1600, Loss 0.5834799408912659\n",
      "Epoch 76, Iter 1700, Loss 0.6149827837944031\n",
      "Epoch 76, Iter 1800, Loss 0.6354870796203613\n",
      "Epoch 76, Iter 1900, Loss 0.5846189260482788\n",
      "Epoch 76, Iter 2000, Loss 0.595573365688324\n",
      "Epoch 76, Iter 2100, Loss 0.6273078918457031\n",
      "Epoch 76, Iter 2200, Loss 0.6242889761924744\n",
      "Epoch 76, Iter 2300, Loss 0.6319518685340881\n",
      "Epoch 76, Iter 2400, Loss 0.6526930332183838\n",
      "Epoch 76, Val Loss 0.8730238080024719\n",
      "Model saved as crnn_512_1024_0.01_64_76_concat.pth\n",
      "Epoch 77, Iter 0, Loss 0.5751923322677612\n",
      "Epoch 77, Iter 100, Loss 0.6133114099502563\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 91\u001b[0m\n\u001b[1;32m     89\u001b[0m dataset \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mConcatDataset([dataset_IAM, dataset_CVL])\n\u001b[1;32m     90\u001b[0m dataloader \u001b[38;5;241m=\u001b[39m DataLoader(dataset, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m64\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, num_workers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m16\u001b[39m)\n\u001b[0;32m---> 91\u001b[0m \u001b[43mtrain_crnn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.01\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m500\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcuda:0\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[7], line 62\u001b[0m, in \u001b[0;36mtrain_crnn\u001b[0;34m(model, dataloader, learning_rate, epochs, device, start_epoch)\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;66;03m# mask = (target.argmax(-1) != 0).view(-1)\u001b[39;00m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;66;03m# loss = loss[mask].mean()\u001b[39;00m\n\u001b[1;32m     61\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m---> 62\u001b[0m \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m step \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m100\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     64\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Iter \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Loss \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss\u001b[38;5;241m.\u001b[39mitem()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/torch/optim/optimizer.py:391\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    386\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    387\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    388\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    389\u001b[0m             )\n\u001b[0;32m--> 391\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    392\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    394\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/torch/optim/optimizer.py:76\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     74\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     75\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n\u001b[0;32m---> 76\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     78\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/torch/optim/sgd.py:80\u001b[0m, in \u001b[0;36mSGD.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m     76\u001b[0m momentum_buffer_list \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     78\u001b[0m has_sparse_grad \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_group(group, params_with_grad, d_p_list, momentum_buffer_list)\n\u001b[0;32m---> 80\u001b[0m \u001b[43msgd\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     81\u001b[0m \u001b[43m    \u001b[49m\u001b[43md_p_list\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     82\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmomentum_buffer_list\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     83\u001b[0m \u001b[43m    \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mweight_decay\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     84\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmomentum\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmomentum\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     85\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     86\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdampening\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdampening\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     87\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnesterov\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mnesterov\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     88\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmaximize\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     89\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhas_sparse_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_sparse_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     90\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforeach\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mforeach\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     91\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfused\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfused\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     92\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgrad_scale\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     93\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfound_inf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;66;03m# update momentum_buffers in state\u001b[39;00m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m p, momentum_buffer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(params_with_grad, momentum_buffer_list):\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/torch/optim/sgd.py:224\u001b[0m, in \u001b[0;36msgd\u001b[0;34m(params, d_p_list, momentum_buffer_list, has_sparse_grad, foreach, fused, grad_scale, found_inf, weight_decay, momentum, lr, dampening, nesterov, maximize)\u001b[0m\n\u001b[1;32m    220\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m foreach \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m fused \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    221\u001b[0m     \u001b[38;5;66;03m# why must we be explicit about an if statement for torch.jit.is_scripting here?\u001b[39;00m\n\u001b[1;32m    222\u001b[0m     \u001b[38;5;66;03m# because JIT can't handle Optionals nor fancy conditionals when scripting\u001b[39;00m\n\u001b[1;32m    223\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39mis_scripting():\n\u001b[0;32m--> 224\u001b[0m         fused, foreach \u001b[38;5;241m=\u001b[39m \u001b[43m_default_to_fused_or_foreach\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_fused\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    225\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    226\u001b[0m         foreach \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/torch/optim/optimizer.py:122\u001b[0m, in \u001b[0;36m_default_to_fused_or_foreach\u001b[0;34m(params, differentiable, use_fused)\u001b[0m\n\u001b[1;32m    116\u001b[0m foreach_supported_devices \u001b[38;5;241m=\u001b[39m _get_foreach_kernels_supported_devices()\n\u001b[1;32m    117\u001b[0m fused \u001b[38;5;241m=\u001b[39m use_fused \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mall\u001b[39m(\n\u001b[1;32m    118\u001b[0m     p \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m (\u001b[38;5;28mtype\u001b[39m(p) \u001b[38;5;129;01min\u001b[39;00m _foreach_supported_types \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[1;32m    119\u001b[0m                   p\u001b[38;5;241m.\u001b[39mdevice\u001b[38;5;241m.\u001b[39mtype \u001b[38;5;129;01min\u001b[39;00m fused_supported_devices \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[1;32m    120\u001b[0m                   torch\u001b[38;5;241m.\u001b[39mis_floating_point(p)) \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m params\n\u001b[1;32m    121\u001b[0m )\n\u001b[0;32m--> 122\u001b[0m foreach \u001b[38;5;241m=\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m fused \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28;43mall\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    123\u001b[0m \u001b[43m    \u001b[49m\u001b[43mp\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mtype\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mp\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_foreach_supported_types\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mand\u001b[39;49;00m\n\u001b[1;32m    124\u001b[0m \u001b[43m                  \u001b[49m\u001b[43mp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mforeach_supported_devices\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\n\u001b[1;32m    125\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    126\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m fused, foreach\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/torch/optim/optimizer.py:123\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    116\u001b[0m foreach_supported_devices \u001b[38;5;241m=\u001b[39m _get_foreach_kernels_supported_devices()\n\u001b[1;32m    117\u001b[0m fused \u001b[38;5;241m=\u001b[39m use_fused \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mall\u001b[39m(\n\u001b[1;32m    118\u001b[0m     p \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m (\u001b[38;5;28mtype\u001b[39m(p) \u001b[38;5;129;01min\u001b[39;00m _foreach_supported_types \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[1;32m    119\u001b[0m                   p\u001b[38;5;241m.\u001b[39mdevice\u001b[38;5;241m.\u001b[39mtype \u001b[38;5;129;01min\u001b[39;00m fused_supported_devices \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[1;32m    120\u001b[0m                   torch\u001b[38;5;241m.\u001b[39mis_floating_point(p)) \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m params\n\u001b[1;32m    121\u001b[0m )\n\u001b[1;32m    122\u001b[0m foreach \u001b[38;5;241m=\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m fused \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mall\u001b[39m(\n\u001b[0;32m--> 123\u001b[0m     p \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m (\u001b[38;5;28mtype\u001b[39m(p) \u001b[38;5;129;01min\u001b[39;00m _foreach_supported_types \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[1;32m    124\u001b[0m                   p\u001b[38;5;241m.\u001b[39mdevice\u001b[38;5;241m.\u001b[39mtype \u001b[38;5;129;01min\u001b[39;00m foreach_supported_devices) \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m params\n\u001b[1;32m    125\u001b[0m )\n\u001b[1;32m    126\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m fused, foreach\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 可能要调的超参数有：hidden_dim, io_dim, lr, batch_size, num_epochs, dataset_name\n",
    "def get_model_name(hidden_dim, io_dim, lr, batch_size, num_epochs, dataset_name):\n",
    "    return f\"crnn_{hidden_dim}_{io_dim}_{lr}_{batch_size}_{num_epochs}_{dataset_name}.pth\"\n",
    "\n",
    "\n",
    "def get_val_loss(model, dataloader, device):\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
    "    model.eval()\n",
    "    loss = 0\n",
    "    with torch.no_grad():\n",
    "        for step, (img, target) in enumerate(dataloader):\n",
    "            img, target = img.to(device), target.to(device)\n",
    "            h_n, c_n = model.init_state(img)\n",
    "            h_n, c_n = h_n.to(device), c_n.to(device)\n",
    "            output = torch.zeros(target.shape[0], model.max_len, model.num_classes).to(device)\n",
    "            x = target[:, 0]\n",
    "            output[:, 0] = x\n",
    "            x = model.embedding(x.long().unsqueeze(1))\n",
    "            for i in range(model.max_len-1):\n",
    "                x = model.embedding(target[:, i].argmax(-1).unsqueeze(1))\n",
    "                x, (h_n, c_n) = model.next_char(x, (h_n, c_n))\n",
    "                output[:, i+1] = x.squeeze(1)\n",
    "            loss += criterion(output.view(-1, model.num_classes), target.argmax(-1).view(-1))\n",
    "    return loss / len(dataloader)\n",
    "\n",
    "\n",
    "# 下面是训练的代码，使用教师强制训练\n",
    "def train_crnn(model, dataloader, learning_rate, epochs, device, start_epoch=0):\n",
    "\n",
    "    if start_epoch:\n",
    "        model_name = get_model_name(model.hidden_dim, model.io_dim, learning_rate, dataloader.batch_size, start_epoch, dataset.name)\n",
    "        model_path = \"/root/autodl-tmp/APS360_Project/Machine_Learning_Output/CRNN/\"\n",
    "        model.load_state_dict(torch.load(model_path + model_name))\n",
    "        start_epoch += 1\n",
    "        print(f\"Model loaded from {model_name}\")\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=5, factor=0.5, verbose=True)\n",
    "    model.to(device)\n",
    "    for epoch in range(start_epoch, epochs):\n",
    "        model.train()\n",
    "        for step, (img, target) in enumerate(dataloader):\n",
    "            img, target = img.to(device), target.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            loss = 0\n",
    "            # 准备初始状态\n",
    "            h_n, c_n = model.init_state(img)\n",
    "            h_n, c_n = h_n.to(device), c_n.to(device)\n",
    "            output = torch.zeros(target.shape[0], model.max_len, model.num_classes).to(device)\n",
    "            x = target[:, 0]\n",
    "            output[:, 0] = x\n",
    "            x = model.embedding(x.long().unsqueeze(1))\n",
    "            for i in range(model.max_len-1):\n",
    "                x = model.embedding(target[:, i].argmax(-1).unsqueeze(1))\n",
    "                x, (h_n, c_n) = model.next_char(x, (h_n, c_n))\n",
    "                output[:, i+1] = x.squeeze(1)\n",
    "            loss = criterion(output.view(-1, model.num_classes), target.argmax(-1).view(-1))\n",
    "            # mask = (target.argmax(-1) != 0).view(-1)\n",
    "            # loss = loss[mask].mean()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            if step % 100 == 0:\n",
    "                print(f\"Epoch {epoch}, Iter {step}, Loss {loss.item()}\")\n",
    "        \n",
    "        # 计算验证集上的loss\n",
    "        val_loss = get_val_loss(model, val_dataloader, device)\n",
    "        scheduler.step(val_loss)\n",
    "        print(f\"Epoch {epoch}, Val Loss {val_loss.item()}\")\n",
    "\n",
    "        model_name = get_model_name(model.hidden_dim, model.io_dim, learning_rate, dataloader.batch_size, epoch, \"concat\")\n",
    "        model_path = \"/root/autodl-tmp/APS360_Project/Machine_Learning_Output/CRNN/\"\n",
    "        torch.save(model.state_dict(), model_path + model_name)\n",
    "        print(f\"Model saved as {model_name}\")\n",
    "\n",
    "\n",
    "model = CRNN()\n",
    "# model_name = get_model_name(model.hidden_dim, model.io_dim, 0.001, 64, 84, \"IAM\")\n",
    "model_path = \"/root/autodl-tmp/APS360_Project/Machine_Learning_Output/CRNN/\"\n",
    "# model.load_state_dict(torch.load(model_path + model_name))\n",
    "# dataset = RecDataset(\"CVL\", \"train\")\n",
    "# dataloader = DataLoader(dataset, batch_size=64, shuffle=True, num_workers=8)\n",
    "val_dataset_IAM = RecDataset(\"IAM\", \"val\")\n",
    "val_dataset_CVL = RecDataset(\"CVL\", \"val\")\n",
    "val_dataset = torch.utils.data.ConcatDataset([val_dataset_IAM, val_dataset_CVL])\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=64, shuffle=False, num_workers=16)\n",
    "dataset_IAM = RecDataset(\"IAM\", \"train\")\n",
    "dataset_CVL = RecDataset(\"CVL\", \"train\")\n",
    "dataset = torch.utils.data.ConcatDataset([dataset_IAM, dataset_CVL])\n",
    "dataloader = DataLoader(dataset, batch_size=64, shuffle=True, num_workers=16)\n",
    "train_crnn(model, dataloader, 0.01, 500, \"cuda:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model\n",
    "model = CRNN()\n",
    "model_name = get_model_name(512, 1024, 0.01, 64, 76, \"concat\")\n",
    "model_path = \"/root/autodl-tmp/APS360_Project/Machine_Learning_Output/CRNN/\"\n",
    "model.load_state_dict(torch.load(model_path + model_name))\n",
    "model.to(\"cuda:0\")\n",
    "model.eval()\n",
    "\n",
    "def show_img(img):\n",
    "    img = img.squeeze().cpu().numpy()\n",
    "    plt.imshow(img, cmap=\"gray\")\n",
    "    plt.show()\n",
    "\n",
    "def get_word(output):\n",
    "    output = output.cpu().detach().numpy()\n",
    "    word = \"\"\n",
    "    for i in range(1, 64):\n",
    "        o = np.argmax(output[i])\n",
    "        if o == 3:\n",
    "            break\n",
    "        c = chr(o)\n",
    "        word += c\n",
    "    return word\n",
    "\n",
    "# test\n",
    "dataset = RecDataset(\"IAM\", \"val\")\n",
    "dataloader = DataLoader(dataset, batch_size=10, shuffle=True, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predict: came\n",
      "answer:  came\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for step, (img, label) in enumerate(dataloader):\n",
    "    img, label = img.to(\"cuda:0\"), label.to(\"cuda:0\")\n",
    "    output = model(img)\n",
    "    print(\"predict:\", get_word(output[0]))\n",
    "    print(\"answer: \", get_word(label[0]))\n",
    "    show_img(img[0])\n",
    "    plt.show()\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.12/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/root/miniconda3/lib/python3.12/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Val Loss 0.23449024558067322\n",
      "Epoch 1, Val Loss 0.19780364632606506\n",
      "Epoch 2, Val Loss 0.18526691198349\n",
      "Epoch 3, Val Loss 0.1751430779695511\n",
      "Epoch 4, Val Loss 0.1766366809606552\n",
      "Epoch 5, Val Loss 0.1695147007703781\n",
      "Epoch 6, Val Loss 0.16747263073921204\n",
      "Epoch 7, Val Loss 0.1705005019903183\n",
      "Epoch 8, Val Loss 0.16582122445106506\n",
      "Epoch 9, Val Loss 0.16494160890579224\n",
      "Epoch 10, Val Loss 0.16596901416778564\n",
      "Epoch 11, Val Loss 0.16609138250350952\n",
      "Epoch 12, Val Loss 0.16744473576545715\n",
      "Epoch 13, Val Loss 0.16565147042274475\n",
      "Epoch 14, Val Loss 0.16715189814567566\n",
      "Epoch 15, Val Loss 0.16947263479232788\n",
      "Epoch 16, Val Loss 0.16958896815776825\n",
      "Epoch 17, Val Loss 0.16812944412231445\n",
      "Epoch 18, Val Loss 0.16993674635887146\n",
      "Epoch 19, Val Loss 0.1703726351261139\n",
      "Epoch 20, Val Loss 0.16811177134513855\n",
      "Epoch 21, Val Loss 0.16878286004066467\n",
      "Epoch 22, Val Loss 0.16931147873401642\n",
      "Epoch 23, Val Loss 0.1694454848766327\n",
      "Epoch 24, Val Loss 0.1701781153678894\n",
      "Epoch 25, Val Loss 0.17109806835651398\n",
      "Epoch 26, Val Loss 0.17095360159873962\n",
      "Epoch 27, Val Loss 0.17102670669555664\n",
      "Epoch 28, Val Loss 0.1715192049741745\n",
      "Epoch 29, Val Loss 0.17176704108715057\n",
      "Epoch 30, Val Loss 0.1718057543039322\n",
      "Epoch 31, Val Loss 0.17196063697338104\n",
      "Epoch 32, Val Loss 0.17187152802944183\n",
      "Epoch 33, Val Loss 0.1722479611635208\n",
      "Epoch 34, Val Loss 0.17240482568740845\n",
      "Epoch 35, Val Loss 0.17236797511577606\n",
      "Epoch 36, Val Loss 0.17202065885066986\n",
      "Epoch 37, Val Loss 0.1725107580423355\n",
      "Epoch 38, Val Loss 0.17259174585342407\n",
      "Epoch 39, Val Loss 0.17280685901641846\n",
      "Epoch 40, Val Loss 0.17261125147342682\n",
      "Epoch 41, Val Loss 0.1726626604795456\n",
      "Epoch 42, Val Loss 0.17292939126491547\n",
      "Epoch 43, Val Loss 0.17278128862380981\n",
      "Epoch 44, Val Loss 0.17280061542987823\n",
      "Epoch 45, Val Loss 0.17290273308753967\n",
      "Epoch 46, Val Loss 0.1726197749376297\n",
      "Epoch 47, Val Loss 0.1731097549200058\n",
      "Epoch 48, Val Loss 0.17280808091163635\n",
      "Epoch 49, Val Loss 0.17285293340682983\n",
      "Epoch 50, Val Loss 0.17290911078453064\n",
      "Epoch 51, Val Loss 0.17313505709171295\n",
      "Epoch 52, Val Loss 0.1732780784368515\n",
      "Epoch 53, Val Loss 0.17301489412784576\n",
      "Epoch 54, Val Loss 0.17303937673568726\n",
      "Epoch 55, Val Loss 0.17288634181022644\n",
      "Epoch 56, Val Loss 0.17281416058540344\n",
      "Epoch 57, Val Loss 0.17316614091396332\n",
      "Epoch 58, Val Loss 0.17282164096832275\n",
      "Epoch 59, Val Loss 0.17290958762168884\n",
      "Epoch 60, Val Loss 0.17291557788848877\n",
      "Epoch 61, Val Loss 0.17299523949623108\n",
      "Epoch 62, Val Loss 0.17306216061115265\n",
      "Epoch 63, Val Loss 0.1731705665588379\n",
      "Epoch 64, Val Loss 0.17321060597896576\n",
      "Epoch 65, Val Loss 0.17293722927570343\n",
      "Epoch 66, Val Loss 0.17285636067390442\n",
      "Epoch 67, Val Loss 0.17317688465118408\n",
      "Epoch 68, Val Loss 0.1731664538383484\n",
      "Epoch 69, Val Loss 0.1728963702917099\n",
      "Epoch 70, Val Loss 0.17278142273426056\n",
      "Epoch 71, Val Loss 0.17298461496829987\n",
      "Epoch 72, Val Loss 0.17284324765205383\n",
      "Epoch 73, Val Loss 0.17330165207386017\n",
      "Epoch 74, Val Loss 0.17301391065120697\n",
      "Epoch 75, Val Loss 0.17303277552127838\n"
     ]
    }
   ],
   "source": [
    "# plot loss crv\n",
    "\n",
    "model = CRNN().cuda()\n",
    "train_loader = DataLoader(RecDataset(\"IAM\", \"train\"), batch_size=64, shuffle=True, num_workers=8)\n",
    "val_loader = DataLoader(RecDataset(\"IAM\", \"val\"), batch_size=64, shuffle=True, num_workers=8)\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
    "device = \"cuda:0\"\n",
    "\n",
    "for epoch in range(76):\n",
    "    model_name = get_model_name(512, 1024, 0.01, 64, epoch, \"concat\")\n",
    "    model_path = \"/root/autodl-tmp/APS360_Project/Machine_Learning_Output/CRNN/\"\n",
    "    model.load_state_dict(torch.load(model_path + model_name))\n",
    "    model.eval()\n",
    "    loss = 0\n",
    "    with torch.no_grad():\n",
    "        for step, (img, target) in enumerate(val_loader):\n",
    "            img, target = img.to(device), target.to(device)\n",
    "            h_n, c_n = model.init_state(img)\n",
    "            h_n, c_n = h_n.to(device), c_n.to(device)\n",
    "            output = torch.zeros(target.shape[0], model.max_len, model.num_classes).to(device)\n",
    "            x = target[:, 0]\n",
    "            output[:, 0] = x\n",
    "            x = model.embedding(x.long().unsqueeze(1))\n",
    "            for i in range(model.max_len-1):\n",
    "                x = model.embedding(target[:, i].argmax(-1).unsqueeze(1))\n",
    "                x, (h_n, c_n) = model.next_char(x, (h_n, c_n))\n",
    "                output[:, i+1] = x.squeeze(1)\n",
    "            loss += criterion(output.view(-1, model.num_classes), target.argmax(-1).view(-1))\n",
    "    val_losses.append(loss / len(dataloader))\n",
    "    print(f\"Epoch {epoch}, Val Loss {val_losses[-1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Val Loss 3.478210210800171\n",
      "Epoch 1, Val Loss 3.0621519088745117\n",
      "Epoch 2, Val Loss 2.8425073623657227\n",
      "Epoch 3, Val Loss 2.685619592666626\n",
      "Epoch 4, Val Loss 2.55229115486145\n",
      "Epoch 5, Val Loss 2.4177439212799072\n",
      "Epoch 6, Val Loss 2.3057734966278076\n",
      "Epoch 7, Val Loss 2.2105913162231445\n",
      "Epoch 8, Val Loss 2.111955165863037\n",
      "Epoch 9, Val Loss 2.030163049697876\n",
      "Epoch 10, Val Loss 1.9540117979049683\n",
      "Epoch 11, Val Loss 1.8829811811447144\n",
      "Epoch 12, Val Loss 1.8394535779953003\n",
      "Epoch 13, Val Loss 1.757978081703186\n",
      "Epoch 14, Val Loss 1.7055529356002808\n",
      "Epoch 15, Val Loss 1.6645315885543823\n",
      "Epoch 16, Val Loss 1.606818437576294\n",
      "Epoch 17, Val Loss 1.5785996913909912\n",
      "Epoch 18, Val Loss 1.584167242050171\n",
      "Epoch 19, Val Loss 1.4771547317504883\n",
      "Epoch 20, Val Loss 1.4356818199157715\n",
      "Epoch 21, Val Loss 1.4132806062698364\n",
      "Epoch 22, Val Loss 1.370862364768982\n",
      "Epoch 23, Val Loss 1.3342267274856567\n",
      "Epoch 24, Val Loss 1.3297675848007202\n",
      "Epoch 25, Val Loss 1.282596230506897\n",
      "Epoch 26, Val Loss 1.2633514404296875\n",
      "Epoch 27, Val Loss 1.2294220924377441\n",
      "Epoch 28, Val Loss 1.2150299549102783\n",
      "Epoch 29, Val Loss 1.1806330680847168\n",
      "Epoch 30, Val Loss 1.170399785041809\n",
      "Epoch 31, Val Loss 1.1450968980789185\n",
      "Epoch 32, Val Loss 1.131713628768921\n",
      "Epoch 33, Val Loss 1.1029592752456665\n",
      "Epoch 34, Val Loss 1.0796699523925781\n",
      "Epoch 35, Val Loss 1.0609869956970215\n",
      "Epoch 36, Val Loss 1.051054835319519\n",
      "Epoch 37, Val Loss 1.0301908254623413\n",
      "Epoch 38, Val Loss 1.0150494575500488\n",
      "Epoch 39, Val Loss 1.0090326070785522\n",
      "Epoch 40, Val Loss 0.994610071182251\n",
      "Epoch 41, Val Loss 0.9958568811416626\n",
      "Epoch 42, Val Loss 0.9884503483772278\n",
      "Epoch 43, Val Loss 0.959949254989624\n",
      "Epoch 44, Val Loss 0.9637017250061035\n",
      "Epoch 45, Val Loss 0.9511829018592834\n",
      "Epoch 46, Val Loss 0.9385563135147095\n",
      "Epoch 47, Val Loss 0.9345293045043945\n",
      "Epoch 48, Val Loss 0.9347803592681885\n",
      "Epoch 49, Val Loss 0.9203394651412964\n",
      "Epoch 50, Val Loss 0.9232437014579773\n",
      "Epoch 51, Val Loss 0.9063302278518677\n",
      "Epoch 52, Val Loss 0.9094985723495483\n",
      "Epoch 53, Val Loss 0.8941757082939148\n",
      "Epoch 54, Val Loss 0.8945206999778748\n",
      "Epoch 55, Val Loss 0.8955072164535522\n",
      "Epoch 56, Val Loss 0.8866227269172668\n",
      "Epoch 57, Val Loss 0.8780379891395569\n",
      "Epoch 58, Val Loss 0.8748263120651245\n",
      "Epoch 59, Val Loss 0.8722445368766785\n",
      "Epoch 60, Val Loss 0.8674086928367615\n",
      "Epoch 61, Val Loss 0.8668352961540222\n",
      "Epoch 62, Val Loss 0.8633576035499573\n",
      "Epoch 63, Val Loss 0.8563470840454102\n",
      "Epoch 64, Val Loss 0.860308825969696\n",
      "Epoch 65, Val Loss 0.8568587899208069\n",
      "Epoch 66, Val Loss 0.8497547507286072\n",
      "Epoch 67, Val Loss 0.8493278622627258\n",
      "Epoch 68, Val Loss 0.8500618934631348\n",
      "Epoch 69, Val Loss 0.8440350890159607\n",
      "Epoch 70, Val Loss 0.8435754776000977\n",
      "Epoch 71, Val Loss 0.8398423790931702\n",
      "Epoch 72, Val Loss 0.8428664207458496\n",
      "Epoch 73, Val Loss 0.8351067900657654\n",
      "Epoch 74, Val Loss 0.8363223671913147\n",
      "Epoch 75, Val Loss 0.8345384001731873\n",
      "Epoch 76, Val Loss 0.8295291662216187\n",
      "Epoch 77, Val Loss 0.8296698331832886\n",
      "Epoch 78, Val Loss 0.8329849243164062\n",
      "Epoch 79, Val Loss 0.8267735242843628\n",
      "Epoch 80, Val Loss 0.8245104551315308\n",
      "Epoch 81, Val Loss 0.8274577856063843\n",
      "Epoch 82, Val Loss 0.8202401995658875\n",
      "Epoch 83, Val Loss 0.8214477896690369\n",
      "Epoch 84, Val Loss 0.819638192653656\n",
      "Epoch 85, Val Loss 0.8213520050048828\n",
      "Epoch 86, Val Loss 0.8173208236694336\n",
      "Epoch 87, Val Loss 0.8166518807411194\n",
      "Epoch 88, Val Loss 0.8159953355789185\n",
      "Epoch 89, Val Loss 0.8165045976638794\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(90):\n",
    "    model_name = get_model_name(model.hidden_dim, model.io_dim, 0.001, 64, epoch, \"IAM\")\n",
    "    model_path = \"/root/autodl-tmp/APS360_Project/Machine_Learning_Output/CRNN/\"\n",
    "    model.load_state_dict(torch.load(model_path + model_name))\n",
    "    model.eval()\n",
    "    loss = 0\n",
    "    with torch.no_grad():\n",
    "        for step, (img, target) in enumerate(train_loader):\n",
    "            img, target = img.to(device), target.to(device)\n",
    "            h_n, c_n = model.init_state(img)\n",
    "            h_n, c_n = h_n.to(device), c_n.to(device)\n",
    "            output = torch.zeros(target.shape[0], model.max_len, model.num_classes).to(device)\n",
    "            x = target[:, 0]\n",
    "            output[:, 0] = x\n",
    "            x = model.embedding(x.long().unsqueeze(1))\n",
    "            for i in range(model.max_len-1):\n",
    "                x = model.embedding(target[:, i].argmax(-1).unsqueeze(1))\n",
    "                x, (h_n, c_n) = model.next_char(x, (h_n, c_n))\n",
    "                output[:, i+1] = x.squeeze(1)\n",
    "            loss += criterion(output.view(-1, model.num_classes), target.argmax(-1).view(-1))\n",
    "    train_losses.append(loss / len(dataloader))\n",
    "    print(f\"Epoch {epoch}, Val Loss {train_losses[-1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "# fix bug\n",
    "val_temp = [i.item() for i in val_losses]\n",
    "train_temp = [i.item() for i in train_losses]\n",
    "dataset_len = len(dataloader)\n",
    "val_len = len(val_loader)\n",
    "train_len = len(train_loader)\n",
    "val_temp = [i / val_len * dataset_len for i in val_temp]\n",
    "train_temp = [i / train_len * dataset_len for i in train_temp]\n",
    "print(val_temp)\n",
    "print(train_temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"train_losses.pkl\", \"wb\") as f:\n",
    "    pickle.dump(train_losses, f)\n",
    "with open(\"val_losses.pkl\", \"wb\") as f:\n",
    "    pickle.dump(val_losses, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(val_temp)\n",
    "plt.plot(train_temp)\n",
    "plt.legend([\"Validation Loss\", \"Training Loss\"])\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
