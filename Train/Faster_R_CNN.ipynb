{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这个文件用来训练Segmentation模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torch import optim\n",
    "from utils import *\n",
    "from torchvision.models.detection.anchor_utils import AnchorGenerator\n",
    "from torch.utils import tensorboard as tb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FasterRCNN(\n",
      "  (transform): GeneralizedRCNNTransform(\n",
      "      Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
      "      Resize(min_size=(800,), max_size=1333, mode='bilinear')\n",
      "  )\n",
      "  (backbone): BackboneWithFPN(\n",
      "    (body): IntermediateLayerGetter(\n",
      "      (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "      (bn1): FrozenBatchNorm2d(64, eps=1e-05)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "      (layer1): Sequential(\n",
      "        (0): Bottleneck(\n",
      "          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): FrozenBatchNorm2d(64, eps=1e-05)\n",
      "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): FrozenBatchNorm2d(64, eps=1e-05)\n",
      "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): FrozenBatchNorm2d(256, eps=1e-05)\n",
      "          (relu): ReLU(inplace=True)\n",
      "          (downsample): Sequential(\n",
      "            (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): FrozenBatchNorm2d(256, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (1): Bottleneck(\n",
      "          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): FrozenBatchNorm2d(64, eps=1e-05)\n",
      "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): FrozenBatchNorm2d(64, eps=1e-05)\n",
      "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): FrozenBatchNorm2d(256, eps=1e-05)\n",
      "          (relu): ReLU(inplace=True)\n",
      "        )\n",
      "        (2): Bottleneck(\n",
      "          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): FrozenBatchNorm2d(64, eps=1e-05)\n",
      "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): FrozenBatchNorm2d(64, eps=1e-05)\n",
      "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): FrozenBatchNorm2d(256, eps=1e-05)\n",
      "          (relu): ReLU(inplace=True)\n",
      "        )\n",
      "      )\n",
      "      (layer2): Sequential(\n",
      "        (0): Bottleneck(\n",
      "          (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): FrozenBatchNorm2d(128, eps=1e-05)\n",
      "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "          (bn2): FrozenBatchNorm2d(128, eps=1e-05)\n",
      "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): FrozenBatchNorm2d(512, eps=1e-05)\n",
      "          (relu): ReLU(inplace=True)\n",
      "          (downsample): Sequential(\n",
      "            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "            (1): FrozenBatchNorm2d(512, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (1): Bottleneck(\n",
      "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): FrozenBatchNorm2d(128, eps=1e-05)\n",
      "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): FrozenBatchNorm2d(128, eps=1e-05)\n",
      "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): FrozenBatchNorm2d(512, eps=1e-05)\n",
      "          (relu): ReLU(inplace=True)\n",
      "        )\n",
      "        (2): Bottleneck(\n",
      "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): FrozenBatchNorm2d(128, eps=1e-05)\n",
      "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): FrozenBatchNorm2d(128, eps=1e-05)\n",
      "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): FrozenBatchNorm2d(512, eps=1e-05)\n",
      "          (relu): ReLU(inplace=True)\n",
      "        )\n",
      "        (3): Bottleneck(\n",
      "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): FrozenBatchNorm2d(128, eps=1e-05)\n",
      "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): FrozenBatchNorm2d(128, eps=1e-05)\n",
      "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): FrozenBatchNorm2d(512, eps=1e-05)\n",
      "          (relu): ReLU(inplace=True)\n",
      "        )\n",
      "      )\n",
      "      (layer3): Sequential(\n",
      "        (0): Bottleneck(\n",
      "          (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): FrozenBatchNorm2d(256, eps=1e-05)\n",
      "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "          (bn2): FrozenBatchNorm2d(256, eps=1e-05)\n",
      "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): FrozenBatchNorm2d(1024, eps=1e-05)\n",
      "          (relu): ReLU(inplace=True)\n",
      "          (downsample): Sequential(\n",
      "            (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "            (1): FrozenBatchNorm2d(1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (1): Bottleneck(\n",
      "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): FrozenBatchNorm2d(256, eps=1e-05)\n",
      "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): FrozenBatchNorm2d(256, eps=1e-05)\n",
      "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): FrozenBatchNorm2d(1024, eps=1e-05)\n",
      "          (relu): ReLU(inplace=True)\n",
      "        )\n",
      "        (2): Bottleneck(\n",
      "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): FrozenBatchNorm2d(256, eps=1e-05)\n",
      "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): FrozenBatchNorm2d(256, eps=1e-05)\n",
      "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): FrozenBatchNorm2d(1024, eps=1e-05)\n",
      "          (relu): ReLU(inplace=True)\n",
      "        )\n",
      "        (3): Bottleneck(\n",
      "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): FrozenBatchNorm2d(256, eps=1e-05)\n",
      "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): FrozenBatchNorm2d(256, eps=1e-05)\n",
      "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): FrozenBatchNorm2d(1024, eps=1e-05)\n",
      "          (relu): ReLU(inplace=True)\n",
      "        )\n",
      "        (4): Bottleneck(\n",
      "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): FrozenBatchNorm2d(256, eps=1e-05)\n",
      "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): FrozenBatchNorm2d(256, eps=1e-05)\n",
      "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): FrozenBatchNorm2d(1024, eps=1e-05)\n",
      "          (relu): ReLU(inplace=True)\n",
      "        )\n",
      "        (5): Bottleneck(\n",
      "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): FrozenBatchNorm2d(256, eps=1e-05)\n",
      "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): FrozenBatchNorm2d(256, eps=1e-05)\n",
      "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): FrozenBatchNorm2d(1024, eps=1e-05)\n",
      "          (relu): ReLU(inplace=True)\n",
      "        )\n",
      "      )\n",
      "      (layer4): Sequential(\n",
      "        (0): Bottleneck(\n",
      "          (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): FrozenBatchNorm2d(512, eps=1e-05)\n",
      "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "          (bn2): FrozenBatchNorm2d(512, eps=1e-05)\n",
      "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): FrozenBatchNorm2d(2048, eps=1e-05)\n",
      "          (relu): ReLU(inplace=True)\n",
      "          (downsample): Sequential(\n",
      "            (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "            (1): FrozenBatchNorm2d(2048, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (1): Bottleneck(\n",
      "          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): FrozenBatchNorm2d(512, eps=1e-05)\n",
      "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): FrozenBatchNorm2d(512, eps=1e-05)\n",
      "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): FrozenBatchNorm2d(2048, eps=1e-05)\n",
      "          (relu): ReLU(inplace=True)\n",
      "        )\n",
      "        (2): Bottleneck(\n",
      "          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): FrozenBatchNorm2d(512, eps=1e-05)\n",
      "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): FrozenBatchNorm2d(512, eps=1e-05)\n",
      "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): FrozenBatchNorm2d(2048, eps=1e-05)\n",
      "          (relu): ReLU(inplace=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (fpn): FeaturePyramidNetwork(\n",
      "      (inner_blocks): ModuleList(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "        )\n",
      "        (2): Conv2dNormActivation(\n",
      "          (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "        )\n",
      "        (3): Conv2dNormActivation(\n",
      "          (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "        )\n",
      "      )\n",
      "      (layer_blocks): ModuleList(\n",
      "        (0-3): 4 x Conv2dNormActivation(\n",
      "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        )\n",
      "      )\n",
      "      (extra_blocks): LastLevelMaxPool()\n",
      "    )\n",
      "  )\n",
      "  (rpn): RegionProposalNetwork(\n",
      "    (anchor_generator): AnchorGenerator()\n",
      "    (head): RPNHead(\n",
      "      (conv): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (1): ReLU(inplace=True)\n",
      "        )\n",
      "      )\n",
      "      (cls_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (bbox_pred): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))\n",
      "    )\n",
      "  )\n",
      "  (roi_heads): RoIHeads(\n",
      "    (box_roi_pool): MultiScaleRoIAlign(featmap_names=['0', '1', '2', '3'], output_size=(7, 7), sampling_ratio=2)\n",
      "    (box_head): TwoMLPHead(\n",
      "      (fc6): Linear(in_features=12544, out_features=1024, bias=True)\n",
      "      (fc7): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    )\n",
      "    (box_predictor): FastRCNNPredictor(\n",
      "      (cls_score): Linear(in_features=1024, out_features=91, bias=True)\n",
      "      (bbox_pred): Linear(in_features=1024, out_features=364, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = torchvision.models.detection.fasterrcnn_resnet50_fpn(anchor_generator=AnchorGenerator(sizes=((2, 4, 8, 16, 32),), aspect_ratios=((0.5, 1.0, 2.0, 4.0, 8.0),)))\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载预训练的Faster R-CNN模型\n",
    "from torchvision.models.detection import *\n",
    "model = fasterrcnn_resnet50_fpn(weights=FasterRCNN_ResNet50_FPN_Weights.DEFAULT)\n",
    "\n",
    "# 获取分类器的输入特征数\n",
    "in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "\n",
    "# 替换预训练的头部为一个新的，只有两个类别（背景和单词）\n",
    "model.roi_heads.box_predictor = torchvision.models.detection.faster_rcnn.FastRCNNPredictor(in_features, num_classes=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModifiedDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, dataset):\n",
    "        self.dataset = dataset\n",
    "        self.trans = transforms.Normalize(mean=(0.5), std=(0.5))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img, label = self.dataset[idx]\n",
    "        img -= img.mean()\n",
    "        img /= img.std()\n",
    "        # img = self.trans(img)\n",
    "\n",
    "        # 将 (x, y, w, h) 格式的边界框转换为 (x1, y1, x2, y2) 格式\n",
    "        label[:, 2] = label[:, 0] + label[:, 2]\n",
    "        label[:, 3] = label[:, 1] + label[:, 3]\n",
    "\n",
    "        # 仅保留包含单词的边界框\n",
    "        indices = label.sum(dim=-1) > 0\n",
    "        label = label[indices]\n",
    "\n",
    "        # 制造classifier的标签\n",
    "        temp = torch.ones(len(label), dtype=torch.long)\n",
    "        label = {'boxes': label, 'labels': temp}\n",
    "        \n",
    "        return img, label\n",
    "\n",
    "\n",
    "from torch.utils.data.dataloader import default_collate\n",
    "\n",
    "def collate_fn(batch):\n",
    "    images = [item[0] for item in batch]\n",
    "    targets = [item[1] for item in batch]\n",
    "\n",
    "    # 使用默认的 collate 处理图片（因为图片大小相同）\n",
    "    images = default_collate(images)\n",
    "    \n",
    "    # 不尝试合并 targets，因为它们包含不同数量的边界框\n",
    "    # 直接作为列表返回\n",
    "    return images, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset = ModifiedDataset(SegDataset('IAM', 'train'))\n",
    "dataset = SegDatasetNew('IAM', 'train')\n",
    "dataloader = DataLoader(dataset, batch_size=16, shuffle=True, num_workers=0, collate_fn=collate_fn, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_147249/2010779150.py:6: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(f'/root/autodl-tmp/APS360_Project/Machine_Learning_Output/Faster_R_CNN/fasterrcnn_{start_epoch}.pth'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 416 step 0 loss: 0.9809857606887817 box: 0.2578507959842682 obj: 0.19770656526088715 rpn: 0.4306868016719818\n",
      "Epoch 416 step 1 loss: 0.9812888503074646 box: 0.23913773894309998 obj: 0.18901187181472778 rpn: 0.4580654501914978\n",
      "Epoch 416 step 2 loss: 0.9477972388267517 box: 0.22459524869918823 obj: 0.19731254875659943 rpn: 0.438596248626709\n",
      "Epoch 416 step 3 loss: 0.9491804838180542 box: 0.21994149684906006 obj: 0.18588709831237793 rpn: 0.4460453391075134\n",
      "Epoch 416 step 4 loss: 0.9554644823074341 box: 0.22588913142681122 obj: 0.2054150402545929 rpn: 0.4406236410140991\n",
      "Epoch 416 step 5 loss: 0.9801985025405884 box: 0.22963550686836243 obj: 0.2047707885503769 rpn: 0.45498594641685486\n",
      "Epoch 416 step 6 loss: 0.9619699120521545 box: 0.22198623418807983 obj: 0.20980492234230042 rpn: 0.4421209692955017\n",
      "Epoch 416 step 7 loss: 0.995886504650116 box: 0.24091511964797974 obj: 0.20521180331707 rpn: 0.44711196422576904\n",
      "Epoch 416 step 8 loss: 0.9153738021850586 box: 0.18136051297187805 obj: 0.1924220323562622 rpn: 0.46042340993881226\n",
      "Epoch 416 step 9 loss: 0.9969585537910461 box: 0.24754935503005981 obj: 0.19552737474441528 rpn: 0.4441930055618286\n",
      "Epoch 416 step 10 loss: 0.9407505393028259 box: 0.20048660039901733 obj: 0.21468593180179596 rpn: 0.4463356137275696\n",
      "Epoch 416 step 11 loss: 0.9920414090156555 box: 0.22968736290931702 obj: 0.20660924911499023 rpn: 0.44793564081192017\n",
      "Epoch 416 step 12 loss: 0.9456982612609863 box: 0.21161222457885742 obj: 0.20438294112682343 rpn: 0.44366252422332764\n",
      "Epoch 416 step 13 loss: 1.0131012201309204 box: 0.26082026958465576 obj: 0.1932358592748642 rpn: 0.44689562916755676\n",
      "Epoch 416 step 14 loss: 0.9479407072067261 box: 0.21276047825813293 obj: 0.19624830782413483 rpn: 0.4432283639907837\n",
      "Epoch 416 step 15 loss: 0.9900997877120972 box: 0.22570303082466125 obj: 0.20525529980659485 rpn: 0.4600921869277954\n",
      "Epoch 416 step 16 loss: 0.9647539854049683 box: 0.22411902248859406 obj: 0.19974705576896667 rpn: 0.4400683641433716\n",
      "Epoch 416 step 17 loss: 1.049304723739624 box: 0.2679481506347656 obj: 0.19287919998168945 rpn: 0.4622707962989807\n",
      "Epoch 416 step 18 loss: 0.9892775416374207 box: 0.22924098372459412 obj: 0.20102614164352417 rpn: 0.45472151041030884\n",
      "Epoch 416 step 19 loss: 0.9979567527770996 box: 0.24723829329013824 obj: 0.20549404621124268 rpn: 0.44672098755836487\n",
      "Epoch 416 step 20 loss: 0.972129225730896 box: 0.21580570936203003 obj: 0.20418290793895721 rpn: 0.4512810707092285\n",
      "Epoch 416 step 21 loss: 0.9830492734909058 box: 0.2344992309808731 obj: 0.20407862961292267 rpn: 0.4477328658103943\n",
      "Epoch 416 step 22 loss: 0.9643702507019043 box: 0.19875958561897278 obj: 0.2234383225440979 rpn: 0.446205198764801\n",
      "Epoch 416 step 23 loss: 0.9841576814651489 box: 0.22108577191829681 obj: 0.22480115294456482 rpn: 0.4372372627258301\n",
      "Epoch 416 step 24 loss: 0.9704394340515137 box: 0.23622199892997742 obj: 0.1994495540857315 rpn: 0.4360097646713257\n",
      "Epoch 416 step 25 loss: 0.9867532849311829 box: 0.2501140236854553 obj: 0.20715855062007904 rpn: 0.42160916328430176\n",
      "Epoch 416 step 26 loss: 0.9819076061248779 box: 0.22991463541984558 obj: 0.20585806667804718 rpn: 0.4412291646003723\n",
      "Epoch 416 step 27 loss: 0.9624835848808289 box: 0.22227266430854797 obj: 0.19251284003257751 rpn: 0.44842392206192017\n",
      "Epoch 416 step 28 loss: 1.0012929439544678 box: 0.26526787877082825 obj: 0.19170841574668884 rpn: 0.4325316548347473\n",
      "Epoch 416 step 29 loss: 0.9480479955673218 box: 0.19497564435005188 obj: 0.20150884985923767 rpn: 0.44758981466293335\n",
      "Epoch 416 step 30 loss: 0.9798630475997925 box: 0.2193778157234192 obj: 0.19960013031959534 rpn: 0.45383885502815247\n",
      "Epoch 416 step 31 loss: 1.0100600719451904 box: 0.2455378621816635 obj: 0.21516725420951843 rpn: 0.4429618716239929\n",
      "Epoch 416 step 32 loss: 0.9843815565109253 box: 0.2190549224615097 obj: 0.19858387112617493 rpn: 0.4526228904724121\n",
      "Epoch 416 step 33 loss: 0.9416345953941345 box: 0.1878211349248886 obj: 0.2081317901611328 rpn: 0.4420284032821655\n",
      "Epoch 416 step 34 loss: 1.0002076625823975 box: 0.21731433272361755 obj: 0.2073749601840973 rpn: 0.44539526104927063\n",
      "Epoch 416 step 35 loss: 1.0036282539367676 box: 0.22055348753929138 obj: 0.22049671411514282 rpn: 0.4541678726673126\n",
      "Epoch 416 step 36 loss: 0.9734244346618652 box: 0.22459572553634644 obj: 0.1961856484413147 rpn: 0.4444129467010498\n",
      "Epoch 416 step 37 loss: 0.9642381072044373 box: 0.20499393343925476 obj: 0.20833836495876312 rpn: 0.449704647064209\n",
      "Epoch 416 step 38 loss: 1.037751317024231 box: 0.26982343196868896 obj: 0.21342399716377258 rpn: 0.4415249824523926\n",
      "Epoch 416 step 39 loss: 0.9426252841949463 box: 0.20847094058990479 obj: 0.195833221077919 rpn: 0.442524790763855\n",
      "Epoch 416 step 40 loss: 1.003164291381836 box: 0.2585258483886719 obj: 0.19515541195869446 rpn: 0.4374150037765503\n",
      "Epoch 416 step 41 loss: 0.95693039894104 box: 0.23017624020576477 obj: 0.19516518712043762 rpn: 0.4381641447544098\n",
      "Epoch 416 step 42 loss: 1.02485990524292 box: 0.2475624680519104 obj: 0.21761780977249146 rpn: 0.4505468010902405\n",
      "Epoch 416 step 43 loss: 0.9577794075012207 box: 0.20865298807621002 obj: 0.20757067203521729 rpn: 0.4472208023071289\n",
      "Epoch 416 step 44 loss: 0.981566309928894 box: 0.2230307161808014 obj: 0.2221890687942505 rpn: 0.44862183928489685\n",
      "Epoch 416 step 45 loss: 1.02299165725708 box: 0.25607842206954956 obj: 0.20252417027950287 rpn: 0.45065295696258545\n",
      "Epoch 416 step 46 loss: 0.9435534477233887 box: 0.2030600607395172 obj: 0.21442249417304993 rpn: 0.43671783804893494\n",
      "Epoch 416 step 47 loss: 0.9255415201187134 box: 0.20864379405975342 obj: 0.19713689386844635 rpn: 0.4339662790298462\n",
      "Epoch 416 step 48 loss: 1.0351299047470093 box: 0.26119089126586914 obj: 0.19884559512138367 rpn: 0.45063507556915283\n",
      "Epoch 416 step 49 loss: 0.9307218790054321 box: 0.2087559849023819 obj: 0.1877480298280716 rpn: 0.44697049260139465\n",
      "Epoch 416 step 50 loss: 1.0134010314941406 box: 0.23111194372177124 obj: 0.20469039678573608 rpn: 0.47276759147644043\n",
      "Epoch 416 step 51 loss: 0.9160230755805969 box: 0.18462342023849487 obj: 0.20476269721984863 rpn: 0.4397526681423187\n",
      "Epoch 416 step 52 loss: 1.000596046447754 box: 0.25787800550460815 obj: 0.1809232234954834 rpn: 0.4548352062702179\n",
      "Epoch 416 step 53 loss: 0.9346836805343628 box: 0.16966372728347778 obj: 0.21110311150550842 rpn: 0.4655529260635376\n",
      "Epoch 416 step 54 loss: 0.982962965965271 box: 0.24599912762641907 obj: 0.19023212790489197 rpn: 0.44691216945648193\n",
      "Epoch 416 step 55 loss: 0.9176831841468811 box: 0.1929105669260025 obj: 0.18206478655338287 rpn: 0.4608485698699951\n",
      "Epoch 416 step 56 loss: 1.012211799621582 box: 0.27224352955818176 obj: 0.19358325004577637 rpn: 0.4457116425037384\n",
      "Epoch 416 step 57 loss: 0.9482710957527161 box: 0.19851329922676086 obj: 0.20969682931900024 rpn: 0.46233606338500977\n",
      "Epoch 416 step 58 loss: 1.0596885681152344 box: 0.28684619069099426 obj: 0.19926761090755463 rpn: 0.4587196707725525\n",
      "Epoch 416 step 59 loss: 0.9009929895401001 box: 0.1857232302427292 obj: 0.1932288408279419 rpn: 0.4391537606716156\n",
      "Epoch 416 step 60 loss: 1.022826075553894 box: 0.26103031635284424 obj: 0.19664576649665833 rpn: 0.45851758122444153\n",
      "Epoch 416 step 61 loss: 0.9600526690483093 box: 0.22178372740745544 obj: 0.19308912754058838 rpn: 0.45527172088623047\n",
      "Epoch 416 step 62 loss: 0.9387744665145874 box: 0.23491476476192474 obj: 0.18632552027702332 rpn: 0.4272480607032776\n",
      "Epoch 416 step 63 loss: 0.9803775548934937 box: 0.2227577567100525 obj: 0.19851338863372803 rpn: 0.4596516788005829\n",
      "Epoch 416 step 64 loss: 1.0477025508880615 box: 0.2660571336746216 obj: 0.20751507580280304 rpn: 0.46234387159347534\n",
      "Epoch 416 step 65 loss: 0.9572124481201172 box: 0.23486900329589844 obj: 0.2010619342327118 rpn: 0.43142691254615784\n",
      "Epoch 416 step 66 loss: 0.9995234608650208 box: 0.23873627185821533 obj: 0.19965627789497375 rpn: 0.4520987272262573\n",
      "Epoch 416 step 67 loss: 0.9359872341156006 box: 0.20014524459838867 obj: 0.19861973822116852 rpn: 0.44517552852630615\n",
      "Epoch 416 step 68 loss: 0.9884145855903625 box: 0.26553773880004883 obj: 0.19114437699317932 rpn: 0.43398517370224\n",
      "Epoch 416 step 69 loss: 0.9791261553764343 box: 0.23733364045619965 obj: 0.19465982913970947 rpn: 0.44972580671310425\n",
      "Epoch 416 step 70 loss: 0.9721147418022156 box: 0.2298392504453659 obj: 0.19149717688560486 rpn: 0.45130807161331177\n",
      "Epoch 416 step 71 loss: 1.0034425258636475 box: 0.23342031240463257 obj: 0.20181593298912048 rpn: 0.46031540632247925\n",
      "Epoch 416 step 72 loss: 0.974751353263855 box: 0.19021669030189514 obj: 0.21499282121658325 rpn: 0.46417951583862305\n",
      "Epoch 416 step 73 loss: 0.9746052026748657 box: 0.22812840342521667 obj: 0.19916008412837982 rpn: 0.4433031976222992\n",
      "Epoch 416 step 74 loss: 0.9584270715713501 box: 0.2010762095451355 obj: 0.23126710951328278 rpn: 0.43241453170776367\n",
      "Epoch 416 step 75 loss: 0.9881526231765747 box: 0.22125880420207977 obj: 0.21370720863342285 rpn: 0.4476502537727356\n",
      "Epoch 416 loss: 0.9881526231765747\n",
      "Epoch 417 step 0 loss: 0.8517513871192932 box: 0.1686323881149292 obj: 0.19193747639656067 rpn: 0.42010968923568726\n",
      "Epoch 417 step 1 loss: 1.0193276405334473 box: 0.2600743770599365 obj: 0.1918180137872696 rpn: 0.4505224823951721\n",
      "Epoch 417 step 2 loss: 0.9039357900619507 box: 0.17953529953956604 obj: 0.21044737100601196 rpn: 0.4405897557735443\n",
      "Epoch 417 step 3 loss: 1.0043303966522217 box: 0.24643494188785553 obj: 0.2065790891647339 rpn: 0.4484756588935852\n",
      "Epoch 417 step 4 loss: 0.9306470155715942 box: 0.22670653462409973 obj: 0.18104760348796844 rpn: 0.4404793679714203\n",
      "Epoch 417 step 5 loss: 0.9878516793251038 box: 0.24224530160427094 obj: 0.19278785586357117 rpn: 0.4403138756752014\n",
      "Epoch 417 step 6 loss: 0.9293249845504761 box: 0.2242390215396881 obj: 0.18057997524738312 rpn: 0.4421032667160034\n",
      "Epoch 417 step 7 loss: 1.0549299716949463 box: 0.2807983160018921 obj: 0.20206023752689362 rpn: 0.45746076107025146\n",
      "Epoch 417 step 8 loss: 0.9845141768455505 box: 0.20818477869033813 obj: 0.21044883131980896 rpn: 0.4664236307144165\n",
      "Epoch 417 step 9 loss: 1.008124589920044 box: 0.26122862100601196 obj: 0.20716819167137146 rpn: 0.439893513917923\n",
      "Epoch 417 step 10 loss: 0.875289261341095 box: 0.16960114240646362 obj: 0.18329238891601562 rpn: 0.45086801052093506\n",
      "Epoch 417 step 11 loss: 0.9753636121749878 box: 0.24041956663131714 obj: 0.1843767613172531 rpn: 0.45421016216278076\n",
      "Epoch 417 step 12 loss: 0.9338158369064331 box: 0.2095598578453064 obj: 0.19422543048858643 rpn: 0.4443250298500061\n",
      "Epoch 417 step 13 loss: 0.9854507446289062 box: 0.23725536465644836 obj: 0.1975553333759308 rpn: 0.45350635051727295\n",
      "Epoch 417 step 14 loss: 0.9754484295845032 box: 0.24339251220226288 obj: 0.19752031564712524 rpn: 0.4377861022949219\n",
      "Epoch 417 step 15 loss: 0.944729208946228 box: 0.21099643409252167 obj: 0.20142321288585663 rpn: 0.44172409176826477\n",
      "Epoch 417 step 16 loss: 1.0158482789993286 box: 0.26745355129241943 obj: 0.1972840428352356 rpn: 0.44542884826660156\n",
      "Epoch 417 step 17 loss: 0.9423145055770874 box: 0.2013632357120514 obj: 0.2023983895778656 rpn: 0.45613497495651245\n",
      "Epoch 417 step 18 loss: 1.0281950235366821 box: 0.26302409172058105 obj: 0.22102265059947968 rpn: 0.4483107924461365\n",
      "Epoch 417 step 19 loss: 0.9494459629058838 box: 0.20966103672981262 obj: 0.20739203691482544 rpn: 0.4441714286804199\n",
      "Epoch 417 step 20 loss: 1.0196630954742432 box: 0.2520156502723694 obj: 0.19544726610183716 rpn: 0.458849161863327\n",
      "Epoch 417 step 21 loss: 0.8909904360771179 box: 0.1863126903772354 obj: 0.17998361587524414 rpn: 0.43564122915267944\n",
      "Epoch 417 step 22 loss: 1.0202021598815918 box: 0.26227840781211853 obj: 0.2030918002128601 rpn: 0.4417601227760315\n",
      "Epoch 417 step 23 loss: 0.9664883017539978 box: 0.2432781457901001 obj: 0.18503019213676453 rpn: 0.4451987147331238\n",
      "Epoch 417 step 24 loss: 0.9351222515106201 box: 0.22008547186851501 obj: 0.19843462109565735 rpn: 0.4306243658065796\n",
      "Epoch 417 step 25 loss: 1.0092320442199707 box: 0.2620353400707245 obj: 0.2019156813621521 rpn: 0.43894678354263306\n",
      "Epoch 417 step 26 loss: 0.9660632610321045 box: 0.22026348114013672 obj: 0.2101368010044098 rpn: 0.43997299671173096\n",
      "Epoch 417 step 27 loss: 1.0382213592529297 box: 0.27098456025123596 obj: 0.1977718621492386 rpn: 0.45611482858657837\n",
      "Epoch 417 step 28 loss: 0.9172028303146362 box: 0.20213207602500916 obj: 0.19796019792556763 rpn: 0.4367648959159851\n",
      "Epoch 417 step 29 loss: 0.9616016149520874 box: 0.2364509105682373 obj: 0.19502314925193787 rpn: 0.43876078724861145\n",
      "Epoch 417 step 30 loss: 0.9098843336105347 box: 0.20453158020973206 obj: 0.18614982068538666 rpn: 0.4386023283004761\n",
      "Epoch 417 step 31 loss: 0.984123706817627 box: 0.24484647810459137 obj: 0.20147563517093658 rpn: 0.43855199217796326\n",
      "Epoch 417 step 32 loss: 0.901077151298523 box: 0.19342157244682312 obj: 0.18393376469612122 rpn: 0.4422682225704193\n",
      "Epoch 417 step 33 loss: 0.9839257597923279 box: 0.2688526511192322 obj: 0.19617941975593567 rpn: 0.41912132501602173\n",
      "Epoch 417 step 34 loss: 0.9340314865112305 box: 0.2106376439332962 obj: 0.20238140225410461 rpn: 0.43962568044662476\n",
      "Epoch 417 step 35 loss: 0.9596071243286133 box: 0.2288624495267868 obj: 0.2000657320022583 rpn: 0.43284448981285095\n",
      "Epoch 417 step 36 loss: 0.9520028829574585 box: 0.22744759917259216 obj: 0.1876165270805359 rpn: 0.44696903228759766\n",
      "Epoch 417 step 37 loss: 1.0157216787338257 box: 0.2559157907962799 obj: 0.21262939274311066 rpn: 0.44643592834472656\n",
      "Epoch 417 step 38 loss: 0.9796693325042725 box: 0.24507547914981842 obj: 0.18510207533836365 rpn: 0.4438518285751343\n",
      "Epoch 417 step 39 loss: 0.9715312719345093 box: 0.248610258102417 obj: 0.1906571388244629 rpn: 0.434550404548645\n",
      "Epoch 417 step 40 loss: 0.972212553024292 box: 0.2281663417816162 obj: 0.19850865006446838 rpn: 0.4535618722438812\n",
      "Epoch 417 step 41 loss: 0.9108568429946899 box: 0.2064235806465149 obj: 0.18574747443199158 rpn: 0.43611955642700195\n",
      "Epoch 417 step 42 loss: 0.9363897442817688 box: 0.2133987843990326 obj: 0.18630671501159668 rpn: 0.4483816623687744\n",
      "Epoch 417 step 43 loss: 1.018263578414917 box: 0.2638506293296814 obj: 0.19191335141658783 rpn: 0.4467514753341675\n",
      "Epoch 417 step 44 loss: 0.9071301221847534 box: 0.18218490481376648 obj: 0.2006995677947998 rpn: 0.4474828839302063\n",
      "Epoch 417 step 45 loss: 0.956947922706604 box: 0.23818659782409668 obj: 0.18632268905639648 rpn: 0.4469994008541107\n",
      "Epoch 417 step 46 loss: 0.9077979326248169 box: 0.2016606330871582 obj: 0.18601436913013458 rpn: 0.4297991096973419\n",
      "Epoch 417 step 47 loss: 0.9640917778015137 box: 0.25337499380111694 obj: 0.18888333439826965 rpn: 0.42274773120880127\n",
      "Epoch 417 step 48 loss: 0.9450036883354187 box: 0.23900745809078217 obj: 0.1869998574256897 rpn: 0.4352903366088867\n",
      "Epoch 417 step 49 loss: 0.9830453395843506 box: 0.24190089106559753 obj: 0.1942152976989746 rpn: 0.4453129172325134\n",
      "Epoch 417 step 50 loss: 0.9057999849319458 box: 0.2091151773929596 obj: 0.18919064104557037 rpn: 0.4278446435928345\n",
      "Epoch 417 step 51 loss: 0.9772416949272156 box: 0.24444802105426788 obj: 0.19072076678276062 rpn: 0.44125252962112427\n",
      "Epoch 417 step 52 loss: 0.9320086240768433 box: 0.21105197072029114 obj: 0.20215392112731934 rpn: 0.4444892108440399\n",
      "Epoch 417 step 53 loss: 0.9882137775421143 box: 0.247521311044693 obj: 0.19793689250946045 rpn: 0.44258999824523926\n",
      "Epoch 417 step 54 loss: 0.9041157960891724 box: 0.1796237826347351 obj: 0.21998564898967743 rpn: 0.43318912386894226\n",
      "Epoch 417 step 55 loss: 0.9577771425247192 box: 0.23072443902492523 obj: 0.19459018111228943 rpn: 0.4371977150440216\n",
      "Epoch 417 step 56 loss: 0.9171685576438904 box: 0.2033863067626953 obj: 0.19335724413394928 rpn: 0.45005643367767334\n",
      "Epoch 417 step 57 loss: 1.0210113525390625 box: 0.28196805715560913 obj: 0.19505229592323303 rpn: 0.4439045190811157\n",
      "Epoch 417 step 58 loss: 0.8928730487823486 box: 0.2090913951396942 obj: 0.18100237846374512 rpn: 0.42723190784454346\n",
      "Epoch 417 step 59 loss: 0.999070942401886 box: 0.25358015298843384 obj: 0.19588175415992737 rpn: 0.438096821308136\n",
      "Epoch 417 step 60 loss: 0.9348762035369873 box: 0.20805513858795166 obj: 0.19376115500926971 rpn: 0.4496527910232544\n",
      "Epoch 417 step 61 loss: 0.9554508924484253 box: 0.23201045393943787 obj: 0.19346484541893005 rpn: 0.43159979581832886\n",
      "Epoch 417 step 62 loss: 0.9198471307754517 box: 0.21414533257484436 obj: 0.18490451574325562 rpn: 0.4501764178276062\n",
      "Epoch 417 step 63 loss: 0.9922444224357605 box: 0.25006651878356934 obj: 0.1976514458656311 rpn: 0.4538361430168152\n",
      "Epoch 417 step 64 loss: 0.9110175371170044 box: 0.20799222588539124 obj: 0.19778886437416077 rpn: 0.4300757348537445\n",
      "Epoch 417 step 65 loss: 1.0444250106811523 box: 0.27978530526161194 obj: 0.20475976169109344 rpn: 0.44720661640167236\n",
      "Epoch 417 step 66 loss: 0.919306755065918 box: 0.20684409141540527 obj: 0.18599596619606018 rpn: 0.4548310935497284\n",
      "Epoch 417 step 67 loss: 0.9204051494598389 box: 0.21492449939250946 obj: 0.1883617639541626 rpn: 0.4316822290420532\n",
      "Epoch 417 step 68 loss: 0.9337265491485596 box: 0.20762811601161957 obj: 0.20885291695594788 rpn: 0.4412722587585449\n",
      "Epoch 417 step 69 loss: 0.9377686977386475 box: 0.22003473341464996 obj: 0.20599494874477386 rpn: 0.4243067800998688\n",
      "Epoch 417 step 70 loss: 0.9516624212265015 box: 0.22627697885036469 obj: 0.19201362133026123 rpn: 0.42488572001457214\n",
      "Epoch 417 step 71 loss: 0.9747533798217773 box: 0.23856765031814575 obj: 0.20374369621276855 rpn: 0.43020081520080566\n",
      "Epoch 417 step 72 loss: 0.94957435131073 box: 0.23107042908668518 obj: 0.19940897822380066 rpn: 0.4308873116970062\n",
      "Epoch 417 step 73 loss: 0.9781927466392517 box: 0.2529941499233246 obj: 0.17797812819480896 rpn: 0.44872593879699707\n",
      "Epoch 417 step 74 loss: 0.8953274488449097 box: 0.19994783401489258 obj: 0.178220734000206 rpn: 0.4422581195831299\n",
      "Epoch 417 step 75 loss: 0.9949813485145569 box: 0.2677653431892395 obj: 0.20105460286140442 rpn: 0.4206847548484802\n",
      "Epoch 417 loss: 0.9949813485145569\n",
      "Epoch 418 step 0 loss: 0.9621858596801758 box: 0.23495425283908844 obj: 0.2096492350101471 rpn: 0.43763917684555054\n",
      "Epoch 418 step 1 loss: 0.9745689034461975 box: 0.25448793172836304 obj: 0.18638268113136292 rpn: 0.4372784495353699\n",
      "Epoch 418 step 2 loss: 0.9657580852508545 box: 0.24390314519405365 obj: 0.20012247562408447 rpn: 0.4405854344367981\n",
      "Epoch 418 step 3 loss: 0.9523378014564514 box: 0.2431122213602066 obj: 0.201033353805542 rpn: 0.4157969355583191\n",
      "Epoch 418 step 4 loss: 0.9483726024627686 box: 0.23913690447807312 obj: 0.18951201438903809 rpn: 0.427852600812912\n",
      "Epoch 418 step 5 loss: 0.9502931833267212 box: 0.23893411457538605 obj: 0.20011281967163086 rpn: 0.4310438632965088\n",
      "Epoch 418 step 6 loss: 0.963293194770813 box: 0.2457341104745865 obj: 0.19531868398189545 rpn: 0.4188666343688965\n",
      "Epoch 418 step 7 loss: 0.9094917178153992 box: 0.2029915750026703 obj: 0.20212113857269287 rpn: 0.4356212019920349\n",
      "Epoch 418 step 8 loss: 0.9671831130981445 box: 0.23848336935043335 obj: 0.20081713795661926 rpn: 0.4354346990585327\n",
      "Epoch 418 step 9 loss: 0.9039922952651978 box: 0.2127753496170044 obj: 0.19751307368278503 rpn: 0.42139917612075806\n",
      "Epoch 418 step 10 loss: 0.9269520044326782 box: 0.22064587473869324 obj: 0.18567931652069092 rpn: 0.44468092918395996\n",
      "Epoch 418 step 11 loss: 0.9496721029281616 box: 0.22781281173229218 obj: 0.20437179505825043 rpn: 0.4369044899940491\n",
      "Epoch 418 step 12 loss: 0.9611769318580627 box: 0.2604202628135681 obj: 0.19286805391311646 rpn: 0.421520471572876\n",
      "Epoch 418 step 13 loss: 0.9370884895324707 box: 0.23170876502990723 obj: 0.2081407606601715 rpn: 0.41650816798210144\n",
      "Epoch 418 step 14 loss: 0.9533020257949829 box: 0.2342638373374939 obj: 0.19159188866615295 rpn: 0.433259516954422\n",
      "Epoch 418 step 15 loss: 0.9146254658699036 box: 0.21276633441448212 obj: 0.20252607762813568 rpn: 0.4231010675430298\n",
      "Epoch 418 step 16 loss: 0.9533400535583496 box: 0.25371429324150085 obj: 0.1824568808078766 rpn: 0.41307950019836426\n",
      "Epoch 418 step 17 loss: 0.92226243019104 box: 0.2144670933485031 obj: 0.1987539529800415 rpn: 0.42978397011756897\n",
      "Epoch 418 step 18 loss: 0.9898749589920044 box: 0.2525055706501007 obj: 0.19418084621429443 rpn: 0.43872785568237305\n",
      "Epoch 418 step 19 loss: 0.9084013104438782 box: 0.21161749958992004 obj: 0.18728859722614288 rpn: 0.4271860718727112\n",
      "Epoch 418 step 20 loss: 1.003281593322754 box: 0.2682037055492401 obj: 0.19139117002487183 rpn: 0.4417176842689514\n",
      "Epoch 418 step 21 loss: 0.8990266919136047 box: 0.21110591292381287 obj: 0.18791139125823975 rpn: 0.4296913743019104\n",
      "Epoch 418 step 22 loss: 1.0077544450759888 box: 0.26095983386039734 obj: 0.20454923808574677 rpn: 0.44248807430267334\n",
      "Epoch 418 step 23 loss: 0.9186459183692932 box: 0.20177604258060455 obj: 0.205483540892601 rpn: 0.43824565410614014\n",
      "Epoch 418 step 24 loss: 0.9394078254699707 box: 0.2263946235179901 obj: 0.2062121480703354 rpn: 0.42227229475975037\n",
      "Epoch 418 step 25 loss: 0.9428725838661194 box: 0.21804212033748627 obj: 0.20437899231910706 rpn: 0.44178324937820435\n",
      "Epoch 418 step 26 loss: 0.9096550941467285 box: 0.2219863086938858 obj: 0.17947998642921448 rpn: 0.4184035062789917\n",
      "Epoch 418 step 27 loss: 0.9217603206634521 box: 0.22634343802928925 obj: 0.20442882180213928 rpn: 0.41259142756462097\n",
      "Epoch 418 step 28 loss: 0.96248859167099 box: 0.2686951458454132 obj: 0.17330586910247803 rpn: 0.42997390031814575\n",
      "Epoch 418 step 29 loss: 0.9508228302001953 box: 0.22660109400749207 obj: 0.20442350208759308 rpn: 0.43274080753326416\n",
      "Epoch 418 step 30 loss: 0.9401682615280151 box: 0.23003414273262024 obj: 0.1969727873802185 rpn: 0.4251231551170349\n",
      "Epoch 418 step 31 loss: 0.9145951271057129 box: 0.22486531734466553 obj: 0.20120146870613098 rpn: 0.4115440547466278\n",
      "Epoch 418 step 32 loss: 0.9382762908935547 box: 0.22904908657073975 obj: 0.20369279384613037 rpn: 0.42715883255004883\n",
      "Epoch 418 step 33 loss: 0.9427109956741333 box: 0.24771347641944885 obj: 0.18137240409851074 rpn: 0.42583534121513367\n",
      "Epoch 418 step 34 loss: 0.8970379829406738 box: 0.20422708988189697 obj: 0.18760496377944946 rpn: 0.43249213695526123\n",
      "Epoch 418 step 35 loss: 0.8997709155082703 box: 0.21501636505126953 obj: 0.18795591592788696 rpn: 0.42107605934143066\n",
      "Epoch 418 step 36 loss: 0.8989176750183105 box: 0.20107582211494446 obj: 0.19651652872562408 rpn: 0.4261537194252014\n",
      "Epoch 418 step 37 loss: 0.9286309480667114 box: 0.2160075604915619 obj: 0.191609725356102 rpn: 0.4353300929069519\n",
      "Epoch 418 step 38 loss: 0.9370620846748352 box: 0.21328604221343994 obj: 0.21049460768699646 rpn: 0.4335453510284424\n",
      "Epoch 418 step 39 loss: 0.8782355189323425 box: 0.1967998743057251 obj: 0.18655313551425934 rpn: 0.4248475432395935\n",
      "Epoch 418 step 40 loss: 0.9289124011993408 box: 0.23411844670772552 obj: 0.19771455228328705 rpn: 0.42004188895225525\n",
      "Epoch 418 step 41 loss: 0.9303805828094482 box: 0.22778157889842987 obj: 0.1919974982738495 rpn: 0.4304392635822296\n",
      "Epoch 418 step 42 loss: 0.9068124294281006 box: 0.21757283806800842 obj: 0.19387832283973694 rpn: 0.4244266450405121\n",
      "Epoch 418 step 43 loss: 0.9586607217788696 box: 0.2387574017047882 obj: 0.2055584192276001 rpn: 0.43216586112976074\n",
      "Epoch 418 step 44 loss: 0.918708324432373 box: 0.1965976059436798 obj: 0.20875559747219086 rpn: 0.43230220675468445\n",
      "Epoch 418 step 45 loss: 0.9855883717536926 box: 0.26461145281791687 obj: 0.1932867467403412 rpn: 0.4342610836029053\n",
      "Epoch 418 step 46 loss: 0.9222438335418701 box: 0.2223491668701172 obj: 0.1985286921262741 rpn: 0.4199441969394684\n",
      "Epoch 418 step 47 loss: 0.9379964470863342 box: 0.22415640950202942 obj: 0.17941290140151978 rpn: 0.4429796040058136\n",
      "Epoch 418 step 48 loss: 0.8704063892364502 box: 0.19565801322460175 obj: 0.18994933366775513 rpn: 0.42142775654792786\n",
      "Epoch 418 step 49 loss: 0.9397909641265869 box: 0.2431076318025589 obj: 0.19035905599594116 rpn: 0.42169713973999023\n",
      "Epoch 418 step 50 loss: 0.9495853781700134 box: 0.25075918436050415 obj: 0.18837928771972656 rpn: 0.43228304386138916\n",
      "Epoch 418 step 51 loss: 0.9464883804321289 box: 0.23884767293930054 obj: 0.19393517076969147 rpn: 0.44145774841308594\n",
      "Epoch 418 step 52 loss: 0.8916268348693848 box: 0.20282962918281555 obj: 0.2012682557106018 rpn: 0.41960257291793823\n",
      "Epoch 418 step 53 loss: 0.9054819345474243 box: 0.22617381811141968 obj: 0.18008194863796234 rpn: 0.416747510433197\n",
      "Epoch 418 step 54 loss: 0.9054932594299316 box: 0.21028998494148254 obj: 0.19127969443798065 rpn: 0.4309003949165344\n",
      "Epoch 418 step 55 loss: 0.9112595319747925 box: 0.2315891683101654 obj: 0.18358078598976135 rpn: 0.42421549558639526\n",
      "Epoch 418 step 56 loss: 0.92109215259552 box: 0.2342739999294281 obj: 0.18582624197006226 rpn: 0.424894243478775\n",
      "Epoch 418 step 57 loss: 0.9248625040054321 box: 0.23963168263435364 obj: 0.17814812064170837 rpn: 0.41990160942077637\n",
      "Epoch 418 step 58 loss: 0.905096173286438 box: 0.20720934867858887 obj: 0.18448086082935333 rpn: 0.44358590245246887\n",
      "Epoch 418 step 59 loss: 0.9508054852485657 box: 0.2579619884490967 obj: 0.18823757767677307 rpn: 0.424415647983551\n",
      "Epoch 418 step 60 loss: 0.9030742645263672 box: 0.22540919482707977 obj: 0.1772306263446808 rpn: 0.4213613271713257\n",
      "Epoch 418 step 61 loss: 0.9572761654853821 box: 0.2404906153678894 obj: 0.21818125247955322 rpn: 0.41765159368515015\n",
      "Epoch 418 step 62 loss: 0.948655903339386 box: 0.23730215430259705 obj: 0.19225242733955383 rpn: 0.42871570587158203\n",
      "Epoch 418 step 63 loss: 0.9081060290336609 box: 0.21292996406555176 obj: 0.1927223801612854 rpn: 0.42097383737564087\n",
      "Epoch 418 step 64 loss: 0.9498071074485779 box: 0.2554686963558197 obj: 0.18078434467315674 rpn: 0.42739367485046387\n",
      "Epoch 418 step 65 loss: 0.8975066542625427 box: 0.2218695878982544 obj: 0.19097746908664703 rpn: 0.4031038284301758\n",
      "Epoch 418 step 66 loss: 0.9514567852020264 box: 0.23199741542339325 obj: 0.20711787045001984 rpn: 0.42174801230430603\n",
      "Epoch 418 step 67 loss: 0.9166417121887207 box: 0.2363872230052948 obj: 0.1899605095386505 rpn: 0.4176179766654968\n",
      "Epoch 418 step 68 loss: 0.931633472442627 box: 0.24564801156520844 obj: 0.18022969365119934 rpn: 0.42631596326828003\n",
      "Epoch 418 step 69 loss: 0.9389709830284119 box: 0.2366582453250885 obj: 0.1971043199300766 rpn: 0.4195377826690674\n",
      "Epoch 418 step 70 loss: 0.9659725427627563 box: 0.25316575169563293 obj: 0.2098153978586197 rpn: 0.4213889241218567\n",
      "Epoch 418 step 71 loss: 0.8687982559204102 box: 0.18294420838356018 obj: 0.19790050387382507 rpn: 0.42457348108291626\n",
      "Epoch 418 step 72 loss: 0.9216778874397278 box: 0.22642064094543457 obj: 0.1910352259874344 rpn: 0.42242228984832764\n",
      "Epoch 418 step 73 loss: 0.9432143568992615 box: 0.24076245725154877 obj: 0.20704269409179688 rpn: 0.42118120193481445\n",
      "Epoch 418 step 74 loss: 0.9561951160430908 box: 0.2561153173446655 obj: 0.19127002358436584 rpn: 0.420492947101593\n",
      "Epoch 418 step 75 loss: 0.9271148443222046 box: 0.23670007288455963 obj: 0.19376406073570251 rpn: 0.4157358407974243\n",
      "Epoch 418 loss: 0.9271148443222046\n",
      "Epoch 419 step 0 loss: 0.9394922852516174 box: 0.2536929249763489 obj: 0.1841701865196228 rpn: 0.4218689799308777\n",
      "Epoch 419 step 1 loss: 0.8690894842147827 box: 0.19591303169727325 obj: 0.19735513627529144 rpn: 0.41424572467803955\n",
      "Epoch 419 step 2 loss: 0.9319199919700623 box: 0.24601130187511444 obj: 0.19784832000732422 rpn: 0.41200143098831177\n",
      "Epoch 419 step 3 loss: 0.8983567953109741 box: 0.22550228238105774 obj: 0.20083153247833252 rpn: 0.4056410789489746\n",
      "Epoch 419 step 4 loss: 0.9374328851699829 box: 0.24884209036827087 obj: 0.20229293406009674 rpn: 0.4070519506931305\n",
      "Epoch 419 step 5 loss: 0.9472173452377319 box: 0.2452162504196167 obj: 0.197097510099411 rpn: 0.41974595189094543\n",
      "Epoch 419 step 6 loss: 0.9707322120666504 box: 0.2560575306415558 obj: 0.19858671724796295 rpn: 0.42527517676353455\n",
      "Epoch 419 step 7 loss: 0.9341858625411987 box: 0.25401002168655396 obj: 0.1943274438381195 rpn: 0.4012657701969147\n",
      "Epoch 419 step 8 loss: 0.9001641273498535 box: 0.23746855556964874 obj: 0.18028624355793 rpn: 0.4110586941242218\n",
      "Epoch 419 step 9 loss: 0.9728282690048218 box: 0.27286094427108765 obj: 0.17951463162899017 rpn: 0.4344976544380188\n",
      "Epoch 419 step 10 loss: 0.889176607131958 box: 0.22194276750087738 obj: 0.1781657338142395 rpn: 0.4128369092941284\n",
      "Epoch 419 step 11 loss: 0.9389381408691406 box: 0.2440062165260315 obj: 0.18676665425300598 rpn: 0.431541383266449\n",
      "Epoch 419 step 12 loss: 0.901073157787323 box: 0.23952096700668335 obj: 0.18995341658592224 rpn: 0.4042699933052063\n",
      "Epoch 419 step 13 loss: 0.9549986124038696 box: 0.25629106163978577 obj: 0.2010873556137085 rpn: 0.41875994205474854\n",
      "Epoch 419 step 14 loss: 0.9058838486671448 box: 0.2228294312953949 obj: 0.18500208854675293 rpn: 0.43154874444007874\n",
      "Epoch 419 step 15 loss: 0.9278939962387085 box: 0.2428375482559204 obj: 0.19469141960144043 rpn: 0.4033472537994385\n",
      "Epoch 419 step 16 loss: 0.857825756072998 box: 0.18435323238372803 obj: 0.19534271955490112 rpn: 0.41809186339378357\n",
      "Epoch 419 step 17 loss: 0.9292330741882324 box: 0.2534833550453186 obj: 0.18266218900680542 rpn: 0.4201951026916504\n",
      "Epoch 419 step 18 loss: 0.8638511300086975 box: 0.20420265197753906 obj: 0.18082648515701294 rpn: 0.41558462381362915\n",
      "Epoch 419 step 19 loss: 0.9092215299606323 box: 0.22912535071372986 obj: 0.19629374146461487 rpn: 0.4086437523365021\n",
      "Epoch 419 step 20 loss: 0.8829114437103271 box: 0.2272738367319107 obj: 0.1769237220287323 rpn: 0.4111834168434143\n",
      "Epoch 419 step 21 loss: 0.8906912803649902 box: 0.22613425552845 obj: 0.17540135979652405 rpn: 0.41646796464920044\n",
      "Epoch 419 step 22 loss: 0.8982132077217102 box: 0.230345219373703 obj: 0.19499921798706055 rpn: 0.3982927203178406\n",
      "Epoch 419 step 23 loss: 0.9693450331687927 box: 0.25738561153411865 obj: 0.20062384009361267 rpn: 0.42343395948410034\n",
      "Epoch 419 step 24 loss: 0.8845608234405518 box: 0.22871598601341248 obj: 0.18112798035144806 rpn: 0.40752872824668884\n",
      "Epoch 419 step 25 loss: 0.9130234122276306 box: 0.2416362166404724 obj: 0.18626388907432556 rpn: 0.40677744150161743\n",
      "Epoch 419 step 26 loss: 0.897449254989624 box: 0.23099255561828613 obj: 0.18263033032417297 rpn: 0.41157811880111694\n",
      "Epoch 419 step 27 loss: 0.9076514840126038 box: 0.23868310451507568 obj: 0.17670223116874695 rpn: 0.406790554523468\n",
      "Epoch 419 step 28 loss: 0.9024144411087036 box: 0.21153827011585236 obj: 0.1947403848171234 rpn: 0.4250870943069458\n",
      "Epoch 419 step 29 loss: 0.9132952690124512 box: 0.227636456489563 obj: 0.1913069188594818 rpn: 0.4187721610069275\n",
      "Epoch 419 step 30 loss: 0.9030759334564209 box: 0.22549010813236237 obj: 0.19971059262752533 rpn: 0.40829116106033325\n",
      "Epoch 419 step 31 loss: 0.9272177219390869 box: 0.26909345388412476 obj: 0.17694130539894104 rpn: 0.410216748714447\n",
      "Epoch 419 step 32 loss: 0.8775736689567566 box: 0.21027034521102905 obj: 0.1877729743719101 rpn: 0.40724045038223267\n",
      "Epoch 419 step 33 loss: 0.9182466268539429 box: 0.24803851544857025 obj: 0.1951032280921936 rpn: 0.4001197814941406\n",
      "Epoch 419 step 34 loss: 0.8989280462265015 box: 0.22278353571891785 obj: 0.17390555143356323 rpn: 0.425187349319458\n",
      "Epoch 419 step 35 loss: 0.8966777324676514 box: 0.22125555574893951 obj: 0.19064538180828094 rpn: 0.41461241245269775\n",
      "Epoch 419 step 36 loss: 0.8737752437591553 box: 0.2085709422826767 obj: 0.19740012288093567 rpn: 0.39776715636253357\n",
      "Epoch 419 step 37 loss: 0.9142082929611206 box: 0.26199278235435486 obj: 0.1745680570602417 rpn: 0.40369176864624023\n",
      "Epoch 419 step 38 loss: 0.8796394467353821 box: 0.2131461501121521 obj: 0.19406725466251373 rpn: 0.41189467906951904\n",
      "Epoch 419 step 39 loss: 0.8875300288200378 box: 0.23154282569885254 obj: 0.1808863878250122 rpn: 0.4030840992927551\n",
      "Epoch 419 step 40 loss: 0.8986548185348511 box: 0.23797857761383057 obj: 0.1856209933757782 rpn: 0.4044516384601593\n",
      "Epoch 419 step 41 loss: 0.920276939868927 box: 0.23982134461402893 obj: 0.18422675132751465 rpn: 0.4237077534198761\n",
      "Epoch 419 step 42 loss: 0.8713588714599609 box: 0.21040642261505127 obj: 0.18641844391822815 rpn: 0.40615418553352356\n",
      "Epoch 419 step 43 loss: 0.9363914728164673 box: 0.25236761569976807 obj: 0.19651637971401215 rpn: 0.4149872064590454\n",
      "Epoch 419 step 44 loss: 0.8844988942146301 box: 0.22464215755462646 obj: 0.18105030059814453 rpn: 0.4066571891307831\n",
      "Epoch 419 step 45 loss: 0.90006422996521 box: 0.22737222909927368 obj: 0.19442838430404663 rpn: 0.40931281447410583\n",
      "Epoch 419 step 46 loss: 0.896487832069397 box: 0.2356284260749817 obj: 0.17643877863883972 rpn: 0.4145519733428955\n",
      "Epoch 419 step 47 loss: 0.8902668952941895 box: 0.2154666781425476 obj: 0.18769553303718567 rpn: 0.4120158553123474\n",
      "Epoch 419 step 48 loss: 0.9075734615325928 box: 0.23621751368045807 obj: 0.18727505207061768 rpn: 0.4109066128730774\n",
      "Epoch 419 step 49 loss: 0.9183109998703003 box: 0.22486147284507751 obj: 0.19326597452163696 rpn: 0.4227823317050934\n",
      "Epoch 419 step 50 loss: 0.874260425567627 box: 0.20939487218856812 obj: 0.18666136264801025 rpn: 0.4101448059082031\n",
      "Epoch 419 step 51 loss: 0.9157751202583313 box: 0.23862944543361664 obj: 0.18764227628707886 rpn: 0.4211791753768921\n",
      "Epoch 419 step 52 loss: 0.9308829307556152 box: 0.2588823735713959 obj: 0.19754736125469208 rpn: 0.4037119448184967\n",
      "Epoch 419 step 53 loss: 0.8732573390007019 box: 0.2033894956111908 obj: 0.17910511791706085 rpn: 0.42094844579696655\n",
      "Epoch 419 step 54 loss: 0.9007976651191711 box: 0.24473461508750916 obj: 0.18215009570121765 rpn: 0.39632338285446167\n",
      "Epoch 419 step 55 loss: 0.8942845463752747 box: 0.23171833157539368 obj: 0.179483100771904 rpn: 0.41039353609085083\n",
      "Epoch 419 step 56 loss: 0.8988292813301086 box: 0.23396678268909454 obj: 0.184376060962677 rpn: 0.4061068892478943\n",
      "Epoch 419 step 57 loss: 0.921581506729126 box: 0.25060898065567017 obj: 0.18315550684928894 rpn: 0.4135955572128296\n",
      "Epoch 419 step 58 loss: 0.8902392387390137 box: 0.236745685338974 obj: 0.1750761866569519 rpn: 0.4021161198616028\n",
      "Epoch 419 step 59 loss: 0.8895412087440491 box: 0.22427986562252045 obj: 0.19278192520141602 rpn: 0.399958074092865\n",
      "Epoch 419 step 60 loss: 0.8969037532806396 box: 0.2233634889125824 obj: 0.19461199641227722 rpn: 0.410491943359375\n",
      "Epoch 419 step 61 loss: 0.8836773633956909 box: 0.21652640402317047 obj: 0.18219944834709167 rpn: 0.4219295382499695\n",
      "Epoch 419 step 62 loss: 0.8875777125358582 box: 0.22732321918010712 obj: 0.18634143471717834 rpn: 0.40154051780700684\n",
      "Epoch 419 step 63 loss: 0.9161208868026733 box: 0.22773849964141846 obj: 0.18952587246894836 rpn: 0.4225574731826782\n",
      "Epoch 419 step 64 loss: 0.9310816526412964 box: 0.2572440505027771 obj: 0.18486137688159943 rpn: 0.4149513840675354\n",
      "Epoch 419 step 65 loss: 0.8734226226806641 box: 0.22034934163093567 obj: 0.18045350909233093 rpn: 0.4118320643901825\n",
      "Epoch 419 step 66 loss: 0.880366861820221 box: 0.21748173236846924 obj: 0.194189190864563 rpn: 0.4004172682762146\n",
      "Epoch 419 step 67 loss: 0.8935593962669373 box: 0.22425180673599243 obj: 0.1948162168264389 rpn: 0.40422677993774414\n",
      "Epoch 419 step 68 loss: 0.9226216077804565 box: 0.24403652548789978 obj: 0.18157124519348145 rpn: 0.4153563976287842\n",
      "Epoch 419 step 69 loss: 0.9001748561859131 box: 0.23222693800926208 obj: 0.19249150156974792 rpn: 0.40916579961776733\n",
      "Epoch 419 step 70 loss: 0.8918949961662292 box: 0.22720561921596527 obj: 0.1868179589509964 rpn: 0.41092318296432495\n",
      "Epoch 419 step 71 loss: 0.8729699850082397 box: 0.20676954090595245 obj: 0.1863907426595688 rpn: 0.4093688130378723\n",
      "Epoch 419 step 72 loss: 0.9125374555587769 box: 0.2361992597579956 obj: 0.19331389665603638 rpn: 0.41412511467933655\n",
      "Epoch 419 step 73 loss: 0.9092172980308533 box: 0.23626500368118286 obj: 0.18661069869995117 rpn: 0.4102635383605957\n",
      "Epoch 419 step 74 loss: 0.8848819732666016 box: 0.19805750250816345 obj: 0.19725900888442993 rpn: 0.41799306869506836\n",
      "Epoch 419 step 75 loss: 0.9027551412582397 box: 0.24533824622631073 obj: 0.17372006177902222 rpn: 0.4028109908103943\n",
      "Epoch 419 loss: 0.9027551412582397\n",
      "Epoch 420 step 0 loss: 0.8651137948036194 box: 0.23350638151168823 obj: 0.17268985509872437 rpn: 0.39009177684783936\n",
      "Epoch 420 step 1 loss: 0.9110661745071411 box: 0.23003029823303223 obj: 0.1958446502685547 rpn: 0.4156687259674072\n",
      "Epoch 420 step 2 loss: 0.8566911220550537 box: 0.1944977343082428 obj: 0.1895461082458496 rpn: 0.41223543882369995\n",
      "Epoch 420 step 3 loss: 0.8685364723205566 box: 0.2196159064769745 obj: 0.17980173230171204 rpn: 0.4023786783218384\n",
      "Epoch 420 step 4 loss: 0.9308516979217529 box: 0.2539147138595581 obj: 0.19217032194137573 rpn: 0.4095584452152252\n",
      "Epoch 420 step 5 loss: 0.9122729897499084 box: 0.26706796884536743 obj: 0.17597410082817078 rpn: 0.4058682322502136\n",
      "Epoch 420 step 6 loss: 0.8812673091888428 box: 0.21914321184158325 obj: 0.1896054446697235 rpn: 0.4021766483783722\n",
      "Epoch 420 step 7 loss: 0.9014574289321899 box: 0.23086082935333252 obj: 0.1939077228307724 rpn: 0.4047694504261017\n",
      "Epoch 420 step 8 loss: 0.8722540140151978 box: 0.21317635476589203 obj: 0.1994110345840454 rpn: 0.40064164996147156\n",
      "Epoch 420 step 9 loss: 0.8759422302246094 box: 0.2277776449918747 obj: 0.1861514002084732 rpn: 0.401263028383255\n",
      "Epoch 420 step 10 loss: 0.8567259311676025 box: 0.20685508847236633 obj: 0.18629957735538483 rpn: 0.40432095527648926\n",
      "Epoch 420 step 11 loss: 0.8693096041679382 box: 0.2098277360200882 obj: 0.1781235933303833 rpn: 0.417804479598999\n",
      "Epoch 420 step 12 loss: 0.8687422275543213 box: 0.22252991795539856 obj: 0.17978467047214508 rpn: 0.39953526854515076\n",
      "Epoch 420 step 13 loss: 0.8634063601493835 box: 0.1964646875858307 obj: 0.194912388920784 rpn: 0.41027504205703735\n",
      "Epoch 420 step 14 loss: 0.9146015644073486 box: 0.252973735332489 obj: 0.18813163042068481 rpn: 0.39970213174819946\n",
      "Epoch 420 step 15 loss: 0.8974624276161194 box: 0.2381850630044937 obj: 0.18718615174293518 rpn: 0.4064845144748688\n",
      "Epoch 420 step 16 loss: 0.8761366009712219 box: 0.23383969068527222 obj: 0.17834678292274475 rpn: 0.38783225417137146\n",
      "Epoch 420 step 17 loss: 0.8880221843719482 box: 0.21559348702430725 obj: 0.1966800093650818 rpn: 0.4106348752975464\n",
      "Epoch 420 step 18 loss: 0.8937159180641174 box: 0.2202831506729126 obj: 0.20286622643470764 rpn: 0.4038735628128052\n",
      "Epoch 420 step 19 loss: 0.8577998876571655 box: 0.20984633266925812 obj: 0.1718091517686844 rpn: 0.40191134810447693\n",
      "Epoch 420 step 20 loss: 0.8839801549911499 box: 0.22377784550189972 obj: 0.19097349047660828 rpn: 0.403043657541275\n",
      "Epoch 420 step 21 loss: 0.8651775121688843 box: 0.2223331779241562 obj: 0.19037741422653198 rpn: 0.38738277554512024\n",
      "Epoch 420 step 22 loss: 0.9137348532676697 box: 0.26031380891799927 obj: 0.17207810282707214 rpn: 0.4102281928062439\n",
      "Epoch 420 step 23 loss: 0.8962351679801941 box: 0.2460540235042572 obj: 0.17775684595108032 rpn: 0.4030521512031555\n",
      "Epoch 420 step 24 loss: 0.8974713087081909 box: 0.24342307448387146 obj: 0.18157649040222168 rpn: 0.39851242303848267\n",
      "Epoch 420 step 25 loss: 0.9151914119720459 box: 0.24379605054855347 obj: 0.18832221627235413 rpn: 0.4118971526622772\n",
      "Epoch 420 step 26 loss: 0.8639421463012695 box: 0.22700348496437073 obj: 0.1773311346769333 rpn: 0.39571183919906616\n",
      "Epoch 420 step 27 loss: 0.89695143699646 box: 0.22736433148384094 obj: 0.19977527856826782 rpn: 0.4053581953048706\n",
      "Epoch 420 step 28 loss: 0.9178277254104614 box: 0.2544635236263275 obj: 0.1941872537136078 rpn: 0.40209588408470154\n",
      "Epoch 420 step 29 loss: 0.8614007830619812 box: 0.20351865887641907 obj: 0.18785536289215088 rpn: 0.4085068106651306\n",
      "Epoch 420 step 30 loss: 0.8963149189949036 box: 0.23878182470798492 obj: 0.19556184113025665 rpn: 0.39665693044662476\n",
      "Epoch 420 step 31 loss: 0.8980823755264282 box: 0.2420022338628769 obj: 0.18797817826271057 rpn: 0.40397322177886963\n",
      "Epoch 420 step 32 loss: 0.9155348539352417 box: 0.22247013449668884 obj: 0.2000565230846405 rpn: 0.42004573345184326\n",
      "Epoch 420 step 33 loss: 0.9088553786277771 box: 0.24603420495986938 obj: 0.18524757027626038 rpn: 0.4078061580657959\n",
      "Epoch 420 step 34 loss: 0.8988372087478638 box: 0.2560282349586487 obj: 0.17258623242378235 rpn: 0.4002634286880493\n",
      "Epoch 420 step 35 loss: 0.8933942317962646 box: 0.2402505725622177 obj: 0.18717798590660095 rpn: 0.4033932685852051\n",
      "Epoch 420 step 36 loss: 0.8854001760482788 box: 0.23573799431324005 obj: 0.1797499656677246 rpn: 0.40775686502456665\n",
      "Epoch 420 step 37 loss: 0.8898139595985413 box: 0.23683303594589233 obj: 0.17456483840942383 rpn: 0.4093586504459381\n",
      "Epoch 420 step 38 loss: 0.8770880699157715 box: 0.2235906422138214 obj: 0.1810857653617859 rpn: 0.40116024017333984\n",
      "Epoch 420 step 39 loss: 0.8825101852416992 box: 0.2281080037355423 obj: 0.19244669377803802 rpn: 0.40107282996177673\n",
      "Epoch 420 step 40 loss: 0.8830509185791016 box: 0.23130808770656586 obj: 0.18709105253219604 rpn: 0.39926254749298096\n",
      "Epoch 420 step 41 loss: 0.8892245888710022 box: 0.23948624730110168 obj: 0.1916571706533432 rpn: 0.3931601643562317\n",
      "Epoch 420 step 42 loss: 0.8746920228004456 box: 0.2253037393093109 obj: 0.18188223242759705 rpn: 0.4004213809967041\n",
      "Epoch 420 step 43 loss: 0.8340487480163574 box: 0.19164112210273743 obj: 0.18911880254745483 rpn: 0.3914453387260437\n",
      "Epoch 420 step 44 loss: 0.8606762886047363 box: 0.23151984810829163 obj: 0.18053975701332092 rpn: 0.38024020195007324\n",
      "Epoch 420 step 45 loss: 0.8890823125839233 box: 0.23067648708820343 obj: 0.1882978081703186 rpn: 0.40783265233039856\n",
      "Epoch 420 step 46 loss: 0.8986779451370239 box: 0.23344728350639343 obj: 0.18344464898109436 rpn: 0.41363781690597534\n",
      "Epoch 420 step 47 loss: 0.901762068271637 box: 0.2288520485162735 obj: 0.20119093358516693 rpn: 0.4068877696990967\n",
      "Epoch 420 step 48 loss: 0.8968862295150757 box: 0.23752188682556152 obj: 0.18936069309711456 rpn: 0.3981155455112457\n",
      "Epoch 420 step 49 loss: 0.8890869617462158 box: 0.21423348784446716 obj: 0.18829643726348877 rpn: 0.41660141944885254\n",
      "Epoch 420 step 50 loss: 0.8922011852264404 box: 0.24528922140598297 obj: 0.1771506369113922 rpn: 0.4001399874687195\n",
      "Epoch 420 step 51 loss: 0.8699187636375427 box: 0.2137109935283661 obj: 0.18612000346183777 rpn: 0.3995305597782135\n",
      "Epoch 420 step 52 loss: 0.9172149896621704 box: 0.25916993618011475 obj: 0.1908993422985077 rpn: 0.3963654637336731\n",
      "Epoch 420 step 53 loss: 0.908484935760498 box: 0.24472780525684357 obj: 0.1768261194229126 rpn: 0.4111473262310028\n",
      "Epoch 420 step 54 loss: 0.8949342966079712 box: 0.23249469697475433 obj: 0.1776857078075409 rpn: 0.413036584854126\n",
      "Epoch 420 step 55 loss: 0.8969813585281372 box: 0.24095770716667175 obj: 0.19081148505210876 rpn: 0.39867329597473145\n",
      "Epoch 420 step 56 loss: 0.9024752974510193 box: 0.24839410185813904 obj: 0.18232481181621552 rpn: 0.40371090173721313\n",
      "Epoch 420 step 57 loss: 0.9033380150794983 box: 0.2525005638599396 obj: 0.19015583395957947 rpn: 0.39267534017562866\n",
      "Epoch 420 step 58 loss: 0.897216260433197 box: 0.23300787806510925 obj: 0.1987280547618866 rpn: 0.3998970091342926\n",
      "Epoch 420 step 59 loss: 0.9115307927131653 box: 0.2566291391849518 obj: 0.17764608561992645 rpn: 0.4103594422340393\n",
      "Epoch 420 step 60 loss: 0.8878067135810852 box: 0.22594031691551208 obj: 0.18594127893447876 rpn: 0.4010079801082611\n",
      "Epoch 420 step 61 loss: 0.8443477749824524 box: 0.20473235845565796 obj: 0.18303370475769043 rpn: 0.3982194662094116\n",
      "Epoch 420 step 62 loss: 0.8814265727996826 box: 0.22118893265724182 obj: 0.18752756714820862 rpn: 0.41230666637420654\n",
      "Epoch 420 step 63 loss: 0.8796623945236206 box: 0.22262470424175262 obj: 0.19920961558818817 rpn: 0.39476606249809265\n",
      "Epoch 420 step 64 loss: 0.8821916580200195 box: 0.21020668745040894 obj: 0.1918627917766571 rpn: 0.4136788845062256\n",
      "Epoch 420 step 65 loss: 0.8897325992584229 box: 0.23782292008399963 obj: 0.18617317080497742 rpn: 0.4073029160499573\n",
      "Epoch 420 step 66 loss: 0.8597118258476257 box: 0.2255365401506424 obj: 0.1799490600824356 rpn: 0.39313602447509766\n",
      "Epoch 420 step 67 loss: 0.8796056509017944 box: 0.23466910421848297 obj: 0.17949360609054565 rpn: 0.40162861347198486\n",
      "Epoch 420 step 68 loss: 0.8752204775810242 box: 0.23287062346935272 obj: 0.17128336429595947 rpn: 0.40783289074897766\n",
      "Epoch 420 step 69 loss: 0.9129697680473328 box: 0.24936069548130035 obj: 0.1921861469745636 rpn: 0.40815943479537964\n",
      "Epoch 420 step 70 loss: 0.8911514282226562 box: 0.241360604763031 obj: 0.1741742491722107 rpn: 0.40621256828308105\n",
      "Epoch 420 step 71 loss: 0.8843364119529724 box: 0.23262706398963928 obj: 0.19348853826522827 rpn: 0.4000229239463806\n",
      "Epoch 420 step 72 loss: 0.9016885757446289 box: 0.2423240840435028 obj: 0.18457654118537903 rpn: 0.411385715007782\n"
     ]
    }
   ],
   "source": [
    "def get_model_name(epoch):\n",
    "    return f\"fasterrcnn_{epoch}.pth\"\n",
    "\n",
    "# load model\n",
    "start_epoch = 415\n",
    "model.load_state_dict(torch.load(f'/root/autodl-tmp/APS360_Project/Machine_Learning_Output/Faster_R_CNN/fasterrcnn_{start_epoch}.pth'))\n",
    "\n",
    "writer = tb.SummaryWriter('/root/tf-logs')\n",
    "\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "model.to(device)\n",
    "\n",
    "# 优化器\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
    "\n",
    "# 学习率调度器\n",
    "lr_scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=500, eta_min=0)\n",
    "\n",
    "# 训练模式\n",
    "model.train()\n",
    "\n",
    "num_epochs = 500\n",
    "\n",
    "for epoch in range(start_epoch + 1, num_epochs):\n",
    "    for step, (images, targets) in enumerate(dataloader):\n",
    "        images = list(image.to(device) for image in images)\n",
    "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "        # 计算损失\n",
    "        loss_dict = model(images, targets)\n",
    "        losses = sum(loss_dict.values())\n",
    "\n",
    "        # 反向传播\n",
    "        optimizer.zero_grad()\n",
    "        losses.backward()\n",
    "        # 添加梯度裁剪\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "        # 更新学习率\n",
    "        lr_scheduler.step()\n",
    "\n",
    "        print(f\"Epoch {epoch} step {step} loss: {losses.item()} box: {loss_dict['loss_box_reg'].item()} \"\n",
    "              f\"obj: {loss_dict['loss_objectness'].item()} rpn: {loss_dict['loss_rpn_box_reg'].item()}\")\n",
    "        writer.add_scalar('Loss/Total', losses.item(), epoch * len(dataloader) + step)\n",
    "        writer.add_scalar('Loss/Box', loss_dict['loss_box_reg'].item(), epoch * len(dataloader) + step)\n",
    "        writer.add_scalar('Loss/Objectness', loss_dict['loss_objectness'].item(), epoch * len(dataloader) + step)\n",
    "        writer.add_scalar('Loss/RPN', loss_dict['loss_rpn_box_reg'].item(), epoch * len(dataloader) + step)\n",
    "        writer.add_scalar('Loss/Classifier', loss_dict['loss_classifier'].item(), epoch * len(dataloader) + step)\n",
    "\n",
    "    print(f\"Epoch {epoch} loss: {losses.item()}\")\n",
    "    model_path = '/root/autodl-tmp/APS360_Project/Machine_Learning_Output/Faster_R_CNN/'\n",
    "    model_name = model_path + get_model_name(epoch)\n",
    "    torch.save(model.state_dict(), model_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "出图"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.12/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/root/miniconda3/lib/python3.12/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n",
      "/root/miniconda3/lib/python3.12/site-packages/torch/nn/modules/conv.py:456: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)\n",
      "  return F.conv2d(input, weight, bias, self.stride,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 loss: 1.4299521843592327\n",
      "Epoch 1 loss: 2.4078004482464914\n",
      "Epoch 2 loss: 2.5131293076735277\n",
      "Epoch 3 loss: 2.5633106353955393\n",
      "Epoch 4 loss: 2.763592799504598\n",
      "Epoch 5 loss: 2.939693114696405\n",
      "Epoch 6 loss: 3.227608858010708\n",
      "Epoch 7 loss: 3.3041126911456766\n",
      "Epoch 8 loss: 3.5120299596052904\n",
      "Epoch 9 loss: 3.5352905652461906\n",
      "Epoch 10 loss: 3.7562476304861216\n",
      "Epoch 11 loss: 3.9067625999450684\n",
      "Epoch 12 loss: 4.062150319417317\n",
      "Epoch 13 loss: 4.202261386773525\n",
      "Epoch 14 loss: 4.46792625158261\n",
      "Epoch 15 loss: 4.469106111771021\n",
      "Epoch 16 loss: 4.6693345461136255\n",
      "Epoch 17 loss: 4.8165292250804415\n",
      "Epoch 18 loss: 4.764869677714812\n",
      "Epoch 19 loss: 4.821755507053473\n",
      "Epoch 20 loss: 4.889397242130378\n",
      "Epoch 21 loss: 4.950071909488776\n",
      "Epoch 22 loss: 5.171686343657664\n",
      "Epoch 23 loss: 5.0569125933524885\n",
      "Epoch 24 loss: 5.0835376763955145\n",
      "Epoch 25 loss: 5.169943540524214\n",
      "Epoch 26 loss: 5.093042716001853\n",
      "Epoch 27 loss: 5.516045704866067\n",
      "Epoch 28 loss: 5.399636525374192\n",
      "Epoch 29 loss: 5.571760752262214\n",
      "Epoch 30 loss: 5.427923447046524\n",
      "Epoch 31 loss: 5.729722402034661\n",
      "Epoch 32 loss: 5.3023319488916645\n"
     ]
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision\n",
    "from torch.utils.data import DataLoader\n",
    "from utils import *\n",
    "\n",
    "def get_model_name(epoch):\n",
    "    return f\"fasterrcnn_{epoch}.pth\"\n",
    "\n",
    "def load_model(model, epoch):\n",
    "    model_path = '/root/autodl-tmp/APS360_Project/Machine_Learning_Output/Faster_R_CNN/'\n",
    "    model_name = model_path + get_model_name(epoch)\n",
    "    model.load_state_dict(torch.load(model_name))\n",
    "    return model\n",
    "\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "loss_crv = []\n",
    "model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=False)\n",
    "in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "model.roi_heads.box_predictor = torchvision.models.detection.faster_rcnn.FastRCNNPredictor(in_features, num_classes=2)\n",
    "model.to(device)\n",
    "\n",
    "def collate_fn(batch):\n",
    "    images = [item[0] for item in batch]\n",
    "    targets = [item[1] for item in batch]\n",
    "    images = default_collate(images)\n",
    "    return images, targets\n",
    "\n",
    "dataset = ModifiedDataset(SegDataset('IAM', 'train'))\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True, num_workers=0, collate_fn=collate_fn)\n",
    "\n",
    "for epoch in range(100):\n",
    "    model = load_model(model, epoch)\n",
    "    # model.eval()\n",
    "    total_loss = 0\n",
    "    total_step = 0\n",
    "    with torch.no_grad():\n",
    "        for step, (images, targets) in enumerate(dataloader):\n",
    "            images = list(image.to(device) for image in images)\n",
    "            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "            # 计算损失\n",
    "            loss_dict = model(images, targets)\n",
    "            # print(loss_dict)\n",
    "            losses = sum(loss for loss in loss_dict.values())\n",
    "            total_loss += losses.item()\n",
    "            total_step += 1\n",
    "\n",
    "            # print(f\"Epoch {epoch} step {step} loss: {losses.item()}\")\n",
    "    losses = total_loss / total_step\n",
    "    loss_crv.append(losses)\n",
    "    print(f\"Epoch {epoch} loss: {losses}\")\n",
    "\n",
    "plt.plot(np.arange(len(loss_crv)), loss_crv)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Loss Curve of Current Faster R-CNN Model')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save loss crv\n",
    "import pickle\n",
    "with open('train_crv.pkl', 'wb') as f:\n",
    "    pickle.dump(loss_crv, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.12/site-packages/torch/nn/modules/conv.py:456: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)\n",
      "  return F.conv2d(input, weight, bias, self.stride,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 loss: 2.3937403678894045\n",
      "Epoch 1 loss: 1.4132689952850341\n",
      "Epoch 2 loss: 0.9501038670539856\n",
      "Epoch 3 loss: 0.6789250016212464\n",
      "Epoch 4 loss: 0.528100597858429\n",
      "Epoch 5 loss: 0.4638660132884979\n",
      "Epoch 6 loss: 0.4135788559913635\n",
      "Epoch 7 loss: 0.3777868807315826\n",
      "Epoch 8 loss: 0.36378070116043093\n",
      "Epoch 9 loss: 0.3573130011558533\n",
      "Epoch 10 loss: 0.3569773077964783\n",
      "Epoch 11 loss: 0.38963437676429746\n",
      "Epoch 12 loss: 0.390592485666275\n",
      "Epoch 13 loss: 0.36112846732139586\n",
      "Epoch 14 loss: 0.3532510221004486\n",
      "Epoch 15 loss: 0.4232593238353729\n",
      "Epoch 16 loss: 0.40438262224197385\n",
      "Epoch 17 loss: 0.372318035364151\n",
      "Epoch 18 loss: 0.3623042583465576\n",
      "Epoch 19 loss: 0.3534569501876831\n",
      "Epoch 20 loss: 0.4087063312530518\n",
      "Epoch 21 loss: 0.33974774479866027\n",
      "Epoch 22 loss: 0.3365471661090851\n",
      "Epoch 23 loss: 0.3191616714000702\n",
      "Epoch 24 loss: 0.29890450835227966\n",
      "Epoch 25 loss: 0.3506736636161804\n",
      "Epoch 26 loss: 0.3467234969139099\n",
      "Epoch 27 loss: 0.317995685338974\n",
      "Epoch 28 loss: 0.3178845465183258\n",
      "Epoch 29 loss: 0.29660236835479736\n",
      "Epoch 30 loss: 0.3254444420337677\n",
      "Epoch 31 loss: 0.3986286997795105\n",
      "Epoch 32 loss: 0.30245853066444395\n",
      "Epoch 33 loss: 0.28588227927684784\n",
      "Epoch 34 loss: 0.3314702928066254\n",
      "Epoch 35 loss: 0.43108431696891786\n",
      "Epoch 36 loss: 0.3300756812095642\n",
      "Epoch 37 loss: 0.3343911826610565\n",
      "Epoch 38 loss: 0.2926682114601135\n",
      "Epoch 39 loss: 0.39249311089515687\n",
      "Epoch 40 loss: 0.33175297975540163\n",
      "Epoch 41 loss: 0.32589096426963804\n",
      "Epoch 42 loss: 0.3487608551979065\n",
      "Epoch 43 loss: 0.3269377529621124\n",
      "Epoch 44 loss: 0.3619072675704956\n",
      "Epoch 45 loss: 0.3542791724205017\n",
      "Epoch 46 loss: 0.3518116056919098\n",
      "Epoch 47 loss: 0.3780890226364136\n",
      "Epoch 48 loss: 0.35391283631324766\n",
      "Epoch 49 loss: 0.3460298299789429\n",
      "Epoch 50 loss: 0.3525032699108124\n",
      "Epoch 51 loss: 0.42144421935081483\n",
      "Epoch 52 loss: 0.4231848955154419\n",
      "Epoch 53 loss: 0.35042551159858704\n",
      "Epoch 54 loss: 0.3667317986488342\n",
      "Epoch 55 loss: 0.39513776898384095\n",
      "Epoch 56 loss: 0.37250648736953734\n",
      "Epoch 57 loss: 0.4165038764476776\n",
      "Epoch 58 loss: 0.38584624528884887\n",
      "Epoch 59 loss: 0.4350834429264069\n",
      "Epoch 60 loss: 0.3996675908565521\n",
      "Epoch 61 loss: 0.4031430959701538\n",
      "Epoch 62 loss: 0.4891513824462891\n",
      "Epoch 63 loss: 0.4300051271915436\n",
      "Epoch 64 loss: 0.41618417501449584\n",
      "Epoch 65 loss: 0.4314471185207367\n",
      "Epoch 66 loss: 0.38994516134262086\n",
      "Epoch 67 loss: 0.43741575479507444\n",
      "Epoch 68 loss: 0.37819170355796816\n",
      "Epoch 69 loss: 0.43985897302627563\n",
      "Epoch 70 loss: 0.42714120745658873\n",
      "Epoch 71 loss: 0.5388075590133667\n",
      "Epoch 72 loss: 0.40728360414505005\n",
      "Epoch 73 loss: 0.4974247753620148\n",
      "Epoch 74 loss: 0.4325538814067841\n",
      "Epoch 75 loss: 0.4286285102367401\n",
      "Epoch 76 loss: 0.43352845311164856\n",
      "Epoch 77 loss: 0.5041579067707062\n",
      "Epoch 78 loss: 0.43618383407592776\n",
      "Epoch 79 loss: 0.46565365195274355\n",
      "Epoch 80 loss: 0.45592126846313474\n",
      "Epoch 81 loss: 0.44806390404701235\n",
      "Epoch 82 loss: 0.4354166805744171\n",
      "Epoch 83 loss: 0.5024714231491089\n",
      "Epoch 84 loss: 0.46823217272758483\n",
      "Epoch 85 loss: 0.4890959084033966\n",
      "Epoch 86 loss: 0.49815213680267334\n",
      "Epoch 87 loss: 0.5156235277652741\n",
      "Epoch 88 loss: 0.4989587128162384\n",
      "Epoch 89 loss: 0.4974174499511719\n",
      "Epoch 90 loss: 0.4928646385669708\n",
      "Epoch 91 loss: 0.5848118126392364\n",
      "Epoch 92 loss: 0.49932810068130495\n",
      "Epoch 93 loss: 0.5318194687366485\n",
      "Epoch 94 loss: 0.6467710733413696\n",
      "Epoch 95 loss: 0.5068363904953003\n",
      "Epoch 96 loss: 0.5489901423454284\n",
      "Epoch 97 loss: 0.525320315361023\n",
      "Epoch 98 loss: 0.5060726225376129\n",
      "Epoch 99 loss: 0.5451704800128937\n"
     ]
    }
   ],
   "source": [
    "# validation crv\n",
    "\n",
    "dataset = ModifiedDataset(SegDataset('IAM', 'val'))\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True, num_workers=0, collate_fn=collate_fn)\n",
    "\n",
    "val_crv = []\n",
    "for epoch in range(100):\n",
    "    model = load_model(model, epoch)\n",
    "    # model.eval()\n",
    "    total_loss = 0\n",
    "    total_step = 0\n",
    "    with torch.no_grad():\n",
    "        for step, (images, targets) in enumerate(dataloader):\n",
    "            images = list(image.to(device) for image in images)\n",
    "            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "            # 计算损失\n",
    "            loss_dict = model(images, targets)\n",
    "            # print(loss_dict)\n",
    "            losses = sum(loss for loss in loss_dict.values())\n",
    "            total_loss += losses.item()\n",
    "            total_step += 1\n",
    "\n",
    "            # print(f\"Epoch {epoch} step {step} loss: {losses.item()}\")\n",
    "    losses = total_loss / total_step\n",
    "    val_crv.append(losses)\n",
    "    print(f\"Epoch {epoch} loss: {losses}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjcAAAHHCAYAAABDUnkqAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjkuMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy80BEi2AAAACXBIWXMAAA9hAAAPYQGoP6dpAACFHElEQVR4nO3dd3hTZR/G8e9J0qZ7AR3MssveIEOGgCwRFAEBFRBFBBVUXC8uXLjAiaKiIIqiyBAZskH2Lnvv0cHq3snz/nHaQGiBFtIEyu9zXbloTk7OeXIamjvP1JRSCiGEEEKIIsLg6gIIIYQQQjiShBshhBBCFCkSboQQQghRpEi4EUIIIUSRIuFGCCGEEEWKhBshhBBCFCkSboQQQghRpEi4EUIIIUSRIuFGCCGEEEWKhBshioiYmBgeeughihUrhqZpfP75564ukhB3hAEDBhAeHn5Dz23dujWtW7d2aHmEhBtxHZMnT0bTNDZv3uzqouRLZGQkjzzyCGXKlMFsNhMUFES7du2YNGkSFovF1cUrVM8//zwLFy7ktdde45dffqFjx47X3D8tLY3PPvuMJk2a4O/vj4eHB1WqVOGZZ57hwIEDTiq14505c4a3336byMjIfO2f8x7P6/bqq686tGzz58/n7bffdugxr2fAgAF2r8lsNlOlShXefPNN0tLSCnSsWbNm0alTJ4oXL467uzslS5akV69eLFu2zLbPihUrbOfasmVLnuXx8fGx29a6dWs0TaNr16659j927BiapvHpp59et3w5533iiSfyfHzUqFG2fc6dO3fd44nbl8nVBRDCUSZOnMiQIUMICQnh0UcfpXLlyiQmJrJ06VIGDRpEVFQU//vf/1xdzEKzbNkyunXrxsiRI6+777lz5+jYsSNbtmzhvvvuo2/fvvj4+LB//36mTZvG999/T0ZGhhNK7Xhnzpxh9OjRhIeHU7du3Xw/75133qF8+fJ222rWrOnQss2fP5/x48c7PeCYzWYmTpwIQHx8PH///Tfvvvsuhw8fZurUqdd9vlKKxx9/nMmTJ1OvXj1eeOEFQkNDiYqKYtasWbRt25Y1a9bQrFkzu+e9/fbb/PPPP/ku59y5c9myZQsNGjQo2Au8jIeHBzNmzOCbb77B3d3d7rHff/8dDw+PAoc6cfuRcCOKhPXr1zNkyBCaNm3K/Pnz8fX1tT02YsQINm/ezK5duxxyruTkZLy9vR1yLEeKjY0lICAgX/sOGDCAbdu28ddff9GjRw+7x959911GjRrlkDJlZWVhtVpzfcjArXcdO3XqRMOGDV1djAJTSpGWloanp+dV9zGZTDzyyCO2+0OHDqVZs2b8/vvvjBs3jpCQkGueY+zYsUyePJkRI0Ywbtw4NE2zPTZq1Ch++eUXTCb7j5O6desyd+5ctm7dSv369a/7OsqWLUtiYiKjR49mzpw5193/ajp27MicOXNYsGAB3bp1s21fu3YtR48epUePHsyYMeOGjy9uD9IsJRxi27ZtdOrUCT8/P3x8fGjbti3r16+32yczM5PRo0dTuXJlPDw8KFasGC1atGDx4sW2faKjoxk4cCClS5fGbDYTFhZGt27dOHbs2DXPP3r0aDRNY+rUqXbBJkfDhg0ZMGAAcKnafMWKFXb75FR/T5482bYtpwr98OHDdO7cGV9fX/r168czzzyDj48PKSkpuc7Vp08fQkND7ZrBFixYwN133423tze+vr506dKF3bt3X/M15Thy5Ag9e/YkKCgILy8v7rrrLubNm2d7PKdZRSnF+PHjbdXuV7NhwwbmzZvHoEGDcgUb0L/lX94EcLU+AVf2M7i8+eDzzz+nYsWKmM1m9uzZw9tvv42maezZs4e+ffsSGBhIixYtbM/99ddfadCgAZ6engQFBfHwww9z8uRJu/O1bt2amjVrsmfPHtq0aYOXlxelSpXi448/tu2zYsUKGjVqBMDAgQNt1+Ly32lBHT9+nKFDh1K1alU8PT0pVqwYPXv2zPWevN77e8CAAYwfPx7Arpkoh9Vq5fPPP6dGjRp4eHgQEhLCU089xcWLF+3OEx4ezn333cfChQtp2LAhnp6efPfddwV6TZqm0aJFC5RSHDly5Jr7pqamMmbMGCIiIvj000/zfG89+uijNG7c2G7bs88+S2BgYL5rqXx9fXn++ef5559/2Lp1a75fy5VKlSpFy5Yt+e233+y2T506lVq1al21Nm769Om292Dx4sV55JFHOH36dK79Zs+eTc2aNfHw8KBmzZrMmjUrz+Pl9/cpCoeEG3HTdu/ezd1338327dt5+eWXeeONNzh69CitW7dmw4YNtv3efvttRo8eTZs2bfj6668ZNWoUZcuWtftD1qNHD2bNmsXAgQP55ptveO6550hMTOTEiRNXPX9KSgpLly6lZcuWlC1b1uGvLysriw4dOhAcHMynn35Kjx496N27N8nJyXYhI6cs//zzDw899BBGoxGAX375hS5duuDj48NHH33EG2+8wZ49e2jRosV1Q1tMTAzNmjVj4cKFDB06lPfff5+0tDTuv/9+2x/Vli1b8ssvvwDQvn17fvnlF9v9vOR8K3700Udv9JJc06RJk/jqq68YPHgwY8eOJSgoyPZYz549SUlJ4YMPPuDJJ58E4P333+exxx6jcuXKjBs3jhEjRth+n3FxcXbHvnjxIh07dqROnTqMHTuWiIgIXnnlFRYsWABAtWrVeOeddwAYPHiw7Vq0bNnyuuWOj4/n3LlzdjeATZs2sXbtWh5++GG+/PJLhgwZwtKlS2ndurVduL3e+/upp56iffv2ALZyXf57euqpp3jppZdo3rw5X3zxBQMHDmTq1Kl06NCBzMxMu7Lu37+fPn360L59e7744osCNb/lyHnvBQYGXnO/1atXc+HCBfr27Wt7T+eHn59fgcPK8OHDCxSIrqZv3778888/JCUlAfr/4enTp9O3b9889588eTK9evXCaDQyZswYnnzySWbOnEmLFi3s3oOLFi2iR48eaJrGmDFj6N69OwMHDsyzT2JBfp+iECghrmHSpEkKUJs2bbrqPt27d1fu7u7q8OHDtm1nzpxRvr6+qmXLlrZtderUUV26dLnqcS5evKgA9cknnxSojNu3b1eAGj58eL72X758uQLU8uXL7bYfPXpUAWrSpEm2bf3791eAevXVV+32tVqtqlSpUqpHjx522//8808FqP/++08ppVRiYqIKCAhQTz75pN1+0dHRyt/fP9f2K40YMUIBatWqVbZtiYmJqnz58io8PFxZLBbbdkANGzbsuq//gQceUIC6ePHidfdVSqlWrVqpVq1a5drev39/Va5cOdv9nOvn5+enYmNj7fZ96623FKD69Oljt/3YsWPKaDSq999/3277zp07lclkstveqlUrBagpU6bYtqWnp6vQ0FC738OmTZty/R6vJec9ntdNKaVSUlJyPWfdunW5ynK997dSSg0bNkzl9Wd31apVClBTp0612/7vv//m2l6uXDkFqH///Tdfr69///7K29tbnT17Vp09e1YdOnRIffrpp0rTNFWzZk1ltVqv+fwvvvhCAWrWrFn5Ol/O/6/p06eruLg4FRgYqO6///5c5blcq1atVI0aNZRSSo0ePVoBasuWLUqpS++r/PxdyPk/cOHCBeXu7q5++eUXpZRS8+bNU5qmqWPHjtnei2fPnlVKKZWRkaGCg4NVzZo1VWpqqu1Yc+fOVYB68803bdvq1q2rwsLCVFxcnG3bokWLFGD3f6Egv8+r/f8SN0dqbsRNsVgsLFq0iO7du1OhQgXb9rCwMPr27cvq1atJSEgAICAggN27d3Pw4ME8j+Xp6Ym7uzsrVqwoUNVtzvHzao5ylKefftruvqZp9OzZk/nz59u+HQL88ccflCpVytbksnjxYuLi4ujTp49djYDRaKRJkyYsX778muedP38+jRs3tmvC8fHxYfDgwRw7dow9e/YU+LUU9vXq0aMHJUqUyPOxIUOG2N2fOXMmVquVXr162V2f0NBQKleunOv6+Pj42PUdcXd3p3HjxtdtWsmP8ePHs3jxYrsbYNeXJTMzk/Pnz1OpUiUCAgLsaiSu9/6+lunTp+Pv70/79u3trkODBg3w8fHJdR3Kly9Phw4d8n385ORkSpQoQYkSJahUqRIjR46kefPm/P3339dswoSbe7/4+/szYsQI5syZw7Zt2/L1nJzam9GjRxf4fDkCAwPp2LEjv//+OwC//fYbzZo1o1y5crn23bx5M7GxsQwdOhQPDw/b9i5duhAREWGrnY2KiiIyMpL+/fvj7+9v2699+/ZUr17d7pgF/X0Kx5NwI27K2bNnSUlJoWrVqrkeq1atGlar1dZ34p133iEuLo4qVapQq1YtXnrpJXbs2GHb32w289FHH7FgwQJCQkJo2bIlH3/8MdHR0dcsg5+fHwCJiYkOfGWXmEwmSpcunWt77969SU1NtTXzJCUlMX/+fHr27Gn7wMj5oLvnnntsHy45t0WLFhEbG3vNcx8/fvyq1zbn8YIq7Ot15Yijaz128OBBlFJUrlw51/XZu3dvrutTunTpXB/GgYGBDunH0LhxY9q1a2d3A73PyZtvvmmbXqB48eKUKFGCuLg44uPjbc+/3vv7Wg4ePEh8fDzBwcG5rkNSUlKu63Cta5wXDw8PW2CbNGkS1apVIzY21i64JSUlER0dbbudPXsWuPn3y/DhwwkICMh3U9ONBKK89O3bl8WLF3PixAlmz5591SapnP9Def0/i4iIsD2e82/lypVz7Xflcwv6+xSOJ6OlhNO0bNmSw4cP8/fff7No0SImTpzIZ599xoQJE2zzUowYMYKuXbsye/ZsFi5cyBtvvMGYMWNYtmwZ9erVy/O4lSpVwmQysXPnznyV42rfVK82D47ZbMZgyP094K677iI8PJw///zT1safmppK7969bftYrVZA72MRGhqa6xhXjjBxhoiICAB27tzJ3Xfffd39czorX+lq1+tao3aufMxqtaJpGgsWLMizP8eV86Fcrc9HXuVzlGeffZZJkyYxYsQImjZtir+/P5qm8fDDD9t+v5C/9/fVWK1WgoODrzos+8qasGtd47wYjUZbWAPo0KEDERERPPXUU7Zw/umnn9rVlpQrV45jx47ZvV+6d+9eoPPCpbDy9ttvF6j25rPPPmP06NE3PBnl/fffj9lspn///qSnp9OrV68bOs6NKOjvUziehBtxU0qUKIGXlxf79+/P9di+ffswGAyUKVPGti0oKIiBAwcycOBAkpKSaNmyJW+//bbdH/+KFSvy4osv8uKLL3Lw4EHq1q3L2LFj+fXXX/Msg5eXF/fccw/Lli3j5MmTdufLS04Hyis7q95ILUivXr344osvSEhI4I8//iA8PJy77rrL7rUABAcH23245Fe5cuWuem1zHi+orl27MmbMGH799dd8hZvAwMA8m31u5HpdqWLFiiilKF++PFWqVLnp48HVw+uN+uuvv+jfvz9jx461bUtLS8v1/oHrv7+vVraKFSuyZMkSmjdvXuDgciPCwsJ4/vnnGT16NOvXr+euu+7iscces2v+zClHixYtCAwM5Pfff+d///tfgToV5xgxYgSff/45o0ePztd0BZcHov79+xf4fKCXv3v37vz666+2iQfzkvN/aP/+/dxzzz12j+3fv9/2eM6/eTU7Xvl/1Nm/T5GbNEuJm2I0Grn33nv5+++/7Ub+xMTE8Ntvv9GiRQtbtfb58+ftnuvj40OlSpVIT08H9JFGV06uVbFiRXx9fW37XM1bb72FUopHH33Urg9Mji1btvDzzz8D+h8po9HIf//9Z7fPN998k78XfZnevXuTnp7Ozz//zL///pvr22GHDh3w8/Pjgw8+yHOERE7V/9V07tyZjRs3sm7dOtu25ORkvv/+e8LDw3O19edH06ZN6dixIxMnTmT27Nm5Hs/IyLCbCLBixYrs27fPrqzbt29nzZo1BT73lR588EGMRiOjR4/OVfuilMr1nsmPnLlz8gofN8JoNOYq21dffZWr5up67+9rla1Xr15YLBbefffdXOfPyspy2Gu53LPPPouXlxcffvghABUqVLBrkmvevDmgf3l45ZVX2Lt3L6+88kqetWS//vorGzduvOq5csLK33//ne+Zo0eMGEFAQIBt9NuNGDlyJG+99RZvvPHGVfdp2LAhwcHBTJgwwe53tWDBAvbu3UuXLl0APRDWrVuXn3/+2a45cvHixbn6vrni9ynsSc2NyJeffvqJf//9N9f24cOH895777F48WJatGjB0KFDMZlMfPfdd6Snp9vNQVK9enVat25NgwYNCAoKYvPmzfz1118888wzABw4cIC2bdvSq1cvqlevjslkYtasWcTExPDwww9fs3zNmjVj/PjxDB06lIiICLsZilesWMGcOXN47733AP0Pbc+ePfnqq6/QNI2KFSsyd+7cG2oHr1+/PpUqVWLUqFGkp6fbNUmB3l/h22+/5dFHH6V+/fo8/PDDlChRghMnTjBv3jyaN2/O119/fdXjv/rqq/z+++906tSJ5557jqCgIH7++WeOHj3KjBkz8mwuy48pU6Zw77338uCDD9K1a1fatm2Lt7c3Bw8eZNq0aURFRdnmunn88ccZN24cHTp0YNCgQcTGxjJhwgRq1Khh62x6oypWrMh7773Ha6+9xrFjx+jevTu+vr4cPXqUWbNmMXjw4HzNuHzlMQMCApgwYQK+vr54e3vTpEmTAvdTyXHffffxyy+/4O/vT/Xq1Vm3bh1LliyhWLFidvtd7/0N2Gbefe655+jQoQNGo5GHH36YVq1a8dRTTzFmzBgiIyO59957cXNz4+DBg0yfPp0vvviChx566IbKfzXFihWzTbmwd+9eWz+uvLz00kvs3r2bsWPHsnz5ch566CFCQ0OJjo5m9uzZbNy4kbVr117zfDlNTdu3b8/X5I3+/v4MHz78pjoW16lThzp16lxzHzc3Nz766CMGDhxIq1at6NOnDzExMXzxxReEh4fz/PPP2/YdM2YMXbp0oUWLFjz++ONcuHCBr776iho1ath9qXLF71NcwWXjtMRt4VrDZAF18uRJpZRSW7duVR06dFA+Pj7Ky8tLtWnTRq1du9buWO+9955q3LixCggIUJ6enioiIkK9//77KiMjQyml1Llz59SwYcNURESE8vb2Vv7+/qpJkybqzz//zHd5t2zZovr27atKliyp3NzcVGBgoGrbtq36+eef7YZNnz17VvXo0UN5eXmpwMBA9dRTT6ldu3blORT8ymGrVxo1apQCVKVKla66z/Lly1WHDh2Uv7+/8vDwUBUrVlQDBgxQmzdvvu5rOnz4sHrooYdUQECA8vDwUI0bN1Zz587NtR/5HAqeIyUlRX366aeqUaNGysfHR7m7u6vKlSurZ599Vh06dMhu319//VVVqFBBubu7q7p166qFCxdedSh4XkN2rxx+e6UZM2aoFi1aKG9vb+Xt7a0iIiLUsGHD1P79+237XD5c+HJXlkMppf7++29VvXp1ZTKZrjss/HrTHVy8eFENHDhQFS9eXPn4+KgOHTqoffv2qXLlyqn+/fvb9rve+1sppbKystSzzz6rSpQooTRNyzUs/Pvvv1cNGjRQnp6eytfXV9WqVUu9/PLL6syZM7Z9ypUrd90h51den6u9hw8fPqyMRqPd67iWv/76S917770qKChImUwmFRYWpnr37q1WrFhh2+fyoeBXynkfXGso+OUuXryo/P39CzwU/Fqu9l78448/VL169ZTZbFZBQUGqX79+6tSpU7meP2PGDFWtWjVlNptV9erV1cyZM/N8DyqVv9+nDAUvHJpShdgTTwghhBDCyaTPjRBCCCGKFAk3QgghhChSJNwIIYQQokiRcCOEEEKIIkXCjRBCCCGKFAk3QgghhChS7rhJ/KxWK2fOnMHX19fh07QLIYQQonAopUhMTKRkyZLXncD0jgs3Z86cue7aQ0IIIYS4NZ08eZLSpUtfc587Ltz4+voC+sXJWfNICCGEELe2hIQEypQpY/scv5Y7LtzkNEX5+flJuBFCCCFuM/npUiIdioUQQghRpEi4EUIIIUSRIuFGCCGEEEXKHdfnRgghRNFhsVjIzMx0dTGEg7i7u193mHd+SLgRQghx21FKER0dTVxcnKuLIhzIYDBQvnx53N3db+o4Em6EEELcdnKCTXBwMF5eXjIpaxGQM8luVFQUZcuWvanfqYQbIYQQtxWLxWILNsWKFXN1cYQDlShRgjNnzpCVlYWbm9sNH0c6FAshhLit5PSx8fLycnFJhKPlNEdZLJabOo6EGyGEELclaYoqehz1O5VwI4QQQogiRcKNEEIIcRsLDw/n888/d3UxbikSboQQQggn0DTtmre33377ho67adMmBg8e7NjC3uZktJSjZGVAciwoBQFlXF0aIYQQt5ioqCjbz3/88Qdvvvkm+/fvt23z8fGx/ayUwmKxYDJd/2O6RIkSji1oESA1N45yegt8VgOm3O/qkgghhLgFhYaG2m7+/v5omma7v2/fPnx9fVmwYAENGjTAbDazevVqDh8+TLdu3QgJCcHHx4dGjRqxZMkSu+Ne2SylaRoTJ07kgQcewMvLi8qVKzNnzhwnv1rXknDjKKbs2RSzMlxbDiGEuAMppUjJyHLJTSnlsNfx6quv8uGHH7J3715q165NUlISnTt3ZunSpWzbto2OHTvStWtXTpw4cc3jjB49ml69erFjxw46d+5Mv379uHDhgsPKeauTZilHMXno/2alubYcQghxB0rNtFD9zYUuOfeedzrg5e6Yj9N33nmH9u3b2+4HBQVRp04d2/13332XWbNmMWfOHJ555pmrHmfAgAH06dMHgA8++IAvv/ySjRs30rFjR4eU81YnNTeOYjTr/1qk5kYIIcSNadiwod39pKQkRo4cSbVq1QgICMDHx4e9e/det+amdu3atp+9vb3x8/MjNja2UMp8K5KaG0cxZYcbqbkRQgin83QzsuedDi47t6N4e3vb3R85ciSLFy/m008/pVKlSnh6evLQQw+RkXHtL9JXLl2gaRpWq9Vh5bzVSbhxFNNlNTdKgcycKYQQTqNpmsOahm4la9asYcCAATzwwAOAXpNz7Ngx1xbqNiDNUo6SE24AstJdVw4hhBBFRuXKlZk5cyaRkZFs376dvn373lE1MDdKwo2DZGmXVQFaJNwIIYS4eePGjSMwMJBmzZrRtWtXOnToQP369V1drFuephw5hu02kJCQgL+/P/Hx8fj5+TnsuJuOnqfRzxX0OyMPgY9MqiSEEIUhLS2No0ePUr58eTw8PFxdHOFA1/rdFuTzW2puHMTsZiRdZbf3SqdiIYQQwmUk3DiIu8lAOtlNUzIcXAghhHAZCTcOYjYZycgJN1JzI4QQQriMhBsHMV9ecyOjpYQQQgiXkXDjIGaTgYzsPjfWTKm5EUIIIVxFwo2DmN2MpKMvnpmVIeFGCCGEcBUJNw5iNhnIyJ7wOTMj1cWlEUIIIe5cEm4cxGTQbH1upOZGCCGEcB0JNw6iaZptluKsdKm5EUIIIVxFwo0DZWk5fW5ktJQQQgjHa926NSNGjLDdDw8P5/PPP7/mczRNY/bs2Td9bkcdxxkk3DhQTrixZErNjRBCCHtdu3alY8eOeT62atUqNE1jx44dBTrmpk2bGDx4sCOKZ/P2229Tt27dXNujoqLo1KmTQ89VWCTcOJDFoDdLyVBwIYQQVxo0aBCLFy/m1KlTuR6bNGkSDRs2pHbt2gU6ZokSJfDy8nJUEa8pNDQUs9nslHPdLAk3DpRl0H/pVulQLIQQ4gr33XcfJUqUYPLkyXbbk5KSmD59Ot27d6dPnz6UKlUKLy8vatWqxe+//37NY17ZLHXw4EFatmyJh4cH1atXZ/Hixbme88orr1ClShW8vLyoUKECb7zxBpmZmQBMnjyZ0aNHs337djRNQ9M0W3mvbJbauXMn99xzD56enhQrVozBgweTlJRke3zAgAF0796dTz/9lLCwMIoVK8awYcNs5ypMpkI/wx3Emt2h2CozFAshhHMpBZkprjm3mxdo2nV3M5lMPPbYY0yePJlRo0ahZT9n+vTpWCwWHnnkEaZPn84rr7yCn58f8+bN49FHH6VixYo0btz4use3Wq08+OCDhISEsGHDBuLj4+365+Tw9fVl8uTJlCxZkp07d/Lkk0/i6+vLyy+/TO/evdm1axf//vsvS5YsAcDf3z/XMZKTk+nQoQNNmzZl06ZNxMbG8sQTT/DMM8/Yhbfly5cTFhbG8uXLOXToEL1796Zu3bo8+eST1309N0PCjQNZjNk1N9IsJYQQzpWZAh+UdM25/3cG3L3ztevjjz/OJ598wsqVK2ndujWgN0n16NGDcuXKMXLkSNu+zz77LAsXLuTPP//MV7hZsmQJ+/btY+HChZQsqV+LDz74IFc/mddff932c3h4OCNHjmTatGm8/PLLeHp64uPjg8lkIjQ09Krn+u2330hLS2PKlCl4e+uv/euvv6Zr16589NFHhISEABAYGMjXX3+N0WgkIiKCLl26sHTp0kIPN9Is5UDKqHcoVplScyOEECK3iIgImjVrxk8//QTAoUOHWLVqFYMGDcJisfDuu+9Sq1YtgoKC8PHxYeHChZw4cSJfx967dy9lypSxBRuApk2b5trvjz/+oHnz5oSGhuLj48Prr7+e73Ncfq46derYgg1A8+bNsVqt7N+/37atRo0aGI1G2/2wsDBiY2MLdK4bITU3DmTN7nMjq4ILIYSTuXnpNSiuOncBDBo0iGeffZbx48czadIkKlasSKtWrfjoo4/44osv+Pzzz6lVqxbe3t6MGDGCjIwMhxV13bp19OvXj9GjR9OhQwf8/f2ZNm0aY8eOddg5Lufm5mZ3X9M0rFZroZzrchJuHCin5gaL496IQggh8kHT8t005Gq9evVi+PDh/Pbbb0yZMoWnn34aTdNYs2YN3bp145FHHgH0PjQHDhygevXq+TputWrVOHnyJFFRUYSFhQGwfv16u33Wrl1LuXLlGDVqlG3b8ePH7fZxd3fHYrFc91yTJ08mOTnZVnuzZs0aDAYDVatWzVd5C5M0SzmQMnnoP0jNjRBCiKvw8fGhd+/evPbaa0RFRTFgwAAAKleuzOLFi1m7di179+7lqaeeIiYmJt/HbdeuHVWqVKF///5s376dVatW2YWYnHOcOHGCadOmcfjwYb788ktmzZplt094eDhHjx4lMjKSc+fOkZ6eu6tFv3798PDwoH///uzatYvly5fz7LPP8uijj9r627iShBtHkpobIYQQ+TBo0CAuXrxIhw4dbH1kXn/9derXr0+HDh1o3bo1oaGhdO/ePd/HNBgMzJo1i9TUVBo3bswTTzzB+++/b7fP/fffz/PPP88zzzxD3bp1Wbt2LW+88YbdPj169KBjx460adOGEiVK5Dkc3cvLi4ULF3LhwgUaNWrEQw89RNu2bfn6668LfjEKgaaUUq4uhDMlJCTg7+9PfHw8fn5+Dj32jIlj6HHqQ44GtqD88HkOPbYQQghdWloaR48epXz58nh4eLi6OMKBrvW7Lcjnt9TcOJBm0mtuNKvU3AghhBCuIuHGgbTsPjcGiwwFF0IIIVxFwo0DGdz0oeAGqbkRQgghXEbCjQNpbnrNjVFqboQQQgiXkXDjQAZTds2NKvxFwYQQ4k53h42HuSM46ncq4caBDO6eAJisUnMjhBCFJWfW25QUFy2UKQpNzmzMly/ZcCNkhmIHMrrrNTcmq9TcCCFEYTEajQQEBNjWKPLy8rKtsC1uX1arlbNnz+Ll5YXJdHPxRMKNA5ncsmtulHQoFkKIwpSzYrUzFmEUzmMwGChbtuxNh1UJNw5kctc7FJukz40QQhQqTdMICwsjODiYzEz5m1tUuLu7YzDcfI8ZCTcOZDLrNTdmMkApfSE3IYQQhcZoNN50/wxR9EiHYgcyuV02VbRFvkkIIYQQriDhxoHcPDwv3ZGVwYUQQgiXkHDjQO7my8KNrAwuhBBCuISEGwcyu5nIUNltv1JzI4QQQriEhBsHMpuMZKBPLkWWTOQnhBBCuIKEGwcyuxlIzwk30iwlhBBCuISEGwdyNxpsNTdZGakuLo0QQghxZ5Jw40BmNwPpKifcSJ8bIYQQwhUk3DiQXnOjz4uYmSY1N0IIIYQrSLhxIJPRQAbuAGRKzY0QQgjhEhJuHCxTkz43QgghhCtJuHGwLE2vubFIzY0QQgjhEhJuHCwru+bGkinhRgghhHAFCTcOlmWQmhshhBDClSTcOJglO9xYM6XPjRBCCOEKEm4c7FK4keUXhBBCCFeQcONgl8KNNEsJIYQQriDhxsGsRjMAShbOFEIIIVxCwo2DWbNrbiTcCCGEEK7h0nAzZswYGjVqhK+vL8HBwXTv3p39+/df93nTp08nIiICDw8PatWqxfz5851Q2vxRUnMjhBBCuJRLw83KlSsZNmwY69evZ/HixWRmZnLvvfeSnJx81eesXbuWPn36MGjQILZt20b37t3p3r07u3btcmLJr04Z9ZobLUv63AghhBCuoCmllKsLkePs2bMEBwezcuVKWrZsmec+vXv3Jjk5mblz59q23XXXXdStW5cJEyZc9xwJCQn4+/sTHx+Pn5+fw8qeY+6E17gv+hv2B3em6tDfHX58IYQQ4k5UkM/vW6rPTXx8PABBQUFX3WfdunW0a9fObluHDh1Yt25dnvunp6eTkJBgdytU2c1SmkWapYQQQghXuGXCjdVqZcSIETRv3pyaNWtedb/o6GhCQkLstoWEhBAdHZ3n/mPGjMHf3992K1OmjEPLnYvJAwDNklG45xFCCCFEnm6ZcDNs2DB27drFtGnTHHrc1157jfj4eNvt5MmTDj3+lTQ3vc+NwSJ9boQQQghXMLm6AADPPPMMc+fO5b///qN06dLX3Dc0NJSYmBi7bTExMYSGhua5v9lsxmw2O6ys12XyBMBgzXTeOYUQQghh49KaG6UUzzzzDLNmzWLZsmWUL1/+us9p2rQpS5cutdu2ePFimjZtWljFLBCDmx6kDNLnRgghhHAJl9bcDBs2jN9++42///4bX19fW78Zf39/PD31GpDHHnuMUqVKMWbMGACGDx9Oq1atGDt2LF26dGHatGls3ryZ77//3mWv43I54caopM+NEEII4Qourbn59ttviY+Pp3Xr1oSFhdluf/zxh22fEydOEBUVZbvfrFkzfvvtN77//nvq1KnDX3/9xezZs6/ZCdmZjNkdio1WCTdCCCGEK7i05iY/U+ysWLEi17aePXvSs2fPQijRzTO66+HGJH1uhBBCCJe4ZUZLFRW2cKOkz40QQgjhChJuHMzolhNupOZGCCGEcAUJNw5mMusdod0k3AghhBAuIeHGwUzZzVJuZMKts2yXEEIIcceQcONgbtk1N0asYM1ycWmEEEKIO4+EGwdzc/e8dCdLOhULIYQQzibhxsHcPTwu3ZFwI4QQQjidhBsHc3dzJ0tlX1ZZgkEIIYRwOgk3DmZ2M5COGwAqU1YGF0IIIZxNwo2DmU1GMrLDTWaGhBshhBDC2STcOJjZdKnmJjM91cWlEUIIIe48Em4czGwykKH0JbsyMyTcCCGEEM4m4cbBNE0jQ3MHICtdmqWEEEIIZ5NwUwgys5ulsqTPjRBCCOF0Em4KQWZOzY00SwkhhBBOJ+GmEGRpes2NRWpuhBBCCKeTcFMIsgzZNTcyz40QQgjhdBJuCoElu1nKKjU3QgghhNNJuCkEFmN2uJGaGyGEEMLpJNwUAlvNTaasLSWEEEI4m4SbQmDNrrlRWVJzI4QQQjibhJtCYAs30iwlhBBCOJ2Em0KgDB76v1kZLi6JEEIIceeRcFMIVHbNDRapuRFCCCGcTcJNIVAmMwCa1NwIIYQQTifhpjBIzY0QQgjhMhJuCoNJ73OjWaTmRgghhHA2CTeFIbtZymCReW6EEEIIZ5NwUwg0t+w+N1JzI4QQQjidhJtCYMhuljJaJdwIIYQQzibhphAY3PVwY5BwI4QQQjidhJtCoGX3uTFZpc+NEEII4WwSbgqB0T2nWSrTxSURQggh7jwSbgqB0U0PNyYlzVJCCCGEs0m4KQRGd08ATEpqboQQQghnk3BTCEzZzVJuUnMjhBBCOJ2Em0LgZtZrbtyk5kYIIYRwOgk3hSCnz40bUnMjhBBCOJuEm0Lg5pHd5wYrWLJcXBohhBDiziLhphC4Z3coBkDWlxJCCCGcSsJNIXD3uCzcZEm4EUIIIZxJwk0hMLu7Y1EaANbMNBeXRgghhLizSLgpBGY3I+m4A5CZIeFGCCGEcCYJN4XA3WggAxMAGWmpLi6NEEIIcWeRcFMI3Iwa6bgBkJkh4UYIIYRwJgk3hUDTNDJzwk26NEsJIYQQziThppBkaNl9btKl5kYIIYRwJgk3hSQru+YmS0ZLCSGEEE4l4aaQZBr0mhuL9LkRQgghnErCTSHJ0vSaG0uGTOInhBBCOJOEm0KSld3nxpIpNTdCCCGEM0m4KSSW7GYpq9TcCCGEEE4l4aaQWA16s5QsvyCEEEI4l4SbQmIxmAGwysKZQgghhFNJuCkkVqPeLKWkz40QQgjhVBJuCoky6jU3ZGW4tiBCCCHEHUbCTSGxZncoJkv63AghhBDOJOGmkCiTXnOjpOZGCCGEcCoJN4Ulu1lKs0jNjRBCCOFMEm4Ki8kDAM0iNTdCCCGEM0m4KSSaSe9zo1lkKLgQQgjhTBJuCoubXnNjkJobIYQQwqkk3BQSLbvPjcEqNTdCCCGEM0m4KSQGd6m5EUIIIVxBwk0hMWQPBTcpCTdCCCGEM0m4KSQ5NTdGq4QbIYQQwpkk3BQSY3aHYpOEGyGEEMKpJNwUEqO7p/6vynRxSYQQQog7i4SbQmJy0/vcuEmfGyGEEMKpXBpu/vvvP7p27UrJkiXRNI3Zs2dfc/8VK1agaVquW3R0tHMKXAAms15z4yY1N0IIIYRTuTTcJCcnU6dOHcaPH1+g5+3fv5+oqCjbLTg4uJBKeOPcspul3JCaGyGEEMKZTK48eadOnejUqVOBnxccHExAQIDjC+RAbma9Q7E7UnMjhBBCONNt2eembt26hIWF0b59e9asWXPNfdPT00lISLC7OYNbTrMUFrBanXJOIYQQQtxm4SYsLIwJEyYwY8YMZsyYQZkyZWjdujVbt2696nPGjBmDv7+/7VamTBmnlNXdw+vSHVk8UwghhHAaTSmlXF0IAE3TmDVrFt27dy/Q81q1akXZsmX55Zdf8nw8PT2d9PRL4SIhIYEyZcoQHx+Pn5/fzRT5mi4mJBE4rhQAWSOPYvIJKrRzCSGEEEVdQkIC/v7++fr8dmmfG0do3Lgxq1evvurjZrMZs9nsxBJdOq9VaRg0RUZG6u1/oYUQQojbxG3VLJWXyMhIwsLCXF2MXNxNRjKyI01mWqqLSyOEEELcOVxaoZCUlMShQ4ds948ePUpkZCRBQUGULVuW1157jdOnTzNlyhQAPv/8c8qXL0+NGjVIS0tj4sSJLFu2jEWLFrnqJVyVyWggGXc8yCQjPc3VxRFCCCHuGC4NN5s3b6ZNmza2+y+88AIA/fv3Z/LkyURFRXHixAnb4xkZGbz44oucPn0aLy8vateuzZIlS+yOcSvJzKm5SU9xcUmEEEKIO8ct06HYWQrSIelmnXm7EiU5y4kH/6Fs7ZaFei4hhBCiKCvI5/dt3+fmVpaJGwBZGdIsJYQQQjiLhJtClGlwByArQzoUCyGEEM4i4aYQZWl6zY0lQybxE0IIIZxFwk0hytL0mhtLptTcCCGEEM4i4aYQWQw54UZqboQQQghnkXBTiHLCjVU6FAshhBBOI+GmEOWEGyXNUkIIIYTT3FC4OXnyJKdOnbLd37hxIyNGjOD77793WMGKggyjDwBaRqKLSyKEEELcOW4o3PTt25fly5cDEB0dTfv27dm4cSOjRo3inXfecWgBb2dZ7r76D2nxri2IEEIIcQe5oXCza9cuGjduDMCff/5JzZo1Wbt2LVOnTmXy5MmOLN9tzWr2139IT3BtQYQQQog7yA2Fm8zMTMxmMwBLlizh/vvvByAiIoKoqCjHle42p3no4cYo4UYIIYRwmhsKNzVq1GDChAmsWrWKxYsX07FjRwDOnDlDsWLFHFrA25nBUw83pkwJN0IIIYSz3FC4+eijj/juu+9o3bo1ffr0oU6dOgDMmTPH1lwlwOQVAIA5SzoUCyGEEM5iupEntW7dmnPnzpGQkEBgYKBt++DBg/Hy8nJY4W537j5BAHhYklxcEiGEEOLOcUM1N6mpqaSnp9uCzfHjx/n888/Zv38/wcHBDi3g7czso18fT6uEGyGEEMJZbijcdOvWjSlTpgAQFxdHkyZNGDt2LN27d+fbb791aAFvZ55+es2Nj0p2cUmEEEKIO8cNhZutW7dy9913A/DXX38REhLC8ePHmTJlCl9++aVDC3g78/HXO1e7kyWzFAshhBBOckPhJiUlBV9ffYK6RYsW8eCDD2IwGLjrrrs4fvy4Qwt4O/P1C8CiNABSEy+4uDRCCCHEneGGwk2lSpWYPXs2J0+eZOHChdx7770AxMbG4ufn59AC3s68zG4koXewTo6XcCOEEEI4ww2FmzfffJORI0cSHh5O48aNadq0KaDX4tSrV8+hBbydaZpGkuYNQGrCeReXRgghhLgz3NBQ8IceeogWLVoQFRVlm+MGoG3btjzwwAMOK1xRkGLwBiukJl50dVGEEEKIO8INhRuA0NBQQkNDbauDly5dWibwy0Oq0QeskJkszVJCCCGEM9xQs5TVauWdd97B39+fcuXKUa5cOQICAnj33XexWq2OLuNtLcOod7zOTI5zbUGEEEKIO8QN1dyMGjWKH3/8kQ8//JDmzZsDsHr1at5++23S0tJ4//33HVrI21mmmx+kgTU1ztVFEUIIIe4INxRufv75ZyZOnGhbDRygdu3alCpViqFDh0q4uYzF7AuJoFJl8UwhhBDCGW6oWerChQtERETk2h4REcGFC9K35HLKrK8MrqXHu7gkQgghxJ3hhsJNnTp1+Prrr3Nt//rrr6ldu/ZNF6pI8dDDjTFDam6EEEIIZ7ihZqmPP/6YLl26sGTJEtscN+vWrePkyZPMnz/foQW83Rk8AwAwSbgRQgghnOKGam5atWrFgQMHeOCBB4iLiyMuLo4HH3yQ3bt388svvzi6jLc1k5dec+OeJSuDCyGEEM5ww/PclCxZMlfH4e3bt/Pjjz/y/fff33TBigqzj74yuKcl0cUlEUIIIe4MN1RzI/LP7BsIgKc12cUlEUIIIe4MEm4KmadvMQC8kXAjhBBCOIOEm0Lm7Z/dLEUG1ow0F5dGCCGEKPoK1OfmwQcfvObjcXFxN1OWIsnXL9D2c3LiBXyLlXRhaYQQQoiir0Dhxt/f/7qPP/bYYzdVoKLGw+xOovLEV0slKe68hBshhBCikBUo3EyaNKmwylGkJWne+JJKaqLM3iyEEEIUNulz4wQpBh8A0hIvurgkQgghRNEn4cYJ0ox6uMlIkpobIYQQorBJuHGCdJMvAFkpsnimEEIIUdgk3DhBlpsfAJbUONcWRAghhLgDSLhxAou7XnNDmtTcCCGEEIVNwo0TKLNec6NJuBFCCCEKnYQbZ/AIAMCYkeDacgghhBB3AAk3TmD0CgDALVPCjRBCCFHYJNw4gck7AAD3rCTXFkQIIYS4A0i4cQJ3b319KU9LootLIoQQQhR9Em6cwMM3e2Vwa7KLSyKEEEIUfRJunMAre2VwHyXhRgghhChsEm6cwNuvGABeWjqZGekuLo0QQghRtEm4cQIf/yDbz0nxsr6UEEIIUZgk3DiByc2dZOUBQHL8eReXRgghhCjaJNw4SZLmDUBKgtTcCCGEEIVJwo2TpBh8AEhPknAjhBBCFCYJN06SZtTDTUZynGsLIoQQQhRxEm6cJMOkh5tMCTdCCCFEoZJw4yRZ7vrK4NbUONcWRAghhCjiJNw4iSU73JAW59JyCCGEEEWdhBsnUWZ/ALR0WRlcCCGEKEwSbpzFUw83xgwJN0IIIURhknDjJEbPAADcMmRlcCGEEKIwSbhxEjfvAADcLRJuhBBCiMIk4cZJ3Lz1lcE9LEkuLokQQghRtEm4cRJPXz3ceFuTXVwSIYQQomiTcOMknn7FAPBREm6EEEKIwiThxkl8/IP0f7VU0tLTXVwaIYQQouiScOMkXr5Btp8T42TxTCGEEKKwSLhxEoObmVTMACQnnHdxaYQQQoiiS8KNEyVp3gCkJFx0cUmEEEKIosul4ea///6ja9eulCxZEk3TmD179nWfs2LFCurXr4/ZbKZSpUpMnjy50MvpKCkGfWXw9CQJN0IIIURhcWm4SU5Opk6dOowfPz5f+x89epQuXbrQpk0bIiMjGTFiBE888QQLFy4s5JI6RrpRr7nJTJZwI4QQQhQWkytP3qlTJzp16pTv/SdMmED58uUZO3YsANWqVWP16tV89tlndOjQobCK6TAZJl/IgKwUCTdCCCFEYbmt+tysW7eOdu3a2W3r0KED69atu+pz0tPTSUhIsLu5SqabHwCWlDiXlUEIIYQo6m6rcBMdHU1ISIjdtpCQEBISEkhNTc3zOWPGjMHf3992K1OmjDOKmierWQ83pMnK4EIIIURhua3CzY147bXXiI+Pt91OnjzpsrIosz8AhvR4l5VBCCGEKOpc2uemoEJDQ4mJibHbFhMTg5+fH56ennk+x2w2YzabnVG869I89XBjzJCaGyGEEKKw3FY1N02bNmXp0qV22xYvXkzTpk1dVKKCMXgGAGDKTHRtQYQQQogizKXhJikpicjISCIjIwF9qHdkZCQnTpwA9Calxx57zLb/kCFDOHLkCC+//DL79u3jm2++4c8//+T55593RfELzN07AABzVpJrCyKEEEIUYS4NN5s3b6ZevXrUq1cPgBdeeIF69erx5ptvAhAVFWULOgDly5dn3rx5LF68mDp16jB27FgmTpx4WwwDB3D3CQDAwyo1N0IIIURhcWmfm9atW6OUuurjec0+3Lp1a7Zt21aIpSo8Zh998Uxva7KLSyKEEEIUXbdVn5vbnY9/cf1flUyWxeri0gghhBBFk4QbJwoops/R46elcDZORkwJIYQQhUHCjRMZvIuRjhsAF6JPXGdvIYQQQtwICTfOpGlcNBQDICHWdZMJCiGEEEWZhBsnS3IvAUDaBQk3QgghRGGQcONkaZ56vxtL/GkXl0QIIYQomiTcOJnVJxQAY2K0i0sihBBCFE0SbpzM4F8KAHNqzHX2FEIIIcSNkHDjZB5BpQHwyYh1cUmEEEKIoknCjZP5ligDQKDl/DVnZxZCCCHEjZFw42QBoeUACOYi8SkZLi6NEEIIUfRIuHEyc2B2nxstk9iYKBeXRgghhCh6JNw4m8lMnOYPQFzscRcXRgghhCh6JNy4QLybvoBm6jmZyE8IIYRwNAk3LpBq1ifyy7goE/kJIYQQjibhxgUyvfVwoyVKnxshhBDC0STcuIDmVxIAtxSZpVgIIYRwNAk3LuCWPWLKM00m8hNCCCEcTcKNC3gX1yfyC8g66+KSCCGEEEWPhBsX8A/RJ/IroS6QlmlxcWmEEEKIokXCjQv4ZNfcBGpJxJy/6OLSCCGEEEWLhBsX0DwDSccdgAsxJ1xcGiGEEKJokXDjCprGRWMxAJJiZSI/IYQQwpEk3LhIUvZEfukXT7m4JEIIIUTRIuHGRdI99XBjjT/j4pIIIYQQRYuEGxdRvmEAGJNklmIhhBDCkSTcuIjRX5+l2CMtxsUlEUIIIYoWCTcu4lmsNAC+GTKRnxBCCOFIEm5cxC+4LABB1vNYrMrFpRFCCCGKDgk3LpIzS3EwFzmXmOri0gghhBBFh4QbFzH6hWFFw12zcDZGRkwJIYQQjiLhxlWMbsRrAQDEyyzFQgghhMNIuHGhRPfiAKSdl3AjhBBCOIqEGxdK9dAn8suMk2YpIYQQwlEk3LhQlncoAIZEmchPCCGEcBQJNy5kyJ7Izz0l2sUlEUIIIYoOCTcu5B6oT+TnnRHr4pIIIYQQRYeEGxfyLl4GgICscyglE/kJIYQQjiDhxoUCQ3Mm8rtAQmqWi0sjhBBCFA0SblzIHKQ3S/lrKcScv+ji0gghhBBFg4QbVzL7kYoHAHExx11cGCGEEKJokHDjSppGnEmfyC/pnEzkJ4QQQjiChBsXSzEHA5B+4ZSLSyKEEEIUDRJuXCzLR5/IT8KNEEII4RgSblzMo3h5ANzij7i4JEIIIUTRIOHGxQIq1AOgbPphUjMsLi6NEEIIcfuTcONi/uUbAFBFO8n+qAsuLo0QQghx+5Nw42oB4aRoXpi1LM4c3O7q0gghhBC3PQk3rmYwcM6nKgCpJ7a5uDBCCCGKPKVg5SewfoKrS1JoJNzcAjJL1ADAfG6Xi0sihBCiyDu7D5a/B/++AilFszuEhJtbgFfZ+gAEJx/EapUFNIUQQhSiY6sv/RwV6bJiFCYJN7eA4pUbAlCVo5y8kOzi0gghhCjSjq269POZotkdQsLNLcAtpBqZmPDXUjh2eK+riyOEEKKoUgqOrbl0//RW15WlEEm4uRWY3Inx0Cfziz9aNN9oQgghbgFn90PKuUv3z0S6rCiFScLNLSIlSO9UrEXvcHFJhBBCFFk5TVKlGgIaJJyCpFiXFqkwSLi5RbiVqgNAYMI+F5dECCFEkZXTmbhqRyheRf+5CPa7kXBziyhRuREAFS1HiE/NdHFphBBCFDlKXQo35VpAKX2kblHsdyPh5hbhU64uAGHaBQ4dPerawgghhCh6cvrbmDz1YFNSX9tQam5E4TH7EmMqBcC5Q5tdXBghhBC3hc0/wcR2EH/6+vvm9Lcp0xhMZiiZXXNzZqteq1OESLi5hcT5RwCQdVrWmBJCCHEdVgss/wBObdJDzvXkNEmF363/G1oTNCMkn4WEfISj24iEm1tJaG0AfC7KXDdCCCGu49RmPZgA7J517doXpeB49vw24S30f908Ibi6/nMR63cj4eYWElBBn6m4dPpBsixWF5dGCCHELW3/vEs/XzgMMddYn/DcAT0I5fS3yVGqaPa7kXBzCymRvQxDeaI4GnXWxaURQghxy1IK9s7Vf/YM1P/dPevq+1/Z3yZHEe1ULOHmFmLwC+WiIRCDpog6sMXVxRFCCHGrOndAr60xukO7t/Vt12qaurK/TQ5bp+JtuZ+blgCpFwtetlugc7KEm1tMrHdVAFJOFK32TyGEEA60L7vWpnwrqPmQ3tx04QjkNcv95fPb5PS3yRFcXQ9IaXFw8bJpSFIuwLfN4NOqsGN6/suVkQw/dYB98wv0chxNws0tJrOEvgyD+ew12k6FEOJWdmI9rJ9wS3yDL7L2Zfe3iegCZh+ocq9+P6+mqSv621isihf/3M47/+wBkzuE1tL3u7xT8bwXIf4kWNJh5hOw5G2w5qMv6PyX4eQG/fkZKTf1Em+GhJtbjFc5vYowOPmgi0sihBA3QCn4axD8+wocWe7q0hRNCVFwegugQdXO+rYaD+j/5tU0dUV/m9WHzjFj6yl+WnOU2MS03P1uds2A3TP1YeJ1+urbVn8Gf/SD9MSrl2vHnxD5K2gG6PEDuHs55OXeCAk3t5iwqo0BqKiOczw2zrWFEUKIgjp/WF+MEeDkRteWpajan93kU7oR+IboP1e+F9y84OIxiIq8tG/SWVj7lf5zdn+bv7acsj0ceSLOvt9NYoxe6wLQciQ88C088D0Yzfp5f7xX/x1f6fxhmPt89vNezt385WS3RLgZP3484eHheHh40KRJEzZuvPp/iMmTJ6Npmt3Nw8PDiaUtXJ7BlYgzBOKhZbJn6a+uLo4QQhTM0ZWXfj61yXXlKMoub5LK4e4NVTroP+c0TWUkw2+99MATGA4NHyc+NZOFu6NtT4s8GXdZzU0kzHlW70QcWhtavqRvr9MbBs4Hn1CI3QMTWsDGHy41U2VlwF+PQ0YSlGt+6Xku5PJw88cff/DCCy/w1ltvsXXrVurUqUOHDh2Ijb36Eux+fn5ERUXZbsePH3diiQuZwcCZyno1YPmDk1D5aeMUQohbRU4TCOiTzMnfsNxi98IvD8L3beBwAZvu0uLh6H/6z5eHG7BvmrJk6YHjzFbwDIJHZoJ3MebuOENG1qXfSeTJOChRVa/1yUyGgwv1DsYPfAdGt0vHLt0QBi/Xa38yU2D+SJhyvx6clo7Wa4s8A+HBH8BoKugVcTiXh5tx48bx5JNPMnDgQKpXr86ECRPw8vLip5+uPpW0pmmEhobabiEhIU4sceEr1/E50pUbEdZDHNy82NXFEUKI/LFa4ehl4SYtTh+uXNiyMgr/HI6QngSL39RrPg4v1YPHL91hWj+4kM8Fkw8uBmsmFK8CxSvbP1apPbh5Q9wJmNoDDvwLJg/o+wcUqwhcapJ6sL6+luGOU/FYMEBYnUvHaTMKQqrnPrdfSXhsDnT6RA9Dx1bBN01h3df6492/Bf9SBbkihcal4SYjI4MtW7bQrl072zaDwUC7du1Yt27dVZ+XlJREuXLlKFOmDN26dWP37t3OKK7TeAeGsiVAr17MWvO1i0sjhBD5dHYvpJwjFTPbrJX0bYXdNLVrJrxXArZMLtzzAGSmwh+PwpLRBRsJphTs/QfGN4E1X4A1CyLug8aD9U67++bqjy19B46vg+hdeo1IygW9BuZyeTVJ5XD3gqod9Z+PrAA06PGj3pEYOBSbxLYTcRgNGi93iMDL3UhSehZHzibp/XcASjeGZs9e/bUYDNBkMDy9Bso202txAJo8DVU75f+aFDKX1h2dO3cOi8WSq+YlJCSEffv25fmcqlWr8tNPP1G7dm3i4+P59NNPadasGbt376Z06dK59k9PTyc9Pd12PyEhwbEvopC4tXgG5s0lIn4VGbGHcA+u5OoiCSHEtWU3l2y0VGWfKkM9wyE93NTtm3vflZ/Axu/g0VmXhiIXlFLw3yeXjlf3kcJtEtn+O+ydo//s4Qctnr/6vpYsOLFODy775unDqgECyuo1HzkhpOHj8O+rehhZNVa/Xc5ggqAK2TU1VfSaG9DDUV6qd9dHOwF0/gSqXdpvxla91qZ1lRKE+ntQq5Q/G45eYNvJOCq3eB68i0OdPmAwXv9aBFWAAfNg68/6a2v1yvWf40Qub5YqqKZNm/LYY49Rt25dWrVqxcyZMylRogTfffddnvuPGTMGf39/261MmTJOLvGNqVe/Cau1+hhQRC8a5+riCCFuFXvmwIJX9FqEW012k9Q6a3W2WbObTPKqubFkwfrx+twrOeHkRpzcoHdwBX2E1oEFN36s61EKNnx/6f6S0ZeWP7hcehIseBU+rQw/3wcbJugf/m5ecPdIGLrhUrABCK4Gj86Gh3/Ta0KCKoJ3sD4nDei1POcO6CFp9TjISNQ79uaMcLpS1U7QcBB0+hgaP2nbbLEqZmaHmx4N9IqAumUCgOx+N15B0Hw4+ATn/5oYDNBwILR9035Jh1uAS2tuihcvjtFoJCYmxm57TEwMoaGh+TqGm5sb9erV49ChQ3k+/tprr/HCCy/Y7ickJNwWAcdkNHC08gBaHNhKyOEZkPKe/uYTQty5sjL00SxpceBXCpo/5+oSXWK12GbBXWutQYzKXu8oZo8+asfd+9K+x9dcmtZ/7z9w8TgEliv4OTdn981089Y7w278Hqp1vYkXcQ1H/9Ob3dy8oUZ3iJwKMwfD4/9CWG19nzOReifenH5GnoH6PDQRXaBCm6vP+6Jp+j5XNjVZsiAxSg835w7Cuf36tar3iB4s8mJ0g/tyfyFefegcMQnp+Hu60baaHmBs4eZEXIEuxe3ApTU37u7uNGjQgKVLl9q2Wa1Wli5dStOmTfN1DIvFws6dOwkLC8vzcbPZjJ+fn93tdlGvZTf2WstiVmmkbbh6B2shxB3i6Eo92IA+d8mtVHsTtR3S40nWvNmtwokhiCgVBMqif+hfbu8/l35WVtiQd837NSWfh92z9Z8fmKBPHHf0P4jNu0vDTduYXWtTtw90/QIqtNYD1e99IDEa1o2Hie30YONXCvr8ASMPQfdv9NByIxPaGU0QUAYqtYW7hsB9n8GjM6Hmg7Zddp+JZ/zyQ3YjoPIyI7sjcbe6JTGb9GanumUDANgfk0hqhqXg5buFubxZ6oUXXuCHH37g559/Zu/evTz99NMkJyczcOBAAB577DFee+012/7vvPMOixYt4siRI2zdupVHHnmE48eP88QTT7jqJRSaGqX8+cdbfxNb1393+4wIEEIUjpwPc4DkWNj6i8uKkkt2f5t1lggsGDEZtLw7FVutlzrFNspuNtk6RV+ksSC2/6YvDRBWF6rff2mm3k0/3PhruJqLxy5NnNd4sF470nMyFKusN4d91RAW/k8fxRRxHwxZrTc9FfKQ6KT0LAZO2sQnC/czdcPVp0S5fG6bhxpc6psa5u9JiJ8Zi1Wx83R8oZbV2Vwebnr37s2nn37Km2++Sd26dYmMjOTff/+1dTI+ceIEUVFRtv0vXrzIk08+SbVq1ejcuTMJCQmsXbuW6tXzGLZ2m9M0Df9GfYhRAXilx17qJCaEuPNkZVxaLLFG9jf3NZ9DVvpVn+JU2eFmjaU6xbzdaVAuMO9wc2YrJJ4Bdx+49129k2xGImwrwKSlSsHmSQCk1OnP18sOEldL/0JM5O/6XDCX278APqkEP3XSJ59Luvo8annaNFGvYarQRp8TBvQmp75/gEeAXn6jGbqMhd6/Oq0LwVdLDxKbqP/+/9h0EnWVEVzzdkSRnmWlSogPtUr52z12qd/NDaz+fQtzebgBeOaZZzh+/Djp6els2LCBJk2a2B5bsWIFkydPtt3/7LPPbPtGR0czb9486tWr54JSO0fX+uX42ZI9LHz5mFvnD5kQwrlymqS8g6HbePANg4TT+ggeV8vK0BfLBNZZa1CvbCDVwvzsw03OB292k9RB/2Y0/Gg1MTUe17dvmKD328mPo//pzT/uvnxyqgafLjrA+7uLQ/GqelPR9mmX9j24BP58TO+8fGKtPvnc2Krwc1c9tFy8ziSwGcl6zRJAkyGcjkslPiVTv1+soj7aq+Hj+gR3jZ7Q+884waHYJH5ao8+NY9BgX3QiO07lrn1RSvHz2mOAXmujXVG+umX0vlGRJ+McUi6LVfH67J0cjLnGGlROcEuEG3F1JQM82VNar70xxR+H9d+6ukhCCFfIaZKqfr/ef6NZdmfiVeNyz4XibGe2QmYyiUZ/9qvS1C8XQLUwX3ap8mRhhKQYiD91ab4XYNKFmpxLyuCH+Cb6DLpxxy81V11PdkdiS61ezNytf6AvP3AWa04z18bv9eavIytgWl+wZEC1++He96FUA70W5uh/+hpKX9SGL+vDvJGwb37ulax3/KnXBAWGcyywGfd8uoJe363DYs0Oa6Xq631hQmrk+3IppfjvwFlmbj1FfGpmvp93+fNH/7ObTIvinohg7q9TEoBpm07m2nf5/lj2xyTi7W6kd8OyuR7PqbnZftIxzVJfLDnAr+tP8PD360nJcN37UsLNbaB7kyp8nPkwANb/Pil4laoQ4vaWlQH79FAw8WId7vpgKSfK9wSv4noo2PWXa8uX3SS1QdVAYaB+ds1NGmYOkD0K6tQmOLsPLhxGGdz5O1kPA//suYBqkF17s/6b658rMcbWPLcu8H5bODiXlMGe4M5g9oPzh2DFGPjtYb1fTtXO8NBP0OwZeHIZDN8B7UbrQ68NJr0WaNMPMK0PfFYdlrwNCWf0MJbTkbjxYGZE6s07+2MSWXTZ+kwFsTcqgT4/rOexnzbywp/bafz+EoZP28aaQ+ewWvM3MeDC3TGsOngOd6OBN++rTu9GemiZE3ma5HT7QDFh5REA+jYpi7+XW65j1Srtj6bB6bhUfYXwy8zceooJKw+Tlpm/GrUle2L4cpk+cvn1+6rh5e66AdkSbm4D99cpyZGS97HDWh5DRhIse8/VRRJCOEJqPvs5HF0JafFYvYP5eE8Q0QlpfLT0pP5hDfrEb/lt0rmWg4uvv9bR/Jf1viuRv11qJs8ON8vTIzAaNGqX9qdKiC8GDTZn6dP+c3qLrdbmdLEmJKPP4xKTkM6OUr3A4KZPend667XPv+0Xfe6XMk345agPcKklaOnhlEsTBv73MWSl6ksS9Jxsv05SYDloMQIeXwAvH9XnmGn0BPiX0X8nqz+Dz2vBrw/q8+i4eaHq9uXvyDO2Q/yw6si1y3mFC8kZjJq1ky5frmL9kQuYTQYqlvAmPcvK35Fn6DdxA3d/vJx/d107NKVmWHh3rj63z+CWFQgv7s1dFYIIL+ZFcoaFeTsv9VHdeuIiG49ewM2oMahFhTyP52M2USXYF7AfEv7vrmhe+HM7Hy7YxwPfrOVQbNI1y3XsXDLP/xkJQP+m5XigXu5JdZ1Jws1twGDQePeB2ryX9SgAausUiNrh4lK51rmkdBLSCl6dW2SdOwg7/yrYlPDCtbZOgY/C9bWGrie7SepYiXvIsOqf5PN2RrEj7CG9Q+u5A7Bn9tWfn5agD7e+VnA5uRGmPgS/9oDzV1kP6uRGfVbhE2th9tN6AFjxkb4dfX6bamG+eLmb8HAzUr64t32/m+xwsxS9X6W7Uf8I+uewBWr20Peb85w+Qd7WKfqkgDF79Oet/ASmD4Q1XwKQXPsxlu87C0D/puGA3gRDo8tGzlZorXfwNZnZcyYh7+HOHn76UO0uY2H4dn3/cs31AHV4mb5PnT5sjYUTF1LwdDPibjSw9UQcW47nHU7PJqYzf2cUE1Ye5rWZO3lk4gZafbycqRtOYFXQpXYYS19sxZIXWvH3sOb0a1IWXw8Tp+NSGTp1C7O2nbrqr+nblYc5HZdKSX8PhrbRg6OmafRqpM/f9sdlTVMTVui/x+51SxHq73HVY9pN5gccP5/MS39tB8DNqLE3KoGuX63mj00n8uy0nJphYcivW0hMy6JBuUBGdXH9AB8JN7eJmqX8qdG0I3Mtd6GhsPz72i3/QTZr3R7G/rXC4SHk2LlkWn+ygq5frc53dWmR9+djMGMQHFzk6pLc2qxWvf9E3AmI3gknN0Fm2vWf52jJ52HR6/rPa77QR/hczWVNUrMy9FDg5a7PU/LB0lOoJkP0/WYOhr+H2QeTrHRY9w18WRcWvAy/9dLXLbqSJQvmZU92qiz6KKy8rM7eXrK+3qE5KQZWfACWdBLcinNUhVK/bKBt94gwP7ap7HBzegtE70BpBn46GwHAoLvLA7BgVzSq6VBAg5id+ky8c57VZ/j9tin88Qgsfw92z4T0ePAtyT+ZjciwWIkI9eXp1vqH/PZTcZz3KAtt34L6/eHh38HNgxlbTtH5y1V0/nLVtTu6Goz6JIAD58PgFVD7YSjTBFo8z9+RpwHoWDOUbnX1Pi4T86i9ORSbxD1jVzB06lY+XLCP3zeeYPWhcySmZ1EtzI9pg+9ifN/6lA70QtM06pQJ4P0HarFpVDt6NSyNVcELf27nzyv6z1izZxiesFL//b5+X3W7Zp+H6pfGaNDYcvwih2ITORSbxOK9+gS5T7XKu9YmR858N5En40jLtDDst622oLLypTa0qFSc1EwLr8zYybO/b2PX6Xhb85dSiv/N2sm+6ESK+5j5pl993E2ujxauX5dc5NsL7avw6I7+tM/Ygvn4ar3z3WXrhtxKziemEvHvw3TjBH8f6EKDgZ9RNqwA03pfhVKKN+fsJik9i6T0LKZtPMGA5uUdUOLb2MVjl6agP/AvVOng0uLcsv4alD2dwhVfCmo/DA/ewCRy15Ny4epDgpe/p4esnJl1/xmuD4ku3SD3vtlNUso7mJ9OhgKKz3rX5dnft7H+yAVWNetNy0qb4dASfTh15G/6UPGyd+nBKWdNI6NZ738yc7A+sufy6fI3/6SHPTcvfSHEyN+h5cv6BHI5YvfB/nmABg98B4Hhem3RuvEQFckyU0tAsws31cP8mLcjlGSDH95WfR6b5NDGHD/qha/ZxDNtKvHz2mOcjktlR1Y56jy+UK/huXhUXyX74lFIPgdB5aFENX2pguDqUKYRf03eC+irW4f4eVCjpB+7zySw8sBZHrz70qz0Sim+zQ4ER88l88A3a/msd13aV7df0zCXkvVs74tMi5W5O/TJAbvVLUmYvyfTt5xi4e5oTpxPoWwxfYK+tEwLz2QHg7JBXtQrG0C5IC/KBHlRoYQ3dcsEYjTkPZrKw83Ihw/Wxt1k4Nf1J3h5xg4yLFYeuascm49d4N25e9iePRqqddUSdKppP4t/sJ8H90QEs3hPDH9sOkl8aiZKQfvqIVTKbna6mpyamx2n4nln7h52nU4g0MuNr/vWI8zfkymPN+a7/44wdtF+5u6IYu4OvekrzN+DYD8Ptp/UF+Mc37ceIX5XryFyJtfHK5Fvvh5uPNG1NT9Y9MmqMv8ddWvNUHqZdYumU007jkFTPJAxF8N3zdm9+p/rP/E6/t0VzX8Hztruf7Mi/53diqychfRAH/Z6i9foucSZbdmdbrOvjdEM3iX0n3fNgKSzV33qDVn8JnxcXl/l+UpRO2xztKxp8g1ZlTvpoeOPfvpMt1faPQuAk6HtSc5UlArw5N7qIfRvqnfU/WDJKSx9/4JBi6FyB30k0K6/9CHP8SfBtyR0/VJvcvEqDrG7Yfn7l46fFHupH9+970H5lvpkdGu/tC9Hzv2ILlCiCpjcoXYvGLyCtOd282qCPveOXc1NqC+gsdtQxbYt0vtuAJpUKIa32cQ9EfqXnvm7oqBsE70fUZex+ky8z22D107CU//pQaPFCKhyLydSzGw+fhFNg/vrlAKgTVX9OMv32/8uVx08x6HYJLzdjTQuH0RSehZPTtnMl0sP5rsD76qDZ7mQnEFxH3daVCpO1VBfWlYpgVVhG44NMPqfPdk1GO789XRTvni4Hi/cW5WeDcvQoFzQVYNNDoNB491uNRnYPByA12fvotd363howjq2n4rH293Iyx2rMuGRBrmGdAP0bqiH0T83n2LWNr2maUiritd9fVVCfG0rhP+24QSaBp/1rkuYv6etXE+3rsifQ5rSvFIxinm7AxAVn8b27Kas/3WuRpMKxa57LmeRcHOb6VIrjO3lBhKjAnCLP4b6Z/gt92FmsSr8d+vzQhwPbEqMIZjSxFJjySMc/HFQwWcizZacnsU7c/fgSRorS3zKt17fE5uYxtQNJxxZfKdRSvHxv/v4YP7eq06+lS+Xh5v4E3r/C2Fv00T935o9YFQMvBELLx3ShwVbMx07V8zxtXqNCWSv8nzZOj9K6YteotgV2JZ+S9wYkTEESkToawj98aj9XFaXTdz3T5beJHVvjRA0TWNo60r4epjYF52oN5mUaQz9/tSDQPVu4F8W2r8Lz22FBv3BLwzuzw4oa77Uywmw6A29qadkPWgwAFq+pG/f8rM+Mgkg/rQ+JBqg+Qj716tp7E70Is1ioLiPO2WCPG0PVQvTl7tZmxZu2zY9uQ4AzSrqH4Sda+lL5yzYGZ3v/wezs5uImlcsbutL0iZCD6v/HThLluXSUgQ54aNnwzJMfaKJLRSOW3yAwb9sYfeZ6w+Bnr1N70h8X+2SmLL7CT2Z3aT25+aTxKdkMmf7GX7fqAeDz3vXI9j3xmowNE3jzfuq25qSNh69gKbBw43KsPyl1gxtXQkPt7xX7W5dtQTBvmbiUzPJtCgahwfRoFxgnvtezmjQ7Cb3e6ZNJVpXzV3TXr9sIFOfuIstb7Qn8s32zHi6GZ88VJsv+9Tj8exAdquQcHOb0TSNUQ805iXrM2QpA9qOP/TJr24hG7Ztp5llMwAhPcfh/8Im/gvoDkDlk39x6ot2pCYVfE6FL5ceJCo+jRG+yyiXuJVO1hXU1Q7z7YrDt+W6KEfPJTN35VoWr1rDweuMRLiqzDTbSJVTqri+7fKwI/TmoZ3ZQ6WbDAG3yz506vfX/906xTFfEjJT4e/sEUwlqun/Lh0Nm37Uf949E06sxWryZGjsAwDM3Z/MqoZfgoc/nNoIvzyor1f0bQv4tJKtSerHE/qHTYcaenNEoLc7Q1vr/VnGLjpwqQYzrA70mgLP79QX1nS7FDaI6AJ1HwEUzBoCBxbBjmmApteWGIwQfrfez8SSDuu+0p+3/hs9BJZrDmUa5XrZW4/HAVCvbKBdjUKYvwd+HiZWWmoBYC3bjIWn9N4QzSvp79fWVUvg4WbgxIUUdp+5/hcfpZStVqJ7vVK27XXLBBLg5UZ8aqatY+zhs0ms2H8WTYMBzcJxMxoY3a0mH/WohbvRwJK9MXT5cjXdvtY7y145jBr0L1WL98TkOl+LSsWJCPUlJcPCh//u438zdwIwrHUlWlQuft3XcS2apvFqxwjeuK86XeuUZO6zLfiwR+3rBiaT0UDPhpdGKQ1pfe2+NpfLqXW5q0IQI9pVuc7eEOClz0Lds2EZ7q9TMs+aJFeScHMbCi/uTYf7evFBVj8ArAtH2T7gHM5q1WfoLEDz1/n/vseoKY751sejZHU8fAK4e/hkZtX5nvPKl9Kp+9n5WTd2njiX72MeiEnkx9VH8SOZx5lj2/6U1zLOJaVfc12VW9WaXYeZ6z6Kv91fZ+Oegg0rtTm+GrJSiVJBTMrK7mtzaMmNFyozVX++I4YV3yoip0JWGoTWgtKXPpitVsWeYu1R7j5w/uClmozr2T5Nnz/lysUgQZ9b5cJhlE8Ya1tPxdpipL593ot6gFqkj4xaEtSHE5YgPNz0P8GvLEsirdtEffHH46v1dYxidmYvI6BxosoALqRaCfJ2p1H4pX48A5uHE+rnwem4VKasO5a/8nccAwFl9flxftfnz6LBAL0WC/Rx1Tm1N5t+0jsob5ms328+gq0nLvJ35GnSsy69R7ae0EcNXd4kpR9K02cqVpVZ1uJ3tt71BWmZVor7mKkSog/j9nI30bqKHtwW7IrierafiufouWQ83Ax0vKzfidGg0aqKXnuzbJ8+F9jkNfo1aRsRQnjxS6uS925UlplDm3Ff7TDcjBrbT8XzyoydNPlgKZ8vOWBX87NoTzSpmRbCi3lRp/Sl2g1N0xjUQq+9+X3jCZLSs2gcHsSIdpWv+xryI+f4X/WpR42S/td/QraHG5XF12yiQblAW1NdfgxuWYGxPevww2MNr9t8djuQcHOb6tekHB4thjHT0gKDspAx7TF9BMjNSrkAU7rDmDLwTnF4JxA+KInlgzIkLPnkuk8/eTaeJnH6LKOeTZ+0bdc0jQce6M3Jjj+TipnGlm0c/GEA3yw/eGmmz6tQSvH67F1kWRUfhK7ELTPB1l/iXrWWQBKYsPKwS2fDvBHW7X/gp6Xgp6WSvHvhjR0ku5ZmhaUOK6x1AVDH1+iB9EYse08fCrxizI09/1ZjtV6qNWn0pG1ClIMxifT+fh2dJ2xjvWcr/fGtP1//eBt/gFlPwYEF8FMH+1FOp7fqK3UDU4o9R99f9vHyha6oRk8CSh/9k3CKTN/SjDiln/PH/o0oHejJmfg0PjtaBnpPhRYv6LUofafD0PXw2ikmad0BaFct2O6Dx8PNyPPt9Q/Tj//dz9Ls0THX5OEH3ScAmj4yyjMI2r5JlsXK8GnbeGn6dqwV2uo1QJnJMKUbZCRBcA3Ohrak3w8bGD4tkjafrOD3jSfItFgvCzcBuU6X0zS1Lj2c/07p/9ebVSxm902/Uy09pMy/omkqKT2L/w6cZdOxCxyMSSQ2MY2/tugdpDvUCMXHbD8m5vJ+N/EpmfyVvRJ2Xk0mNUv583Xf+qx/rS2vdYogvJgXSelZfL7kIH1+WM/pOP0LXU6TVLe6pXLVTtxftyQlfPWO2YFebnzRp66t2cpVygR5sfrVe5j6RJMC1ab4mE30aFAaX4/cE/3djmS01G3spY4RvBL3JpX3DqZW+jFSf+2D51NL7KuhCyIzVf8md3JDroeMKhO/1e+xPyGTqg/+76qH2LroV7ppccQZAglp/FCux+s2bUuiz2QsM/rxoHEV3yx9h06RT+DlbiIhLZOE1CwS0jJxM2j4erjh62HC3WRg95kEwtyS6Jysd66kyzhYNRZjVCSDfdfyUWJHfl1/nMEtr9957laQkp5Jk4v/QPbfnpJnV5JpGYlbQf8wZg/9XmGty2FVkpPWEpThrD4/SNWOBTuWUqi9c/QibfgOmj2rN5W4miVTn0X2Rqq9Dy/TR9yY/aHWQ6RlWhi//BATVh4m06J/iH56/i5mmObBnr+h00f6goh52fCdPpwaIKiiPqvt7CH6EOf2o/XmKGUltWp33tsVDij+2nqa2vcP5bHaCbDjDwCm+D5Jylk32lQtQfNKxfUOpJM38ePqozxQvwUREZ3tTquUss2Ge291+xEyAD0blGHNofPM2X6Gp6du5af+ja7fLBLeHFq9DCs/hk4fg1cQU1YftU1S16pqCe5r+ZI+BDtntFXz4UxcfZTU7OavM/FpvDZzJ18vO0RMQjomg0bt0gG5TlUtTB+pszcq0fbc5pXsO57eExGMu8nA0XPJ7I9JJNjXg8lrjjJ57TES0vL+0nJ5E1GOllVKoGn6LMBfLD1IaqaFiFBfmla8ekfXYj5mnmpVkSfvrsCc7Wd4ffYuNh27SOcvVvFqpwhWHzp31fOZTUZe6RjBZ4sP8GGPWrYOuK7m71k0AsrNkJqb25imabzXsxHfh73LOeWH57ldpP/QUf82X9D+A1YLydMGwckNJCgveqa/yV1pX1EvbQJdff/kZ7PeBFZ1x0fMn/i2XZV0jowsKyUP/QbA+Sq99dEUefCt1RlDN/0b7lDTHJqem0HkyTiOnE3mXFI6GVlWkjMsRCekcTA2ydYO/225lRgykyGsrj4XRfZkXY+6LcOAlQkrj7Dp2AU2HDnPmkPnWHngLCfOp+RZBlfbtWkFEdqlmra72Ubk8fw30wF6c8GFI2QoI2usNehapxQrrbX1x26kaercAbSc2r/0hEs1Hq50Yj2Mqw4TWsCFG2i6y+lIXLcvW6LS6fTFKr5adohMi6JtRDBVQnzYklWBCz5V9KarnE6zV1r/7aVg03wEPLMJWr2afY4f4Iu6+igkr2J85/UUmRZl+4B5Z+4+NtZ+B+4eydmGL/LeUb2fzIv36qtLt4kIpmONULKsitdn7co1gmfn6XjOxKfh5W7MM7QYDBpje9Xh3uohZGRZeXLKZjYdu2C3T3xKJrO2nWLHqbhLG9v8D/53Gmr3JDYhjc8WX+qIPm7RAX0UV06/If8yXCh/H7+s15t/v+1Xnzfvq05xH7OthqNamB+e7rk7ukaE6jU3O07F2UbWNKto/zp8PdxoWVmvjX1p+g6af7iML5cdIiEtizB/D8KLeRHg5WbLt1VDfLm7Uu5rEeTtbhvWnNOReGDz8HzVYBgMGt3rlWLecy2oU9qf+NRMXpu5E4tVUae0P+Uva9a63EMNSrPm1Xu4O7v84tYgNTe3OXeTgfcHduLdr19ldOJovGIjYepDXAyogVe7VzHX6Hrdb7zJ6VnsnTSMhtHzSFcmns56kTJ129KvSgmaVSxGsJ8HGVntWf2jGy2iJtP51Gd8NS6TzgNfo2IJH9txVq9fyz1qFxYMlLt36DXPqdV7RB8dsuw9Rrv9zIvBW7hY4X4yIrrjWbwMFqsiITWLxLRMEtKy8E6Loc6C7E6h97yhv6aaPWDR6/iknKKn/z7+iK9Ozwnr7M7j6Wbkn2ebX3eeB6fLXmV4Z0BbKiRsIsCawJGty2lUoXf+j5HdJLXJGkGF0mE81bICn++syyMsxXpwMQalClbbkX28ROWJr5aKWv8N2l1P33hN4M06sBD+7K9PoZ8cCz+0hYenQrlm+Xv+xeP6vD/AheqP0v+nTSSlZxHsa+bt+2vQqWYok9Yc4525e5hOW57igD5CqPFg++u2bjwszK6tvPvFS++/Nq/pI4xmDtbLB6S0+4CJf+uTxI3rVYe/I88wZ/sZhk7byT/PjuTNv3ejVAyda4VS87LRKW92rc5/B8+y+fhF/tpyyjbbLMDC7FobveNt3qNk3IwGvupbj8FTtrDywFkGTtrEj/0bcjYpnb8jz7BifyyZFoXZZGDqE01omNNvx13/wB6zYB+J6VnULOXHmbg0jpxL5q+tZ3i44wcweyjc+x4/rjtJSoaFmqX86FgzFE3TeLhxGaasO84/288wuGXenVdzlmHIqYEpE+RJmSCvXPt1rhXKkr0x7DytDzaoWcqPoa0r0aFGqK0pzmpVJKZl4eNhumq/kHuqBrMtexmBIG93utXNXeNyLeWKeTN9SDPGLtrPd//pgTqvWhtxa5NwUwT4ebjxwpOPM/L3cOqd+pV+xqUExu2Gvx4lak44MSWaYinVEJ+KzSkVXpnEtEwOxyZz5FwSh2OTCNgxkect0wCYWOwlRj/8VK4w4G4y0GLw55z4w0TZfRN5NuVrRn8Rz7HS3WlbtzKdaoaSulb/lnwssDkVg8pdv+B3j4T0JFj7FX4Xd+G3ZRdsGaOPyKjRHap2gtLZPf/nfqSP3ijbFCq1zS6UF9R7BNZ9zcvFVrHJrRFZVoXJqOFmMBCXmkFMQjrP/h7J7GHNMJvy/mBwNpWeSM0LenNSZr0BnN3vgfeZeZiPLAIKEm5ymqTq0KFGKDVK+nE6sBEZSUbc447pNTvFK+X7cBn7/sUd+DLrAfqbFlE6+aw+KVzjJ6/73Hw5sw2OrYYyd+mhwHiNPz/bp+kfqsqirw2Uck5//s/368OZc9YPupYtkwAFFVrz8WYLSelZ1Crlz9Qnm+CX3a/g/roleX/+Xsafq8eT3h4YYnfrzUylG0LsXlg4Cg4v1Y/X8iVoM8o++FTtqE+It+AVKFGVKQkNSUrfT5UQH9pUDaZZxeIcjE1ib1QCfX/YwNFzyRg0fULOy5UM8OSF9lV4b95e3pqzm7WHz9Gueggtq5Rg4W69H03OKKmrMZuMTHikAQMmbWTD0Qv0/n693eMBXm7EpWTyxJTNzHi6me2LyYYj55m17TSaBu93r8WmYxd4b95evlh6kO4jW+Px4j7iUjL4+U996Ybn7qlsqwnxcjcxpFXFa86l4uluJLy4N0fO6v3AmlfMu8msQ41QmlU8hUHTGNyyAndXLp6rxsVg0PJc/PFybSKCGZtdC9WvSdmrBsJrcTcZeK1zNVpXDWbHqTgeuSsff8/ELUVTNzXBxu0nISEBf39/4uPj8fPzc3VxHO7E+RQWbNyJ95bv6J45Dx/Nfmr5KBVEjArEggELBhQajbT9GDTFgVovUvnBN65dhasUyXNewnvbDwBkKiNbVWXWWGsxyDgPfy2FC91/I6hul/wXOvmcPtPpzhn6mjWXC6sDFdvqE4hZs2DAfL2/QI7zh+Gr+oCmT/gVdGm24tiENDp+sYoLyRk80aI8r993/fVOouJTefPv3USE+jKoRXkCvPJuWrsZ0St+IHTFSI6pUEL+t5uUyOkUWzCEg6oUoa9tz1+HvowU1EfhaJZ02qV/zLcj+lI5xJexi/Zz16qBNDfuho4fwl1P569Q6UlYPyyHQWXRJn0sLQw7eddtsj6q5tlt1w4i+XF2v17zkpE99b3ZTx9yXKE1BEeAd7DeSdwzEDZ8e6mmpPbD0O1rvd/N7CF6vxiAFs9Dq1euXquUmaav7pxynhP3/kCrf7xRCmY83ZQG5exnDR40eRNL98Uyt/Qv1Dy3AKp3B+/i+kR7yqIv6Njmf/o5NY30LAuxCem5ah/Ssyzc/dFyYhPT+bRnHR5qoAfzkxdS6Pr1auJS9GVIHqxfinG96uYqcpbFSt+JG9h49FKTksmgkWVVuBk1trzR3hbKriUpPYvHftzA1hNxlA70pFvdktxfpxRlgjzp88MGtp/Ut88c2oxAL3fu+3I1+2MS6dukLB88UIu0TAv3fLqCM/FpvN6lGk/cXYFxiw/w5dKDVAvzY/5zLQo87HfYb1uZlz2r7Zd96nF/nZIFen5BWK2Kjl/8R2xiOgtHtLxlZswVN68gn98SbooopRTbDhwjdutcvM9uJSxhB+FZRzBhzXP/rPqPY+o6Ln/NGErB6nFkbv4Ft/ijdg+dM4VR/H97wHCD3bniTuozsu6bl92x+bK3Z8V74NFZuZ/zy4P6t+tmz8G979o9tHRvDIN+1ufcmfJ4Y1pWuXq7uFKK535cTLvjn2HBQLQhlApVatGiSWN8StdwWOfa6HF3E5qwgz/9B9Hr+XGQGoflo/IYsbK68xJaNM49j0guBxbCb704pYrzqM9Elo1sjaZpHIhJZPqXLzPK7Tcyy7fFrf/M/BVq3zyY1pdj1hDeKDuFTYfOsMY8nGJaAjzwPdS5To1S1A59naFK7XK/h1IuwMS2ep+ZgLL6JI5pcXkfRzPqgQLgrmFE3/U636w8QkaWlVaVi3FP9ETMa7MnxTOa9SaqSm3194abp76A48kN+rDus/tQfqXp6/Ud647Fc3+dknzZp16uU87bEcWw37bS0fcIEzJft38w4j5o/w4Uq4jFqs+vMm7Rfs7Ep/F48/L8r3OEbXTMn5tP8vJfOwj18+C/l9vYra+z+uA5HvtpA0aDxtIXWtum67+SxarYeuIiS/bEsHhvjK22o21EMD8OyMf7IltGlpUTF5KpWMLHLoicT0qnx7drOXY+hZql/Li3eijjFh8g0MuN5SNb28L8H5tO8MqMnQR6uTHvubvp8Pl/JKZl8W2/+nTKnnSvIL5edpBPF+m1KZtGtbONMCosSelZZGTpQ+dF0VGQz29pliqiNE2jftXyUPXZSxszkkk5vgW3zETcNKV/iFgt4F0cU/jd+e+foWlw94u43f2ivv7LkeUk71mEit6JZ9s3bzzYgL6WTfPn9FvyOb3PxL75+pwcHT7I+zmNn9TDzbZf9G/Yl32bb1sthMealmPKuuO8OH07/w6/m2I+ef9hnbsjihbHxtPNdFnt0cG/4CBkGjwwPPQjxuo3uZZX7F5CE3aQpQyoOn30bZ4BHPepQ4WkbSTu+AfyE26ym6SWW+pyb41Q2wdYlRBfjgc2g6Tf0I6v1kfA5aPPjPXAQgzoTVzD21Xh9aQMfjzbkZfd/oTVn0Gtnnn/XpXSJ5FcOEp/P9V8CLp+AebsvliWLPhroB5s/MvCk8v12pmo7XBkBRxbBfGnIPkspF7Uj6EZsLZ5g9/dH+TDz1aRmD2x2rRNJ3EzNmJE8Cv0T5mMT3oMHFmu365iT8XHWbcuHrPJwCudIvLcp221YPw8TPybWJ7k0Ai84/bpc+J0GAPl70YpxYp9sXz07z72RV9adPGnNUc5fDaJr/rWw8fdxPfZ/TMebxGea+HAFpWLM31IM4wG7arBBvS5WhqFB9EoPIjXOlfjyNkktp2I4+4qBZsUzt1kyLOfWTEfM5MHNubBb9ey63QCu07rnfVf7RRhV0vZo35pvvvvCEfOJtP7+3UkpmVRJcTnuk1jV1Mve/6bmqX8Cj3YgD6smcI/jbiFSbi5k7h741W5pWOPGVQegsrj3fBxxx4X9KaBeo/ot2upfK/+wRl/Aj6vrdcO+JUE/9JQuhH/69SV9UfOcyAmiZf/2sHE/g1zVavHp2Qydc58fjOuBEA1G87J06e5cGo/YVknCbHGkfVnfzJ6/ox7jRsPOOkbJ2EGllrr06T2pWayjAodYMc2gqNWXP8gSqEOLEJDDyNDr/jAqV2vCWf+C6Kk9YLex6Vy++yTJENGCviUyHW8rP2LcQc2mhrwSJkA+jUpyyd/t2eY2z94n92rh8wrhiiTkQJzR9iGOAP6mkbRO/QZcoOr6StfH1kBbt5YH/6N46mepCcmk0VFMsuVJ6vMAHzMJoJ9zQSaNQxpFzh2MYOXF5xm49HdgL6oX72yAazYf5aj55L5JKoOnzCO+0IT+LDeOXxOroRja/RgFFZXX4agTBPSwxow5If9QCpPtaxAqYC8Q56Hm5H76pTktw0n+LTEB7x1vwnCW4DBSHR8Gi/8Gcnaw+cB8PMwMbRNJcL8PXhlxg5WHjjLA+PX0LdJOQ7FJuFrNtGncdk8z5OfafCvVKGEDxUu67TvCOHFvfmxf0P6/LCetEwrdcsE0LNBGbt9TEYDL7avyrDftnLygj4a6tl7KmO4wcndmlUsxpd96lGzZNGtLRe3Fgk34vZnMOoL6s17QR+1khwLpy897FGmCRPajabjHwaW7ovl44X7GXlvVbvRFh/+u4+hGZMxGBWWat0w3vsOZYFSVsWszcfwnPs0XQzryJrenxTrZLxqdS14ObPS0XboHbdXeHeiw2VDS0s16Q47PqRW1i6iY2MJDb7GzKLnDqLFnyBdmdjvWZ962UNfc9xXpxQrltehr2k5WYtHY1r7FZw/BAmnAQ16TtY7bOeI3Yt78hnSlBvulVthMhroVq8UH8z3Y0pWO542/QOL39DndSlZT+8HlXJBnwMleofelNThAyhZF6YP1Ne2+uEefTTbtl8AiO/0FY/OiGfHqRVXfVlGg0ZxH3cupmSSkWXF001fJPCxpuEYDRpvddWXrFi2L5avlx1kbrQ/OzeG8fPAQYQHuuu1SJdNP/DTisOcvJBKiJ+ZIa2vPf9Rj/ql+G3DCf7Yn8lLPVvhZTCy41QcT/y8mdjEdNxNBgY0C2do64q2Go6KJXx4cspmDp9N5t25+qrsfe8qe1tMglavbCA/PNaQSWuO8b/O1fIMLZ1qhlKzlB+7TidQsYS3bQ2oG6FpWqH2sxHiSjLPjSgaGg2Clw7rzR69f9UnJmvyNLj7wMkNVJjZmdkV5uBLCt+uOEy/ieuJSdA7W286doHTm/+hpXEnVoMbxvZv2w5rNGg81Lg8xR77mQWqGSaycJsxgITtc65SkGvYPQv3jHiiVBCe1e+1e8i3VDVOG0vhrlk4uuE6q6f/p88Uvc5ag7trlMv1wRRe3JujgXqna1PsTji6MjvYACj4ZzgkXDbN/aHF2cerTvMIvROsn4cb99cpyU9ZnUgzeOnhaNHrMLmLPnv1+MYQvQPlVZzINj/Tf099HpxrIbLLHKjQBjJTbMHmYuORdF0SxI5T8bgb9cUVQ/08KBPkSfni3rYVhi1WRUyCPs/R3ZWLs+j5lgxsXt4uhJYv7s2gFuWZ8XQzygR5cvx8Cj2+XUvkmWRbsEnLtLDm0DnGLz8EwCsdI/Byv/b3uPplAylXzIuUDAsLd0ezYGcUvb5bR2xiOpWDfVg0oiX/61zNrummZil//h7W3DaviptR4/Hm5a9yhlvP3ZVL8NOARlQKzrtmyGDQGPNAbRqWC+S97rWKxJT84s4hHYpF0ZZwRh95s1vviJxmLs5Lqf35J6MBQd7ujHmwFmP/3cMX8c9RzXAS7hoGHfPu27Pr5HlO//QIHdRaMjFxtFwv4jMNxKdmkZCWRaLBlxpdh9MwIo8PuGNrUFMfQstM4bPMHjTo/1Guzs2bv3uahlG/sdGvA41fuMpkcoeWwK89sGCge/o7vDCgd57rx/yw8gDxiz7GU0snxq00aX4VMBQrzwuxrxOctFcfXt1vOmgaGT92xv3kGt7K7M+wVz4mOHt0yfaTcXQbv4bKphjmtDmH59nt+npKCfqU9hf8a/Cs9QXWnLVv7unbqBRv+s3HY91YLla4n/ZHHuZcShZlg7yY8nhjuzV+cmRarJxPyiA2MQ0NjZql/K47IudsYjqPT97EztPxeLoZ6d2oDLtOx7P9VJxt9uE6pf2ZNbR5vppTvlhykM+WHCDY10xsor4yd6sqJfiqb71rjlJKy7Qwee0xKpbwoX31kOueRwhxY2S01DVIuLlDHVoK81/Sm1aAhe7teT6hDyl40NO4gk/cvsdq9scwPBK8gq56mMMxcRz9rh/trKvzfPykKsHptuO5q2WHSxuPr4VfH4LMZFZaavMcL7HhzS655t/YvWYuNRb34yJ+BLxxFO3K4dcZKfDNXRB3nJ+yOjLO+Dhb3miX5/w955LS6TVhHUfO2a8xVUk7xQKP13FTGXDf51CzB9aPymNQWQzy+54fX7g0KkopxX1frWb3mQRe7RRBi0rF2XriIgcOH+bY0YOsTw4jCxPe7kZ6NypLcnoWf2zWp+ov7mNmYOMQxq8+bZv4bdKAxg7vTJqcnsXQqVtZeeCs3fYQPzNNKxRjZIeqlA68egfey528kMLdH1/qnDygWTivd6nm8rWChBA6CTfXIOHmDpaVDss/gDVfAIoL5lIMSxzA527jCdHi4N739PWUriPqYiIrfh+LX+pJ/DxMtpvP0YWUyIoiUxnZVe156vUapQ9L/rUHZCazw9yAnvHPcne10kzsn3tEVHp6GukflMdPS+FY99mE121jv8Pit2DN58Roxbgn9WM6N6jMJz3rXLOsSelZnLyQwskLKRyISWTc4gMMNMznDbdfwc0bWr8Ki9/giDWUv5r9zcsd7UcUTd1wnFGzduV57FA/DwY2D+fhxmVtSw1sOHKe12bttA1hBmhRqTgTHm2Qa5FDR8m0WPl8yQFiE9JpVD6IJuWDKBvkVeC5WECf82blgbO81bU6jzYNd3xhhRA3TMLNNUi4ERxdBbOG2JpXAFRAWbRnNoPpxmsWspIvsue7gdRO0L/9nwpoRMnkPRgyk/nPUosnM18kQ3PnpwGN8mxKAlj/cTfuSlnBKb96lH5s4qUZhqN3ob5riaYsPJHxIgcC7uafZ1pcd7bWK32ycB/fLD/Inx5jaMRu2/afsjpSc9C3NC5vX2uVlJ5F60+Wcy4pA18PE/XKBlK/bAANygXSpHyxXEOeQZ/M7tsVh/nhvyN0rBnGmAdr5bnfrSjTYiUpLYtAmR9FiFuOhJtrkHAjAH1OlXkvwq4Z+v2HftJH99wkZbWy+NcPaXV4HGZNn5F2laUmT2aNpG2tcjx3T2Wqhl59nauF82fSZsMTuGsWLJoRrcnTGFqN1Ju1Tm9mgaURz/MiM59uTvUbGFababHS49u1nD91iMWer+Gl9IVFn1L/4+s3X8pzVfKzienEp2ZQobhPgYYCW63qhocOCyHElSTcXIOEG2Fn/wJIOQ91+xVskcnrmP3vv5RZ+zpRqhhLqrzF0+1rXTPU5LBYFd/PWkjlyA9pZ9wGgHLzQstMIUF50j79E17rfc9NLeR39Fwynb9YRRfrMj51+45kZebVin/z1WNNb/iYQghR2CTcXIOEG+Esu07H4+lutFs5Pb/+jjzNPzOm8Ko2hUqGMwC8njkQU5Mnefv+GjddNn16/R30MKwiiiC6PdCH3o3ynnxOCCFuBbL8ghC3gJqlbnwtqm51S1GxxFAen9KAFkn/4kMKB0r14NfO1RxStl4Ny7BsXywzduszVo+rco1JA4UQ4jYj4UaIW1TNUv7Mfq41r84IYndCGj880tBhHXM1TePDB2sTm7iJSiV8CPWXlZOFEEWHNEsJIYQQ4pZXkM/v22N8phBCCCFEPkm4EUIIIUSRIuFGCCGEEEWKhBshhBBCFCkSboQQQghRpEi4EUIIIUSRIuFGCCGEEEWKhBshhBBCFCkSboQQQghRpEi4EUIIIUSRIuFGCCGEEEWKhBshhBBCFCkSboQQQghRpEi4EUIIIUSRYnJ1AZxNKQXoS6cLIYQQ4vaQ87md8zl+LXdcuElMTASgTJkyLi6JEEIIIQoqMTERf3//a+6jqfxEoCLEarVy5swZfH190TTNocdOSEigTJkynDx5Ej8/P4ceW9iTa+08cq2dR66188i1dh5HXWulFImJiZQsWRKD4dq9au64mhuDwUDp0qUL9Rx+fn7yn8VJ5Fo7j1xr55Fr7TxyrZ3HEdf6ejU2OaRDsRBCCCGKFAk3QgghhChSJNw4kNls5q233sJsNru6KEWeXGvnkWvtPHKtnUeutfO44lrfcR2KhRBCCFG0Sc2NEEIIIYoUCTdCCCGEKFIk3AghhBCiSJFwI4QQQogiRcKNg4wfP57w8HA8PDxo0qQJGzdudHWRbntjxoyhUaNG+Pr6EhwcTPfu3dm/f7/dPmlpaQwbNoxixYrh4+NDjx49iImJcVGJi44PP/wQTdMYMWKEbZtca8c5ffo0jzzyCMWKFcPT05NatWqxefNm2+NKKd58803CwsLw9PSkXbt2HDx40IUlvj1ZLBbeeOMNypcvj6enJxUrVuTdd9+1W5tIrvWN+++//+jatSslS5ZE0zRmz55t93h+ru2FCxfo168ffn5+BAQEMGjQIJKSkm6+cErctGnTpil3d3f1008/qd27d6snn3xSBQQEqJiYGFcX7bbWoUMHNWnSJLVr1y4VGRmpOnfurMqWLauSkpJs+wwZMkSVKVNGLV26VG3evFndddddqlmzZi4s9e1v48aNKjw8XNWuXVsNHz7ctl2utWNcuHBBlStXTg0YMEBt2LBBHTlyRC1cuFAdOnTIts+HH36o/P391ezZs9X27dvV/fffr8qXL69SU1NdWPLbz/vvv6+KFSum5s6dq44ePaqmT5+ufHx81BdffGHbR671jZs/f74aNWqUmjlzpgLUrFmz7B7Pz7Xt2LGjqlOnjlq/fr1atWqVqlSpkurTp89Nl03CjQM0btxYDRs2zHbfYrGokiVLqjFjxriwVEVPbGysAtTKlSuVUkrFxcUpNzc3NX36dNs+e/fuVYBat26dq4p5W0tMTFSVK1dWixcvVq1atbKFG7nWjvPKK6+oFi1aXPVxq9WqQkND1SeffGLbFhcXp8xms/r999+dUcQio0uXLurxxx+32/bggw+qfv36KaXkWjvSleEmP9d2z549ClCbNm2y7bNgwQKlaZo6ffr0TZVHmqVuUkZGBlu2bKFdu3a2bQaDgXbt2rFu3ToXlqzoiY+PByAoKAiALVu2kJmZaXftIyIiKFu2rFz7GzRs2DC6dOlid01BrrUjzZkzh4YNG9KzZ0+Cg4OpV68eP/zwg+3xo0ePEh0dbXet/f39adKkiVzrAmrWrBlLly7lwIEDAGzfvp3Vq1fTqVMnQK51YcrPtV23bh0BAQE0bNjQtk+7du0wGAxs2LDhps5/xy2c6Wjnzp3DYrEQEhJitz0kJIR9+/a5qFRFj9VqZcSIETRv3pyaNWsCEB0djbu7OwEBAXb7hoSEEB0d7YJS3t6mTZvG1q1b2bRpU67H5Fo7zpEjR/j222954YUX+N///semTZt47rnncHd3p3///rbrmdffFLnWBfPqq6+SkJBAREQERqMRi8XC+++/T79+/QDkWhei/Fzb6OhogoOD7R43mUwEBQXd9PWXcCNuC8OGDWPXrl2sXr3a1UUpkk6ePMnw4cNZvHgxHh4eri5OkWa1WmnYsCEffPABAPXq1WPXrl1MmDCB/v37u7h0Rcuff/7J1KlT+e2336hRowaRkZGMGDGCkiVLyrUu4qRZ6iYVL14co9GYa9RITEwMoaGhLipV0fLMM88wd+5cli9fTunSpW3bQ0NDycjIIC4uzm5/ufYFt2XLFmJjY6lfvz4mkwmTycTKlSv58ssvMZlMhISEyLV2kLCwMKpXr263rVq1apw4cQLAdj3lb8rNe+mll3j11Vd5+OGHqVWrFo8++ijPP/88Y8aMAeRaF6b8XNvQ0FBiY2PtHs/KyuLChQs3ff0l3Nwkd3d3GjRowNKlS23brFYrS5cupWnTpi4s2e1PKcUzzzzDrFmzWLZsGeXLl7d7vEGDBri5udld+/3793PixAm59gXUtm1bdu7cSWRkpO3WsGFD+vXrZ/tZrrVjNG/ePNeUBgcOHKBcuXIAlC9fntDQULtrnZCQwIYNG+RaF1BKSgoGg/3HnNFoxGq1AnKtC1N+rm3Tpk2Ji4tjy5Yttn2WLVuG1WqlSZMmN1eAm+qOLJRS+lBws9msJk+erPbs2aMGDx6sAgICVHR0tKuLdlt7+umnlb+/v1qxYoWKioqy3VJSUmz7DBkyRJUtW1YtW7ZMbd68WTVt2lQ1bdrUhaUuOi4fLaWUXGtH2bhxozKZTOr9999XBw8eVFOnTlVeXl7q119/te3z4YcfqoCAAPX333+rHTt2qG7dusnw5BvQv39/VapUKdtQ8JkzZ6rixYurl19+2baPXOsbl5iYqLZt26a2bdumADVu3Di1bds2dfz4caVU/q5tx44dVb169dSGDRvU6tWrVeXKlWUo+K3kq6++UmXLllXu7u6qcePGav369a4u0m0PyPM2adIk2z6pqalq6NChKjAwUHl5eakHHnhARUVFua7QRciV4UauteP8888/qmbNmspsNquIiAj1/fff2z1utVrVG2+8oUJCQpTZbFZt27ZV+/fvd1Fpb18JCQlq+PDhqmzZssrDw0NVqFBBjRo1SqWnp9v2kWt945YvX57n3+j+/fsrpfJ3bc+fP6/69OmjfHx8lJ+fnxo4cKBKTEy86bJpSl02VaMQQgghxG1O+twIIYQQokiRcCOEEEKIIkXCjRBCCCGKFAk3QgghhChSJNwIIYQQokiRcCOEEEKIIkXCjRBCCCGKFAk3Qog7nqZpzJ4929XFEEI4iIQbIYRLDRgwAE3Tct06duzo6qIJIW5TJlcXQAghOnbsyKRJk+y2mc1mF5VGCHG7k5obIYTLmc1mQkND7W6BgYGA3mT07bff0qlTJzw9PalQoQJ//fWX3fN37tzJPffcg6enJ8WKFWPw4MEkJSXZ7fPTTz9Ro0YNzGYzYWFhPPPMM3aPnzt3jgceeAAvLy8qV67MnDlzCvdFCyEKjYQbIcQt74033qBHjx5s376dfv368fDDD7N3714AkpOT6dChA4GBgWzatInp06ezZMkSu/Dy7bffMmzYMAYPHszOnTuZM2cOlSpVsjvH6NGj6dWrFzt27KBz587069ePCxcuOPV1CiEc5KaX3hRCiJvQv39/ZTQalbe3t93t/fffV0rpq8MPGTLE7jlNmjRRTz/9tFJKqe+//14FBgaqpKQk2+Pz5s1TBoNBRUdHK6WUKlmypBo1atRVywCo119/3XY/KSlJAWrBggUOe51CCOeRPjdCCJdr06YN3377rd22oKAg289Nmza1e6xp06ZERkYCsHfvXurUqYO3t7ft8ebNm2O1Wtm/fz+apnHmzBnatm17zTLUrl3b9rO3tzd+fn7Exsbe6EsSQriQhBshhMt5e3vnaiZyFE9Pz3zt5+bmZndf0zSsVmthFEkIUcikz40Q4pa3fv36XPerVasGQLVq1di+fTvJycm2x9esWYPBYKBq1ar4+voSHh7O0qVLnVpmIYTrSM2NEMLl0tPTiY6OtttmMpkoXrw4ANOnT6dhw4a0aNGCqVOnsnHjRn788UcA+vXrx1tvvUX//v15++23OXv2LM8++yyPPvooISEhALz99tsMGTKE4OBgOnXqRGJiImvWrOHZZ5917gsVQjiFhBshhMv9+++/hIWF2W2rWrUq+/btA/SRTNOmTWPo0KGEhYXx+++/U716dQC8vLxYuHAhw4cPp1GjRnh5edGjRw/GjRtnO1b//v1JS0vjs88+Y+TIkRQvXpyHHnrIeS9QCOFUmlJKuboQQghxNZqmMWvWLLp37+7qogghbhPS50YIIYQQRYqEGyGEEEIUKdLnRghxS5OWcyFEQUnNjRBCCCGKFAk3QgghhChSJNwIIYQQokiRcCOEEEKIIkXCjRBCCCGKFAk3QgghhChSJNwIIYQQokiRcCOEEEKIIkXCjRBCCCGKlP8DwHAMc2sNMTsAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot both crv\n",
    "plt.plot(np.arange(len(loss_crv)), loss_crv, label='Train')\n",
    "plt.plot(np.arange(len(val_crv)), val_crv, label='Validation')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Loss Curve of Current Faster R-CNN Model')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
