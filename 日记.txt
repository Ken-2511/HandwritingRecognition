6.15
richard：
解压了CVL数据集到这里：/root/autodl-tmp/APS360_Project/Data_Preprocessing/cvl-database-1-1，但还没有开始处理数据
写了一点点 part2.ipynb (这是一个记事本文件，就像我们lab里的文件一样)
然后他发现没有选择内核的选项，试了好多次都没有用，然后就没有然后了。。。
ken建议他重装一下vscode

ken：
解压了IAM数据集到这里：/root/autodl-tmp/APS360_Project/Datasets/IAM_Processed
写了一点点 part1_test.ipynb

ken和richard讨论了一下，我们最近的目标就是写datasets的类，另外关于segmentation的数据集，输入图片大小暂定为1024x1024px


6.16
ken：
将APS360_Project文件夹里的所有文件都移到了/root/autodl-tmp/APS360_Project里，所以之前写的一些路径可能需要做调整
继续写 part1_test.ipynb

把IAM的所有手写段落的图片都处理好了，保存到了/root/autodl-tmp/APS360_Project/Datasets/IAM_Processed这里
其中images是图片，word_info_lists是包含图片里的单词信息的list[WordInfo]。具体参见part1_test.ipynb

richard:
目前成功导出图像数据，并切转换成灰度图像，但是在处理xml参数文件的时候遇到了很大的问题，这个xml文件的BOM貌似损坏，尝试了UTF-16，utf-16，
以及其他的encoding，但是都没有成功，同时尝试了两种不同的xml解析方法，对于parase以及estree.fromstring 两种方法都尝试过结果都以失败告终。

同时我已经创建好保存框架（包括文件的创建以及自动保存路径以及写入）以及dataprocessed文件夹，其中images用于存储处理好后的图像，xml用于存储原本预定的转换后的编码的xml文件，但是目前情况
不如预期效果，等明天在接着尝试，明天会和ken商讨如果xml文件真的存在损坏，我们需要尝试寻找新的办法来crop图像。


6.17
lou:
把IAM的所有手写段落的图片中的单词进行处理，将剪切后的单独的单词的图片读出到IAM_Processed/cropped_words中

发现单词识别框存在bug，详见part1.ipynb.

ken: ↑已解决


6.18
ken:

今天ken临时变卦，说让roger只做CVL数据集，不做IAM数据集了，因为ken不小心做好了（尴尬

把 WordInfo 这个类放进了一个单独的文件，这样以后我们可以方便地在不同的文件里引用这个类

解析了一下CVL数据集的xml文件，了解了这个文件的结构。详见 part2.ipynb

新建了一个叫train的文件夹，用于训练，并初步了解了Faster R-CNN网络的功能
（顺便一提这个 Faster R-CNN 全称是 Faster Region-based Convolutional Neural Networks，是一种用于目标检测的深度学习网络）

Richard:
今天解决了CVL第一个dataset的图像裁剪问题并成功保存到了CVL_Processed，并且搞定了打包参数的实例化对象部分，明天将解决保存参数文件以及调试实例化过程。


6.22
ken:

今天做了很多事情：
1. 
新写了一个记事本：/root/autodl-tmp/APS360_Project/Data_Preprocessing/ultimate.ipynb
将之前在 `xxx_Processed/` 文件夹里的散着的数据整理了起来，保存成了整的（而不是散的）文件，分别保存在了IAM和CVL的processed文件夹下。
    文件夹1： /root/autodl-tmp/APS360_Project/Datasets/CVL_Processed
    文件夹2： /root/autodl-tmp/APS360_Project/Datasets/IAM_Processed
    这两个文件夹里面都有如下文件：
    rec_data.pt       rec_data_train.pt  rec_label.pt       rec_label_train.pt  seg_data.pt       seg_data_train.pt  seg_label.pt       seg_label_train.pt
    rec_data_test.pt  rec_data_val.pt    rec_label_test.pt  rec_label_val.pt    seg_data_test.pt  seg_data_val.pt    seg_label_test.pt  seg_label_val.pt
    这些文件都是用来训练的数据，分别是训练集、验证集和测试集的数据和标签。
    这样保存的话数据加载起来就会很快
2.
删掉了之前的 IAM_Dataset.py 和 CVL_Dataset.py，新建了一个叫 utils.py 的文件，里面放了dataset类。
3.
在ultimate.ipynb里面写了一些代码，用来测试（可视化）这些数据集的数据是否是正确的，发现并修复了之前代码的一些bug
真的，很好玩。详见 ultimate.ipynb 的最后四个cells，很有趣的！快去试试！


6.26

ken:
今天把文件上传到GitHub上了，地址：github.com/Ken-2511/HandwritingRecognition
note这个是通过Gitee上传到GitHub的，Gitee也有对应的地址：gitee.com/IWMAIN/HandwritingRecognition
发现每次上传文件都要输入用户名和密码，很麻烦，所以在Gitee上设置了SSH key，这样就不用输入用户名和密码了
还没写README文件


6.29

lou:

把Baseline Model的根据连续像素确认框选位置做好了。 文件在baseline model文件夹里。
数据在machine learning output --> binary_image_bounding_box 和 output 文件夹里。

发现IAM数据集效果很差，单词几乎识别不出来
CVL数据集效果还行，但是有时候会把一个单词分成好几个框

加上离谱的代码量这个东西是真逆天


6.30

ken:
把IAM的数据集的值域改了。原来是0~1，现在改成了0~255，重新保存到了IAM_Processed文件夹里

lou:
在baseline model里尝试把框选出的单词单独保存出来， 但是失败了（悲。 这个用baseline model做的数据集要用吗，
如果要比较主CNN和SVM的话用同一个数据集会比较好吧（小声）。！！！ Σ(っ °Д °;)っ


7.3

ken:
改掉了rec_label数据集里面的一些小bug，准确地说，是让one-hot encoding里有且只有一个1，其他都是0（之前有的时候里面全是0没有1）
把CRNN的网络搭建好了，现在输出都是符合要求的了
CRNN可以训练了，目前在IAM数据集上训练了大约90个epoch，loss降到了0.6左右就降不下去了。但目测准确率还可以，大概在80%左右，符合预期
但是Faster R-CNN的网络还没训练

7.14

lou:
把图片框出来了， 然后把分辨率调成了128*128
CVL_rec里的图片似乎太多了我打不开了（难绷
IAM_rec同理
有些图片里边还有红框，我还没有去掉


7.20

ken:
今天对于CRNN做了如下改动：
1. 把这当中的CNN部分从resnet18换成了resnet50，这样的话网络的深度就会更深，效果应该会更好
2. 把CNN部分的权重固定住了，这样的话只会训练RNN部分的权重，这样的话应该更不容易过拟合
（不不不，后来发现固定住会让loss降不下去，所以又把权重解冻了，然后我测试了一下发现纯粹是在瞎预测。所以我又把权重解冻了）

之前总是训练不好Faster R-CNN，我甚至仔细阅读官方的源代码，仔细研究怎么实现的，哪里可能会出问题，
但是还是训练不好，然后我发现每次第0个epoch训练完的时候就会出一个警告，说是卷积操作失败了
然后loss就会陡然增加，然后就不会再降下去了
后来我用conda新建了一个环境，然后安装了pytorch的preview版本，然后就不会出现这个问题了
推测可能这个问题和pytorch版本与cuda版本不兼容有关
我嘞个喵，终于解决了哈哈哈
（喵的，发现即使loss降下去了，还是有问题。推测可能是因为训练集中没有负样本导致的）

还有一件事：我今天学习了tensorboard，这个东西可以用来可视化训练过程，很简单好用，建议你们学一下
###
两分钟就可以学会tensorboard，不学一下吗？？
###
如果你们想看看的话，可以在autoDL的网页上点击`autopanel`，然后点击`tensorboard`，然后就可以看到训练过程了


7.21

ken:
Faster R-CNN
添加了负样本，负样本为随机的背景框框
在大约6k steps的时候添加了learning rate scheduler，类似cosine的函数，使得模型能够不断跳出局部最优解
在大约19.5k steps的时候改变了anchor generator的大小，使其“更适合”我们的数据集

CRNN
将lstm的layers从1改成2，并且从单向改成双向
添加了learning rate scheduler，从头训练
在epoch 10的时候在训练集里添加了transforms，使得模型能够更好地泛化

lou:
把会上决定要做的任务做完了，除了把我的部分和svm接起来
P.S. 最后处理出来的图也太逆天了， 这放到模型里能有 20% 准确率都能开香槟了

7.23

ken:
Faster R-CNN
loss还在降，但发现准确率还是不尽人意。现在rpn loss缓缓地降到0.3几，但是预测得还是依托答辩。
在epoch 450的时候把设置只针对rpn进行反向传播，这样的话应该能够更好地训练rpn

7.24

lou:
修改了框的大小和位置， 现在的图片更容易识别了

7.27

ken:
彻底放弃了Faster R-CNN，因为训练不好，准确率不高，而且训练时间太长。
改用heuristic方法，在当前的数据集下准确率很高，但面对其他数据的时候估计表现不好，因此需要优化
改了CRNN的训练代码，让代码变清晰易懂
新建了可视化CRNN的notebook，便于观察模型的性能

7.31

ken:
主要改写了segmentation的代码，现在基本上把heuristic方法的代码写完了
另外也整合了heuristic代码和CRNN代码，现在可以直接识别一整张图片了。
成果在 Train/Segmentation.ipynb 里面
额 richard 的代码还没看


8.4

ken:
今天写了GUI的代码，方便展示。但是只能在本地跑，因为服务器上没有显示器